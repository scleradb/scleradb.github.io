{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u25c4 Why Sclera? FAQ Technical Details Setup \u25c4 Components Reference Installation Reference Data Platform Connection Reference Configuration Reference ScleraSQL Reference \u25c4 Examples Introduction Standard SQL Using Cross-Tabulation Processing Ordered Data Sequence Alignment Selecting the Optimal Using Machine Learning Data Cleaning Using Text Analytics External Data Access Visualization Miscellaneous Information Interface \u25c4 Command Line Shell Reference Embedded JDBC Reference Sclera Extensions SDK \u25c4 Introduction Data Access Connectors Database System Connectors Machine Learning Library Connectors Text Analytics Library Connectors Acknowledgements \u25c4 Component Third Party Dependencies Component Licenses","title":"Reference Guide"},{"location":"#introduction","text":"Why Sclera? FAQ Technical Details","title":"Introduction"},{"location":"#setup","text":"Components Reference Installation Reference Data Platform Connection Reference Configuration Reference","title":"Setup"},{"location":"#sclerasql-reference","text":"Examples Introduction Standard SQL Using Cross-Tabulation Processing Ordered Data Sequence Alignment Selecting the Optimal Using Machine Learning Data Cleaning Using Text Analytics External Data Access Visualization Miscellaneous Information","title":"ScleraSQL Reference"},{"location":"#interface","text":"Command Line Shell Reference Embedded JDBC Reference","title":"Interface"},{"location":"#sclera-extensions-sdk","text":"Introduction Data Access Connectors Database System Connectors Machine Learning Library Connectors Text Analytics Library Connectors","title":"Sclera Extensions SDK"},{"location":"#acknowledgements","text":"Component Third Party Dependencies Component Licenses","title":"Acknowledgements"},{"location":"ack/dependencies/","text":"In this document we list out the third party dependencies for the Sclera components, and the installer. These dependencies are not distributed with Sclera, but are downloaded directly from a repository during installation of the respective component. Sclera Component Dependencies \u25c4 The following table lists the direct third-party library dependencies for each Sclera component, along with the dependency's license. Clicking on the mentioned license name will take you to the license statement. For a more complete list, please see the file NOTICE.txt in the source of the respective components. Component Dependency Dependency License Sclera - Core Engine H2 Database H2 License version 1.0 Logback Eclipse Public License version 1.0 HikariCP Apache License version 2.0 Sclera - Configuration Manager Typesafe Config Apache License version 2.0 Logback Eclipse Public License version 1.0 Sclera - Command Line Shell JLine3 BSD 3-Clause License Jansi Apache License v2.0 Logback Eclipse Public License version 1.0 Sclera - Oracle Connector Oracle JDBC Driver OTN License Agreement Sclera - MySQL Connector MySQL Connector/J GNU GPL v2 with FOSS Exception Sclera - PostgreSQL Connector PostgreSQL JDBC Driver BSD License Sclera - CSV File Connector Apache Commons CSV Apache License version 2.0 Sclera - Apache OpenNLP Connector Apache OpenNLP version 1.5.3 Apache License version 2.0 Sclera - Weka Connector Weka GNU General Public License version 2 Sclera - Web Display D3 BSD 3-Clause License D3 SVG Legend D3-Legend License Javalin Apache License v2.0 Logback Eclipse Public License version 1.0 Installer Dependencies \u25c4 The following table lists the dependencies for the Sclera installer . Clicking on the mentioned license name will take you to the license statement. Dependency Dependency License SBT Launcher Interface Apache License 2.0","title":"Component Third Party Dependencies"},{"location":"ack/dependencies/#sclera-component-dependencies","text":"The following table lists the direct third-party library dependencies for each Sclera component, along with the dependency's license. Clicking on the mentioned license name will take you to the license statement. For a more complete list, please see the file NOTICE.txt in the source of the respective components. Component Dependency Dependency License Sclera - Core Engine H2 Database H2 License version 1.0 Logback Eclipse Public License version 1.0 HikariCP Apache License version 2.0 Sclera - Configuration Manager Typesafe Config Apache License version 2.0 Logback Eclipse Public License version 1.0 Sclera - Command Line Shell JLine3 BSD 3-Clause License Jansi Apache License v2.0 Logback Eclipse Public License version 1.0 Sclera - Oracle Connector Oracle JDBC Driver OTN License Agreement Sclera - MySQL Connector MySQL Connector/J GNU GPL v2 with FOSS Exception Sclera - PostgreSQL Connector PostgreSQL JDBC Driver BSD License Sclera - CSV File Connector Apache Commons CSV Apache License version 2.0 Sclera - Apache OpenNLP Connector Apache OpenNLP version 1.5.3 Apache License version 2.0 Sclera - Weka Connector Weka GNU General Public License version 2 Sclera - Web Display D3 BSD 3-Clause License D3 SVG Legend D3-Legend License Javalin Apache License v2.0 Logback Eclipse Public License version 1.0","title":"Sclera Component Dependencies"},{"location":"ack/dependencies/#installer-dependencies","text":"The following table lists the dependencies for the Sclera installer . Clicking on the mentioned license name will take you to the license statement. Dependency Dependency License SBT Launcher Interface Apache License 2.0","title":"Installer Dependencies"},{"location":"ack/licenses/","text":"The following table lists the licences governing the use of each Sclera component: Component Component Type Component License Sclera - Core Engine Required Apache License 2.0 Sclera - Command Line Shell Optional Apache License 2.0 Sclera - JDBC Driver Optional Apache License 2.0 Sclera - Oracle Connector Optional Apache License 2.0 Sclera - MySQL Connector Optional Apache License 2.0 Sclera - PostgreSQL Connector Optional Apache License 2.0 Sclera - CSV File Connector Optional Apache License 2.0 Sclera - Text File Connector Optional Apache License 2.0 Sclera - Apache OpenNLP Connector Optional Apache License 2.0 Sclera - Weka Connector Optional GNU General Public License version 2 Sclera - Data Stream Analytics Optional Apache License 2.0 Sclera - Visualization Optional Apache License 2.0","title":"Component Licenses"},{"location":"interface/jdbc/","text":"This document provides the information you need to know to start using Sclera's embedded JDBC driver. The Sclera JDBC driver provides a standard API to manage and analyze your data from within a Java (more generally, JVM) application. This document is not a tutorial on how to use JDBC. JDBC is a Java standard; to get started on the basics, please refer to the the excellent tutorials and references provided by Oracle and PostgreSQL . Also, the Wikipedia entry on JDBC gives a quick overview. JDBC URL \u25c4 The JDBC driver is accessed through the URL jdbc:scleradb . Driver Type \u25c4 The driver is compatible with JDBC type 4 , in the sense that it is pure Java and is platform independent. Example Application \u25c4 For an example application, please see the discussion in the installation document . The code is available on GitHub, both in Java and Scala: Sclera - JDBC Example (Java version) on GitHub Sclera - JDBC Example (Scala version) on GitHub Supported Statements \u25c4 The JDBC driver accepts all SQL statements supported by Sclera . The queries return ResultSet objects , as required by the standard. However, the non-query statements ( CREATE , INSERT , UPDATE and DELETE ), which may be required by the standard to return the number of rows inserted or updated, may not return the correct number; this is because the underlying sources with non-SQL/JDBC interfaces (such as NoSQL datastores) may not return the required information. Limitations \u25c4 The JDBC support is partial (for instance, functions related to transaction processing and cursors are not supported, and only forward scans of resultsets are permitted). However, the supported API should suffice for most analytics applications, and for interfacing with most JDBC-compliant BI tools. A complete list of limitations will be posted here soon. Connecting Sclera with your Existing Applications and Reporting Tools \u25c4 The JDBC support also enables your existing applications and reporting tools to work with Sclera. You need to set the classpath to make Sclera visible to the application. Note that Sclera's SQL is largely compatible with PostgreSQL, so you can mention PostgreSQL when asked, but actually use Sclera's JDBC driver. Some applications and reporting tools can only accept (or generate) standard SQL . Such tools do not accept the SQL extensions needed to perform advanced analytics within your queries. To use such extensions, you need to define views within Sclera to perform the required analytics, and then have the application or reporting tool use these views in its queries. The exact directions depend upon the specific application that you want to link. If you face a problem in linking your application to Sclera, please let us know by sending an email to support@scleradb.com .","title":"Embedded JDBC Reference"},{"location":"interface/jdbc/#jdbc-url","text":"The JDBC driver is accessed through the URL jdbc:scleradb .","title":"JDBC URL"},{"location":"interface/jdbc/#driver-type","text":"The driver is compatible with JDBC type 4 , in the sense that it is pure Java and is platform independent.","title":"Driver Type"},{"location":"interface/jdbc/#example-application","text":"For an example application, please see the discussion in the installation document . The code is available on GitHub, both in Java and Scala: Sclera - JDBC Example (Java version) on GitHub Sclera - JDBC Example (Scala version) on GitHub","title":"Example Application"},{"location":"interface/jdbc/#supported-statements","text":"The JDBC driver accepts all SQL statements supported by Sclera . The queries return ResultSet objects , as required by the standard. However, the non-query statements ( CREATE , INSERT , UPDATE and DELETE ), which may be required by the standard to return the number of rows inserted or updated, may not return the correct number; this is because the underlying sources with non-SQL/JDBC interfaces (such as NoSQL datastores) may not return the required information.","title":"Supported Statements"},{"location":"interface/jdbc/#limitations","text":"The JDBC support is partial (for instance, functions related to transaction processing and cursors are not supported, and only forward scans of resultsets are permitted). However, the supported API should suffice for most analytics applications, and for interfacing with most JDBC-compliant BI tools. A complete list of limitations will be posted here soon.","title":"Limitations"},{"location":"interface/jdbc/#connecting-sclera-with-your-existing-applications-and-reporting-tools","text":"The JDBC support also enables your existing applications and reporting tools to work with Sclera. You need to set the classpath to make Sclera visible to the application. Note that Sclera's SQL is largely compatible with PostgreSQL, so you can mention PostgreSQL when asked, but actually use Sclera's JDBC driver. Some applications and reporting tools can only accept (or generate) standard SQL . Such tools do not accept the SQL extensions needed to perform advanced analytics within your queries. To use such extensions, you need to define views within Sclera to perform the required analytics, and then have the application or reporting tool use these views in its queries. The exact directions depend upon the specific application that you want to link. If you face a problem in linking your application to Sclera, please let us know by sending an email to support@scleradb.com .","title":"Connecting Sclera with your Existing Applications and Reporting Tools"},{"location":"interface/shell/","text":"The Sclera command line shell provides an interactive interface to manage and analyze your data, and also explore the Sclera metadata. The shell can be started by executing the script $SCLERA_ROOT/bin/sclera , where $SCLERA_ROOT is the directory where Sclera is installed . This section lists the commands that are accepted by the shell. The commands can be given on the command line, and executed interactively. Or, they can be put in a script and executed using the source command. The commands can span multiple lines, and are terminated by a semicolon ( ; ). In the following, the keywords appear in upper case to distinguish them from the other terms; however, Sclera is case-insensitive and keywords in actual commands and queries can be in upper or lower case. Configuration Management \u25c4 This set of commands enable you to manage the configuration parameters. Setting the default and cache locations \u25c4 The following command sets the DEFAULT location to a predefined location with name specified in location_name : SET DEFAULT LOCATION = location_name; Similarly, the following command sets the CACHE location to a predefined location with name specified in location_name : SET CACHE LOCATION = location_name; In either case, the location_name must be defined earlier using the ADD LOCATION command . The update is for the current shell session only. It does not affect the locations used in a concurrent session, or when Sclera is accessed through JDBC . To persistently change the locations, please update the sclera.location.datacache and sclera.location.default configuration parameters. Activating and Deactivating Runtime Explain \u25c4 The queries and commands on the command line are translated by Sclera into a sequence of subqueries and subcommands that execute on the underlying database systems. The EXPLAIN SCRIPT command enables you to activate or decativate the display of these commands on the console as they are executed. The following command activates this runtime explain feature: EXPLAIN SCRIPT [ ON ]; The ON at the end is a syntactic sugar, and can be omitted. The following command deactivates this runtime explain feature: EXPLAIN SCRIPT OFF; Display the Parameter Settings \u25c4 The following command shows the current default location, cache location and runtime explain settings: SHOW OPTIONS; Display the Configuration Parameters \u25c4 The following command shows the current configuration settings : SHOW CONFIG; Metadata Management \u25c4 Commands to manage (add and remove) data sources and underlying tables are covered in the Sclera Database System Connection Reference document.These commands can be submitted on the command line prompt. Creating and Dropping Metadata Tables \u25c4 In addition, the following command creates the metadata (aka schema) tables on the designated metadata store : CREATE SCHEMA; This is needed, for instance, if you want to change the location of the metadata store . Also, the following command deletes the tables in the designated metadata store: DROP SCHEMA; Exploring Metadata \u25c4 You can explore Sclera's metadata (the locations, tables, views, and other objects) using the LIST and DESCRIBE commands. LIST [ list_spec ]; DESCRIBE [ list_spec ]; The two commands are identical, except that LIST outputs a list of objects obtained using list_spec (see below) in a short format, whereas DESCRIBE outputs the same list of objects in a more detailed, descriptive, format. The following table presents the possible values of the optional parameter list_spec and the associated list of objects to be output. list_spec Object List (not specified) All objects ( tables across all locations , views , classifiers , clusterers and associators ) REMAINING location_name All tables in location location_name that have not been added [ TABLE ] location_name.* All tables in location location_name that have already been added [ TABLE ] location_name.table_name Table with name table_name added to location location_name , if it exists; otherwise empty TABLE All tables across all locations TABLE table_name All tables with name table_name across all locations VIEW All views VIEW view_name View with name view_name , if it exists; otherwise empty CLASSIFIER All classifiers CLASSIFIER classifier_name Classifier with name classifier_name , if it exists; otherwise empty CLUSTERER All clusterers CLUSTERER clusterer_name Clusterer with name clusterer_name , if it exists; otherwise empty LOCATION All locations ScleraSQL Commands \u25c4 ScleraSQL queries and commands accepted by Sclera are discussed in the ScleraSQL Reference document. These can be submitted on the command line prompt. Commands are executed silently, while returned query results are displayed in a table format. Looking Deeper with the Explain Command \u25c4 In addition, the shell has an EXPLAIN command that explains a query is executed by Sclera. The EXPLAIN command comes in two variants. Run-time Explain \u25c4 The first variant, called runtime explain has been discussed above . This command, called EXPLAIN SCRIPT , causes each ScleraSQL command or query to display the commands it executes on the underlying database systems. This variant of EXPLAIN is useful when you want to trace what is happening under the hood while a query or command is executing. We refer to the earlier discussion for the details. Compile-time Explain \u25c4 The second variant, called the compile-time explain, has the following syntax: EXPLAIN table_expression; This variant takes a table expression table_expression (i.e. a SQL query ) as a parameter. The table_expression is parsed and optimized by the query processor , and the resulting plan is then displayed on the console. Note that unlike the EXPLAIN SCRIPT , the query is not executed; the output shows how Sclera plans to execute the query if given without the EXPLAIN . This variant is useful when you want to explore how a query will be evaluated by Sclera, without actually evaluating the same. Script Execution \u25c4 This command enables you to execute a script of commands from a file. The syntax is: SOURCE script_file_path; where script_file_path is the full path, in quotes, of the command script to be executed. The file is read, and each command therein is executed in sequence; the output of the command, if any, is displayed on the console as the command is executed. Usability Features \u25c4 The shell maintains a command history, at a location given by the sclera.shell.history configuration parameter . You can navigate the history using the up/down keys. You can complete words by pressing tab. The word completion alternatives presented are not context sensitive, but that may change in future versions. Reset Command \u25c4 To reset all connections and recover to a clean state, issue the following command: RESET; This is equivalent to exiting and restarting the shell. Comments \u25c4 A line with two consecutive hyphens ( \"--\" ) as the first non-whitespace characters is considered a comment, and is ignored. Unlike standard SQL and PostgreSQL, comments cannot start midway in a line, after a valid input. Comments are only permitted in the shell, and in scripts input to the shell.","title":"Command Line Shell Reference"},{"location":"interface/shell/#configuration-management","text":"This set of commands enable you to manage the configuration parameters.","title":"Configuration Management"},{"location":"interface/shell/#setting-the-default-and-cache-locations","text":"The following command sets the DEFAULT location to a predefined location with name specified in location_name : SET DEFAULT LOCATION = location_name; Similarly, the following command sets the CACHE location to a predefined location with name specified in location_name : SET CACHE LOCATION = location_name; In either case, the location_name must be defined earlier using the ADD LOCATION command . The update is for the current shell session only. It does not affect the locations used in a concurrent session, or when Sclera is accessed through JDBC . To persistently change the locations, please update the sclera.location.datacache and sclera.location.default configuration parameters.","title":"Setting the default and cache locations"},{"location":"interface/shell/#activating-and-deactivating-runtime-explain","text":"The queries and commands on the command line are translated by Sclera into a sequence of subqueries and subcommands that execute on the underlying database systems. The EXPLAIN SCRIPT command enables you to activate or decativate the display of these commands on the console as they are executed. The following command activates this runtime explain feature: EXPLAIN SCRIPT [ ON ]; The ON at the end is a syntactic sugar, and can be omitted. The following command deactivates this runtime explain feature: EXPLAIN SCRIPT OFF;","title":"Activating and Deactivating Runtime Explain"},{"location":"interface/shell/#display-the-parameter-settings","text":"The following command shows the current default location, cache location and runtime explain settings: SHOW OPTIONS;","title":"Display the Parameter Settings"},{"location":"interface/shell/#display-the-configuration-parameters","text":"The following command shows the current configuration settings : SHOW CONFIG;","title":"Display the Configuration Parameters"},{"location":"interface/shell/#metadata-management","text":"Commands to manage (add and remove) data sources and underlying tables are covered in the Sclera Database System Connection Reference document.These commands can be submitted on the command line prompt.","title":"Metadata Management"},{"location":"interface/shell/#creating-and-dropping-metadata-tables","text":"In addition, the following command creates the metadata (aka schema) tables on the designated metadata store : CREATE SCHEMA; This is needed, for instance, if you want to change the location of the metadata store . Also, the following command deletes the tables in the designated metadata store: DROP SCHEMA;","title":"Creating and Dropping Metadata Tables"},{"location":"interface/shell/#exploring-metadata","text":"You can explore Sclera's metadata (the locations, tables, views, and other objects) using the LIST and DESCRIBE commands. LIST [ list_spec ]; DESCRIBE [ list_spec ]; The two commands are identical, except that LIST outputs a list of objects obtained using list_spec (see below) in a short format, whereas DESCRIBE outputs the same list of objects in a more detailed, descriptive, format. The following table presents the possible values of the optional parameter list_spec and the associated list of objects to be output. list_spec Object List (not specified) All objects ( tables across all locations , views , classifiers , clusterers and associators ) REMAINING location_name All tables in location location_name that have not been added [ TABLE ] location_name.* All tables in location location_name that have already been added [ TABLE ] location_name.table_name Table with name table_name added to location location_name , if it exists; otherwise empty TABLE All tables across all locations TABLE table_name All tables with name table_name across all locations VIEW All views VIEW view_name View with name view_name , if it exists; otherwise empty CLASSIFIER All classifiers CLASSIFIER classifier_name Classifier with name classifier_name , if it exists; otherwise empty CLUSTERER All clusterers CLUSTERER clusterer_name Clusterer with name clusterer_name , if it exists; otherwise empty LOCATION All locations","title":"Exploring Metadata"},{"location":"interface/shell/#sclerasql-commands","text":"ScleraSQL queries and commands accepted by Sclera are discussed in the ScleraSQL Reference document. These can be submitted on the command line prompt. Commands are executed silently, while returned query results are displayed in a table format.","title":"ScleraSQL Commands"},{"location":"interface/shell/#looking-deeper-with-the-explain-command","text":"In addition, the shell has an EXPLAIN command that explains a query is executed by Sclera. The EXPLAIN command comes in two variants.","title":"Looking Deeper with the Explain Command"},{"location":"interface/shell/#run-time-explain","text":"The first variant, called runtime explain has been discussed above . This command, called EXPLAIN SCRIPT , causes each ScleraSQL command or query to display the commands it executes on the underlying database systems. This variant of EXPLAIN is useful when you want to trace what is happening under the hood while a query or command is executing. We refer to the earlier discussion for the details.","title":"Run-time Explain"},{"location":"interface/shell/#compile-time-explain","text":"The second variant, called the compile-time explain, has the following syntax: EXPLAIN table_expression; This variant takes a table expression table_expression (i.e. a SQL query ) as a parameter. The table_expression is parsed and optimized by the query processor , and the resulting plan is then displayed on the console. Note that unlike the EXPLAIN SCRIPT , the query is not executed; the output shows how Sclera plans to execute the query if given without the EXPLAIN . This variant is useful when you want to explore how a query will be evaluated by Sclera, without actually evaluating the same.","title":"Compile-time Explain"},{"location":"interface/shell/#script-execution","text":"This command enables you to execute a script of commands from a file. The syntax is: SOURCE script_file_path; where script_file_path is the full path, in quotes, of the command script to be executed. The file is read, and each command therein is executed in sequence; the output of the command, if any, is displayed on the console as the command is executed.","title":"Script Execution"},{"location":"interface/shell/#usability-features","text":"The shell maintains a command history, at a location given by the sclera.shell.history configuration parameter . You can navigate the history using the up/down keys. You can complete words by pressing tab. The word completion alternatives presented are not context sensitive, but that may change in future versions.","title":"Usability Features"},{"location":"interface/shell/#reset-command","text":"To reset all connections and recover to a clean state, issue the following command: RESET; This is equivalent to exiting and restarting the shell.","title":"Reset Command"},{"location":"interface/shell/#comments","text":"A line with two consecutive hyphens ( \"--\" ) as the first non-whitespace characters is considered a comment, and is ignored. Unlike standard SQL and PostgreSQL, comments cannot start midway in a line, after a valid input. Comments are only permitted in the shell, and in scripts input to the shell.","title":"Comments"},{"location":"intro/faq/","text":"What is Sclera? \u25c4 Sclera is a stand-alone SQL processor with native support for machine learning, data virtualization and streaming data. Sclera can be deployed as an independent application, or can embed within your applications to enable advanced real-time analytics capabilities. I am a BI professional. Why do I need Sclera? \u25c4 As a BI professional, you are conversant with SQL, and understand the need to move to advanced analytics. But so far, exploring advanced analytics has meant sparing valuable time learning the myriad APIs, and getting down to coding in Java, R, Python, or whatever it takes. For you, Sclera is the most efficient way to build analytics applications. You only need to learn a handful of SQL extensions to start exploiting the power of sophisticated libraries such as Weka, incorporate external data from web-services, and perform complex event processing and stream analytics, and more -- all using familiar SQL. Moreover, it works on your existing database systems, with no need for any hardware setup and no need to move your data. I am an analytics consultant. Why do I need Sclera? \u25c4 As an analytics consultant, you develop analytics solutions for your clients. You understand that a project is never fully specified. So, it is crucial for your solutions to be agile, and be able to incorporate incremental changes as quickly as possible. Sclera gives you a modular architecture out of the box, providing exceptional agility to your solutions. Specifically, Sclera separates the analytics logic from the processing and data access. The analytics logic is specified declaratively as SQL queries with Sclera's analytics extensions. This is just a few lines of code, which can be changed easily. The analytics libraries, database systems and external data sources form their own modules and are separated from the analytics logic. The analytics queries are compiled by Sclera into optimized workflows that dynamically tie everything together. Sclera thus provides a highly modular and customizable end-to-end stack for analytics, and enables you to experiment and iterate at the speed of thought. Even after deployment of the solution, Sclera's modular architecture significantly reduces the maintenance complexity and simplifies upgrades. For instance, new technologies (database systems, analytics libraries) can be incorporated by just adding appropriate drivers, with minimal change to the application. I am a data scientist. Why do I need Sclera? \u25c4 As a data scientist, you are an expert in machine learning. You know the underlying mathematics, and your task is to develop algorithms to gain insights. However, to get access and experiment on real-life data, you need to get into implementation. You need to combine data from multiple sources. Occasionally, you find that you need to understand nontrivial APIs to figure how to perform your computations -- and your bookshelf is full of \"In Action\" and \"Definitive Guides\" that you need time to read. You spend more time poring over system logs and complex undocumented open-source code than working on your algorithms. This makes experimentation hard, distracts and slows you down. Sclera gives you a standardized interface to run your algorithms within SQL, and a clean API (in the Sclera's Extensions SDK) that you can use to integrate your algorithms, in just a few lines of code. You can now quickly experiment, iterate and thus focus on your analytics tasks, while Sclera takes care of the legwork. What is the productivity impact of using Sclera? \u25c4 Sclera simplifies the path from idea to insights. Using Sclera saves the need for deep systems skills, months of resources, and hundreds of lines of code in building analytics applications, the accompanying test and maintenance, and so on. This simplicity enables you to quickly experiment and iterate over alternatives, and focus on your analytics tasks without bothering about the complex implementation details. What is the performance impact of using Sclera? \u25c4 Sclera's SQL engine has three components: the query compiler, the embedded streaming SQL processor, and the embedded analytics evaluator. The query compiler compiles the input query into a plan -- this happens once per query, before the evaluation, and the compilation time is negligible as compared to the evaluation time. If the entire query gets pushed to an underlying database system, the cost is thus effectively zero. The embedded streaming SQL processor is used to evaluate SQL (relational) operators on streaming data, or on intermediate results to avoid materialization. This evaluation proceeds in a pipeline, in a single pass, with minimal memory overheads, and in the same JVM as your application. The embedded analytics evaluator, likewise, proceeds in a pipeline, in a single pass, and in the same JVM as your application. A handcoded Java program would have identical overheads when it uses the same library. Sclera also includes a query optimizer that optimizes the workflow before each run. These optimizations can potentially speed up the evaluation in ways that a handcoded Java application cannot. We are working on performance benchmarks, and will share the results soon. How fast is Sclera? \u25c4 Sclera aggressively pushes down query computations to the underlying database systems, and uses external analytics libraries for analytics evaluation where needed. Thus, the performance of Sclera is thus determined by the performance of the underlying database systems and analytics libraries. This means that you can start with your existing infrastructure, identify the bottlenecks, and add more resources intelligently as and when needed -- all without modifying your applications. How is Sclera different from a database system? \u25c4 To the user, Sclera is just like a relational database system -- with SQL as the interface language, and JDBC as the access mechanism from the application programs. However: Sclera does not store data. It works on data from the connected database systems and/or external data sources (on-disk file, web-service, etc.) specified in your query. Sclera queries can work on data across multiple database systems and external data sources. Sclera natively supports analytics. Analytics operations (such as classification) are provided as SQL language extensions, and analytics objects (such as classifiers) as first class-objects, at par with SQL tables. Sclera includes an embedded SQL processor, but also pushes SQL computation to an underlying database systems wherever possible. Sclera's optimizer understands the capabilities of the underlying database systems and the data stored therein, and intelligently decides where to locate the computations. Does Sclera replace my database system? \u25c4 No, Sclera complements your database systems. Sclera works with your database systems, and extends their capability to perform advanced analytics. How is Sclera different from Apache Drill? \u25c4 Apache Drill is a data virtualization solution, enabling standard SQL on Hadoop distributions, NoSQL datastores, cloud storage and local files with a variety of data formats. For data virtualization, thus, it is more versatile than Sclera. Unlike Sclera, however, Apache Drill does not provide the ability to plug in your own data processing extensions. Supporting standard SQL, it also does not provide stream pattern matching, machine learning, text analytics, data cleaning, visualization, and other capabilities that are baked into Sclera. In the near term, we plan to provide a plugin based on Apache Drill that brings its extensive virtualization capabilities to Sclera. How does Sclera compare with R? \u25c4 R is a powerful programming language. The main advantage R brings over other languages is its extensive set of modules, developed over the years by statisticians, and its excellent charting capabilities. At the same time, since R is a low-level programming language, working with R needs programming skills. Especially when you need to move beyond using the provided modules in R. You also need to be conversant with dataframes, factors and such -- which are unique to the statistician's world view. Also, R's execution engine was not designed handle large volumes of data. Further, it is hard to efficiently integrate R with the rest of your eco-system. Working with R thus needs an independent setup -- this adds to the number of processing and data silos. Sclera does not claim to provide the functionality in the hundreds of R modules -- but its own set of analytics extensions should provide most of the capabilities you need, and are well-integrated with your existing ecosystem. As an alternative, the Sclera Extensions SDK can be used to call R from with SQL, using the Java/R Interface - JRI . How does Sclera Visualization compare with ggplot2 and D3? \u25c4 ggplot2 is a graphics library for R . ggplot2 is inspired by the Grammar of Graphics , which makes it very expressible and powerful. However, ggplot2 can only be used to generate static graphs -- this means no interactivity and no support for streaming data. Also, since it is a part of the R ecosystem, you need to be a proficient R programmer and will need to go through some hoops to get it working with the rest of your ecosystem. D3 is a brilliant Javascript library for creating dynamic, interactive graphs in your browser. To use D3, you need to be a proficient Javascript programmer. Support for streaming data has to be built from ground-up in Javascript as well. Furthermore, D3 is confined to the web-browser, so all the needed data transformations needed for visualization happen within the browser, which is computationally expensive for devices such as mobile phones. Sclera Visualization (ScleraViz) , like ggplot2 , is also inspired by the Grammar of Graphics . ScleraViz is implemented as an extension to ScleraSQL and uses D3 as the rendering engine. ScleraViz brings the expressibility of ggplot2 and the power of D3 to SQL users. Unlike ggplot2 , ScleraViz can clean, analyze and plot streaming data. Also, unlike visualization implemented in D3, Sclera pushes expensive computations to the backend database servers, keeping the rendering lean and efficient. Can Sclera work with my database in the cloud? \u25c4 Yes. Sclera works with any database system that can be accessed with an API. You just need a connector to interface with the system. Google Cloud SQL is compatible with MySQL, and Amazon RDS provides MySQL, PostgreSQL and Oracle instances in the cloud. These database systems can be accessed using the relevant included database connector ( sclera-mysql , sclera-postgresql or sclera-oracle ), simply by putting the appropriate JDBC URL in the ADD LOCATION statements. Can Sclera work with web-services? \u25c4 Sclera provides a Sclera Extensions SDK that enables ingestion of data from of any data source into Sclera. A specific connector for the Google Finance web-service is included as an illustrative example . The code for the same can be accessed at GitHub . Can Sclera work with my legacy data store? \u25c4 The Sclera Extensions SDK for external data access or the Sclera Extension SDK for database systems can be used to build custom connectors to your legacy data store. The former is used when you just want to source data from the data sore, and the latter when you want to push computation (such as filter, join) to the data store. You can build the connector yourself using the Sclera Extensions SDK , or request us by sending a mail to support@scleradb.com with the details. Can Sclera work with my reporting software? \u25c4 Sclera understands a large subset of PostgreSQL's dialect of SQL. Therefore, Sclera should work with any reporting tool that works with PostgreSQL. In such tools, Sclera's JDBC driver, downloaded as a part of the installation, can be used as a drop-in replacement for PostgreSQL JDBC driver. The details on the usage of the driver appear in the Sclera JDBC reference document. Alternatively, Sclera can work in a server mode that implements the PostgreSQL backend protocol 3.0, which is compatible with PostgreSQL 7.4+. Though this server, Sclera can interface with the latest PostgreSQL ODBC and JDBC drivers, PostgreSQL\u2019s shell ( psql ) , and anything else that uses PostgreSQL\u2019s native protocol (libpq) . How do I use Sclera's analytics operators in my reporting software? \u25c4 Connected to Sclera, your reporting software can be used to query data across multiple underlying data sources. However, since these tools do not understand Sclera's extensions, they cannot generate queries with embedded analytics. A simple workaround is to create views in Sclera -- the view definition can contain arbitrary Sclera extensions, but to the external tool, they are equivalent to a relational table. Any query on such a view generated by the tool will evaluate the analytics operators included in the view definition. Can Sclera work with an analytics library of my choice? \u25c4 Yes, but the support is currenly limited to classification, clustering and association rule mining. You will need to map the classification, clustering and/or association rule API in the Sclera Extensions SDK to your library's API. The code for the Sclera - Weka Connector appears as an illustrative example on GitHub . How do I ingest data streams into Sclera? \u25c4 Sclera provides a very simple API though the Sclera Extensions SDK . A connector built using this API can be used to ingest data streams -- these data streams can then be used in the FROM clause of SQL queries. What do I need to know before using Sclera? \u25c4 If you know SQL, you can start firing cross-system queries rightaway. Then, incorporate analytics using the analytics extensions we have baked into SQL. Sclera enables use of advanced analytics constructs such as classification and clustering within SQL, but assumes you know what they are useful for. For the background, we recommend reading up a good book on data science. How do I get support? \u25c4 We are always around for help. Send a mail to support@scleradb.com , or leave a voice message at +1-909-979-9796 -- we usually respond within 24 hours.","title":"FAQ"},{"location":"intro/faq/#what-is-sclera","text":"Sclera is a stand-alone SQL processor with native support for machine learning, data virtualization and streaming data. Sclera can be deployed as an independent application, or can embed within your applications to enable advanced real-time analytics capabilities.","title":"What is Sclera?"},{"location":"intro/faq/#i-am-a-bi-professional-why-do-i-need-sclera","text":"As a BI professional, you are conversant with SQL, and understand the need to move to advanced analytics. But so far, exploring advanced analytics has meant sparing valuable time learning the myriad APIs, and getting down to coding in Java, R, Python, or whatever it takes. For you, Sclera is the most efficient way to build analytics applications. You only need to learn a handful of SQL extensions to start exploiting the power of sophisticated libraries such as Weka, incorporate external data from web-services, and perform complex event processing and stream analytics, and more -- all using familiar SQL. Moreover, it works on your existing database systems, with no need for any hardware setup and no need to move your data.","title":"I am a BI professional. Why do I need Sclera?"},{"location":"intro/faq/#i-am-an-analytics-consultant-why-do-i-need-sclera","text":"As an analytics consultant, you develop analytics solutions for your clients. You understand that a project is never fully specified. So, it is crucial for your solutions to be agile, and be able to incorporate incremental changes as quickly as possible. Sclera gives you a modular architecture out of the box, providing exceptional agility to your solutions. Specifically, Sclera separates the analytics logic from the processing and data access. The analytics logic is specified declaratively as SQL queries with Sclera's analytics extensions. This is just a few lines of code, which can be changed easily. The analytics libraries, database systems and external data sources form their own modules and are separated from the analytics logic. The analytics queries are compiled by Sclera into optimized workflows that dynamically tie everything together. Sclera thus provides a highly modular and customizable end-to-end stack for analytics, and enables you to experiment and iterate at the speed of thought. Even after deployment of the solution, Sclera's modular architecture significantly reduces the maintenance complexity and simplifies upgrades. For instance, new technologies (database systems, analytics libraries) can be incorporated by just adding appropriate drivers, with minimal change to the application.","title":"I am an analytics consultant. Why do I need Sclera?"},{"location":"intro/faq/#i-am-a-data-scientist-why-do-i-need-sclera","text":"As a data scientist, you are an expert in machine learning. You know the underlying mathematics, and your task is to develop algorithms to gain insights. However, to get access and experiment on real-life data, you need to get into implementation. You need to combine data from multiple sources. Occasionally, you find that you need to understand nontrivial APIs to figure how to perform your computations -- and your bookshelf is full of \"In Action\" and \"Definitive Guides\" that you need time to read. You spend more time poring over system logs and complex undocumented open-source code than working on your algorithms. This makes experimentation hard, distracts and slows you down. Sclera gives you a standardized interface to run your algorithms within SQL, and a clean API (in the Sclera's Extensions SDK) that you can use to integrate your algorithms, in just a few lines of code. You can now quickly experiment, iterate and thus focus on your analytics tasks, while Sclera takes care of the legwork.","title":"I am a data scientist. Why do I need Sclera?"},{"location":"intro/faq/#what-is-the-productivity-impact-of-using-sclera","text":"Sclera simplifies the path from idea to insights. Using Sclera saves the need for deep systems skills, months of resources, and hundreds of lines of code in building analytics applications, the accompanying test and maintenance, and so on. This simplicity enables you to quickly experiment and iterate over alternatives, and focus on your analytics tasks without bothering about the complex implementation details.","title":"What is the productivity impact of using Sclera?"},{"location":"intro/faq/#what-is-the-performance-impact-of-using-sclera","text":"Sclera's SQL engine has three components: the query compiler, the embedded streaming SQL processor, and the embedded analytics evaluator. The query compiler compiles the input query into a plan -- this happens once per query, before the evaluation, and the compilation time is negligible as compared to the evaluation time. If the entire query gets pushed to an underlying database system, the cost is thus effectively zero. The embedded streaming SQL processor is used to evaluate SQL (relational) operators on streaming data, or on intermediate results to avoid materialization. This evaluation proceeds in a pipeline, in a single pass, with minimal memory overheads, and in the same JVM as your application. The embedded analytics evaluator, likewise, proceeds in a pipeline, in a single pass, and in the same JVM as your application. A handcoded Java program would have identical overheads when it uses the same library. Sclera also includes a query optimizer that optimizes the workflow before each run. These optimizations can potentially speed up the evaluation in ways that a handcoded Java application cannot. We are working on performance benchmarks, and will share the results soon.","title":"What is the performance impact of using Sclera?"},{"location":"intro/faq/#how-fast-is-sclera","text":"Sclera aggressively pushes down query computations to the underlying database systems, and uses external analytics libraries for analytics evaluation where needed. Thus, the performance of Sclera is thus determined by the performance of the underlying database systems and analytics libraries. This means that you can start with your existing infrastructure, identify the bottlenecks, and add more resources intelligently as and when needed -- all without modifying your applications.","title":"How fast is Sclera?"},{"location":"intro/faq/#how-is-sclera-different-from-a-database-system","text":"To the user, Sclera is just like a relational database system -- with SQL as the interface language, and JDBC as the access mechanism from the application programs. However: Sclera does not store data. It works on data from the connected database systems and/or external data sources (on-disk file, web-service, etc.) specified in your query. Sclera queries can work on data across multiple database systems and external data sources. Sclera natively supports analytics. Analytics operations (such as classification) are provided as SQL language extensions, and analytics objects (such as classifiers) as first class-objects, at par with SQL tables. Sclera includes an embedded SQL processor, but also pushes SQL computation to an underlying database systems wherever possible. Sclera's optimizer understands the capabilities of the underlying database systems and the data stored therein, and intelligently decides where to locate the computations.","title":"How is Sclera different from a database system?"},{"location":"intro/faq/#does-sclera-replace-my-database-system","text":"No, Sclera complements your database systems. Sclera works with your database systems, and extends their capability to perform advanced analytics.","title":"Does Sclera replace my database system?"},{"location":"intro/faq/#how-is-sclera-different-from-apache-drill","text":"Apache Drill is a data virtualization solution, enabling standard SQL on Hadoop distributions, NoSQL datastores, cloud storage and local files with a variety of data formats. For data virtualization, thus, it is more versatile than Sclera. Unlike Sclera, however, Apache Drill does not provide the ability to plug in your own data processing extensions. Supporting standard SQL, it also does not provide stream pattern matching, machine learning, text analytics, data cleaning, visualization, and other capabilities that are baked into Sclera. In the near term, we plan to provide a plugin based on Apache Drill that brings its extensive virtualization capabilities to Sclera.","title":"How is Sclera different from Apache Drill?"},{"location":"intro/faq/#how-does-sclera-compare-with-r","text":"R is a powerful programming language. The main advantage R brings over other languages is its extensive set of modules, developed over the years by statisticians, and its excellent charting capabilities. At the same time, since R is a low-level programming language, working with R needs programming skills. Especially when you need to move beyond using the provided modules in R. You also need to be conversant with dataframes, factors and such -- which are unique to the statistician's world view. Also, R's execution engine was not designed handle large volumes of data. Further, it is hard to efficiently integrate R with the rest of your eco-system. Working with R thus needs an independent setup -- this adds to the number of processing and data silos. Sclera does not claim to provide the functionality in the hundreds of R modules -- but its own set of analytics extensions should provide most of the capabilities you need, and are well-integrated with your existing ecosystem. As an alternative, the Sclera Extensions SDK can be used to call R from with SQL, using the Java/R Interface - JRI .","title":"How does Sclera compare with R?"},{"location":"intro/faq/#how-does-sclera-visualization-compare-with-ggplot2-and-d3","text":"ggplot2 is a graphics library for R . ggplot2 is inspired by the Grammar of Graphics , which makes it very expressible and powerful. However, ggplot2 can only be used to generate static graphs -- this means no interactivity and no support for streaming data. Also, since it is a part of the R ecosystem, you need to be a proficient R programmer and will need to go through some hoops to get it working with the rest of your ecosystem. D3 is a brilliant Javascript library for creating dynamic, interactive graphs in your browser. To use D3, you need to be a proficient Javascript programmer. Support for streaming data has to be built from ground-up in Javascript as well. Furthermore, D3 is confined to the web-browser, so all the needed data transformations needed for visualization happen within the browser, which is computationally expensive for devices such as mobile phones. Sclera Visualization (ScleraViz) , like ggplot2 , is also inspired by the Grammar of Graphics . ScleraViz is implemented as an extension to ScleraSQL and uses D3 as the rendering engine. ScleraViz brings the expressibility of ggplot2 and the power of D3 to SQL users. Unlike ggplot2 , ScleraViz can clean, analyze and plot streaming data. Also, unlike visualization implemented in D3, Sclera pushes expensive computations to the backend database servers, keeping the rendering lean and efficient.","title":"How does Sclera Visualization compare with ggplot2 and D3?"},{"location":"intro/faq/#can-sclera-work-with-my-database-in-the-cloud","text":"Yes. Sclera works with any database system that can be accessed with an API. You just need a connector to interface with the system. Google Cloud SQL is compatible with MySQL, and Amazon RDS provides MySQL, PostgreSQL and Oracle instances in the cloud. These database systems can be accessed using the relevant included database connector ( sclera-mysql , sclera-postgresql or sclera-oracle ), simply by putting the appropriate JDBC URL in the ADD LOCATION statements.","title":"Can Sclera work with my database in the cloud?"},{"location":"intro/faq/#can-sclera-work-with-web-services","text":"Sclera provides a Sclera Extensions SDK that enables ingestion of data from of any data source into Sclera. A specific connector for the Google Finance web-service is included as an illustrative example . The code for the same can be accessed at GitHub .","title":"Can Sclera work with web-services?"},{"location":"intro/faq/#can-sclera-work-with-my-legacy-data-store","text":"The Sclera Extensions SDK for external data access or the Sclera Extension SDK for database systems can be used to build custom connectors to your legacy data store. The former is used when you just want to source data from the data sore, and the latter when you want to push computation (such as filter, join) to the data store. You can build the connector yourself using the Sclera Extensions SDK , or request us by sending a mail to support@scleradb.com with the details.","title":"Can Sclera work with my legacy data store?"},{"location":"intro/faq/#can-sclera-work-with-my-reporting-software","text":"Sclera understands a large subset of PostgreSQL's dialect of SQL. Therefore, Sclera should work with any reporting tool that works with PostgreSQL. In such tools, Sclera's JDBC driver, downloaded as a part of the installation, can be used as a drop-in replacement for PostgreSQL JDBC driver. The details on the usage of the driver appear in the Sclera JDBC reference document. Alternatively, Sclera can work in a server mode that implements the PostgreSQL backend protocol 3.0, which is compatible with PostgreSQL 7.4+. Though this server, Sclera can interface with the latest PostgreSQL ODBC and JDBC drivers, PostgreSQL\u2019s shell ( psql ) , and anything else that uses PostgreSQL\u2019s native protocol (libpq) .","title":"Can Sclera work with my reporting software?"},{"location":"intro/faq/#how-do-i-use-scleras-analytics-operators-in-my-reporting-software","text":"Connected to Sclera, your reporting software can be used to query data across multiple underlying data sources. However, since these tools do not understand Sclera's extensions, they cannot generate queries with embedded analytics. A simple workaround is to create views in Sclera -- the view definition can contain arbitrary Sclera extensions, but to the external tool, they are equivalent to a relational table. Any query on such a view generated by the tool will evaluate the analytics operators included in the view definition.","title":"How do I use Sclera's analytics operators in my reporting software?"},{"location":"intro/faq/#can-sclera-work-with-an-analytics-library-of-my-choice","text":"Yes, but the support is currenly limited to classification, clustering and association rule mining. You will need to map the classification, clustering and/or association rule API in the Sclera Extensions SDK to your library's API. The code for the Sclera - Weka Connector appears as an illustrative example on GitHub .","title":"Can Sclera work with an analytics library of my choice?"},{"location":"intro/faq/#how-do-i-ingest-data-streams-into-sclera","text":"Sclera provides a very simple API though the Sclera Extensions SDK . A connector built using this API can be used to ingest data streams -- these data streams can then be used in the FROM clause of SQL queries.","title":"How do I ingest data streams into Sclera?"},{"location":"intro/faq/#what-do-i-need-to-know-before-using-sclera","text":"If you know SQL, you can start firing cross-system queries rightaway. Then, incorporate analytics using the analytics extensions we have baked into SQL. Sclera enables use of advanced analytics constructs such as classification and clustering within SQL, but assumes you know what they are useful for. For the background, we recommend reading up a good book on data science.","title":"What do I need to know before using Sclera?"},{"location":"intro/faq/#how-do-i-get-support","text":"We are always around for help. Send a mail to support@scleradb.com , or leave a voice message at +1-909-979-9796 -- we usually respond within 24 hours.","title":"How do I get support?"},{"location":"intro/technical/","text":"At its core, Sclera contains a SQL processor , which works with a metadata store , and a cache store to process the SQL statements. SQL Processor \u25c4 The SQL processor evaluates the SQL queries and commands submitted to Sclera. The evaluation is done by translating them to subcommands/subqueries that are executed in turn on the underlying database system where the data actually resides. The challenge is to coordinate the multiple database systems in such a way that the end user is not aware of the heterogeneity, getting an experience no different from working on a single system with all the data and the analytics capabilities built in. Query Processor \u25c4 The query processor is responsible for compiling and evaluating the input query, broadly following the following steps: Parsing \u25c4 The query is parsed into an operator plan. This plan contains the details of the base data and the relational operations to be performed on that data to get the final result. The data source can be a table in a linked datastore , or a text or CSV file on the disk, or a web service. The operations can be standard relational operators (e.g. filters, project, join, group-by aggregation), or extension operators (e.g. classification, clustering, entity extraction). Optimization \u25c4 A query optimizer rewrites the plan by reordering the operations to make it more efficient. For instance, the optimizer pushes down evaluation of relational operators to the underlying data sources the extent possible. Relational operators on base tables in a datastore are marked to be executed on that datastore as a part of fetching the data. This is more efficient than fetching just the base data and evaluating the operators externally. Evaluation \u25c4 Finally, the query processor evaluates the optimized plan. This is done in two steps. First, the operator plan is converted to an evaluation plan; this replaces the operators with (a) evaluation operators (to be executed by Sclera's streaming evaluation engine) (b) expression evaluators, which are converted into queries and evaluated on an underlying system, or (c) materializers, which materialize a stream or an expression into a (temporary) table on an underlying system. Second, the evaluation plan is evaluated in a pipeline and the result is passed to the consumer via the appropriate interface (JDBC library or command line shell). Relational Operators \u25c4 Let us consider single-input relational operators first. If the input to the relational operator is already present in a datastore, the operator is evaluated as a part of the query that fetches the data. Otherwise, the operator is evaluated by Sclera's streaming evaluation engine -- the only exception is the sort operator, in which case the input is materialized in the cache store and sorted as a part of the query that fetches the result. Planning of multi-input operators is more involved. We take the example of JOIN . Recall that comma-separated from_items appear in the FROM clause are converted into a sequence of binary cross-products, which may later be converted into binary joins based on the predicates in the WHERE clause. In this section, we only consider planning of a join with two inputs. The inputs to the join are planned recursively; after planning, each input is either an expression that is to be pushed down to an underlying system as a query for evaluation, or a data stream that is the either the result of prior in-memory computations, or ingested from an external source. Further, we assume if an input is a data stream, it is the left input -- if the left input is not a data stream, but the right input is, the join is rewritten to commute its inputs. There are multiple cases: Case 1: Both inputs are data streams, the join is an inner or outer equi-join, and the input streams are sorted on the respective joining columns Sclera evaluates the join in its embedded engine using the merge-join algorithm. Case 2A: Left input is data stream, the join is an inner or outer equi-join, and the left input stream is sorted on its joining column The right input is evaluated with an ORDER BY on its joining column. This case then reduces to the Case 1 above and is evaluated accordingly. Case 2B: Left input is data stream, the join is an inner or left-outer equi-join, and the left input stream is not sorted on its joining column The right input is materialized at source location (if not a base table), and indexed on its joining column. The left input is processed in batches, and each batch probes for the joining right-input rows using the index. This is an indexed nested loop join with the left input in the outer loop. This evaluation strategy is chosen to avoid materializing the left input, which is assumed to be expensive (or impossible) to materialize. Note that a right-outer join cannot be evaluated using this strategy. Case 2C: Left input is data stream, but scenario not covered by the cases above The left input is materialized at the location of the right input (or the cache store if the right input is a stream). If right input is a stream, it is also materialized at the cache store. This reduces to the Case 3A below. This the most inefficient scenario, but we think it is rare. Nevertheless, please be careful when joining streams without the appropriate sort order, especially when the join is a right-outer join. Case 3A: Neither input is a data stream, and both inputs are present at the same location In this case, the join expression is pushed down to the common location, and is computed by the underlying system. Case 3B: Neither input is a data stream, and the inputs are present at different locations This is a cross-system join. To evaluate a cross-system join, Sclera needs all the inputs to be present at a single location; let us call this the \"target location\" for the join. This target location is decided as follows: For each input, Sclera finds the location of the underlying data. These locations are the candidates for the target location, and are listed in the order of appearance of the corresponding from_item in the FROM clause. The list may contain duplicates. From this list, Sclera then removes the cache store , if present, as well as the \"read-only\" locations . If the list is empty, Sclera assigns the cache store as the target location. This has the effect that cross-system joins across multiple read-only locations are evaluated by moving all the data to the cache store; the join is then computed at the cache store. If the list is not empty, Sclera assigns the location of the left input the target location. This has the effect that all the data from locations other than the assigned target location is moved to the target location, where the join is then computed. The ordering of the from_item s in a FROM clause thus matters when evaluating cross-system joins. While this enables you to control how data is moved while evaluating a query, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system join, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query. In the current version, Sclera moves data from a \"source\" to a \"target\" database system by reading in the data from the source and inserting it into a temporary table in the target. This transfer is done in a streaming (pipelined) manner wherever possible, to avoid reading the entire result in memory. This could be a bottleneck when large amounts of data (millions of rows) are transferred. More efficient data transfer mechanisms will be in place in later versions of Sclera. Extension Operators \u25c4 These operators are evaluated using external libraries available through a component. If the input is not already available in memory (entirely, or as a stream/iterator from a datastore), it is fetched using the datastore's interface (e.g. JDBC/SQL for a RDBMS). The component then passes the input to the associated library (after appropriate transformations, if needed); the operator's result is then computed using the library's API. An operator could be evaluated using multiple alternative components . For instance, the \"classification\" operator could be evaluated using WEKA ( component: sclera-weka ), or any other machine learning plugin. The specific library/component used can be enforced by the query, or through defaults in the configuration. See the SQL documentation for details. Note that the way the input is prepared and/or the result is obtained could be very different for different libraries. Without Sclera, moving from one library to an alternative library with similar functionality would be a messy \"porting\" job. With Sclera, all that complexity is taken care of under the hood. Command Processor \u25c4 The command processor is responsible for executing the non-query commands, such as creating tables, inserting, updating and deleting data. A command may or may not have an embedded query. If it does, Sclera makes use of the query processor discussed above to plan and execute the query, and translates the statement to work with the final result (details left out for brevity). In either case, Sclera interfaces with the underlying systems' APIs to get the task done. For instance, to create a table when the underlying system is a NoSQL datastore, Sclera makes use of the appropriate API functions to create the structure. Similar translation happens when inserting, updating or deleting rows, and so on. Schema Store \u25c4 The schema store contains the metadata that enables the SQL processor to plan the SQL statements for execution on the underlying systems. This metadata includes: The connection parameters for every database system that is connected to Sclera . The schema of the tables imported from the connected database systems Specification of the virtual tables By default, an embedded H2 database is used as a data store. This default can be changed by modifying the configuration . Cache Store \u25c4 The cache store is used by the query processor for evaluationg relational operators on intermediate results. By default, an embedded H2 main-memory database is used as a cache data store. This default can be changed by modifying the configuration .","title":"Technical Details"},{"location":"intro/technical/#sql-processor","text":"The SQL processor evaluates the SQL queries and commands submitted to Sclera. The evaluation is done by translating them to subcommands/subqueries that are executed in turn on the underlying database system where the data actually resides. The challenge is to coordinate the multiple database systems in such a way that the end user is not aware of the heterogeneity, getting an experience no different from working on a single system with all the data and the analytics capabilities built in.","title":"SQL Processor"},{"location":"intro/technical/#query-processor","text":"The query processor is responsible for compiling and evaluating the input query, broadly following the following steps:","title":"Query Processor"},{"location":"intro/technical/#parsing","text":"The query is parsed into an operator plan. This plan contains the details of the base data and the relational operations to be performed on that data to get the final result. The data source can be a table in a linked datastore , or a text or CSV file on the disk, or a web service. The operations can be standard relational operators (e.g. filters, project, join, group-by aggregation), or extension operators (e.g. classification, clustering, entity extraction).","title":"Parsing"},{"location":"intro/technical/#optimization","text":"A query optimizer rewrites the plan by reordering the operations to make it more efficient. For instance, the optimizer pushes down evaluation of relational operators to the underlying data sources the extent possible. Relational operators on base tables in a datastore are marked to be executed on that datastore as a part of fetching the data. This is more efficient than fetching just the base data and evaluating the operators externally.","title":"Optimization"},{"location":"intro/technical/#evaluation","text":"Finally, the query processor evaluates the optimized plan. This is done in two steps. First, the operator plan is converted to an evaluation plan; this replaces the operators with (a) evaluation operators (to be executed by Sclera's streaming evaluation engine) (b) expression evaluators, which are converted into queries and evaluated on an underlying system, or (c) materializers, which materialize a stream or an expression into a (temporary) table on an underlying system. Second, the evaluation plan is evaluated in a pipeline and the result is passed to the consumer via the appropriate interface (JDBC library or command line shell).","title":"Evaluation"},{"location":"intro/technical/#relational-operators","text":"Let us consider single-input relational operators first. If the input to the relational operator is already present in a datastore, the operator is evaluated as a part of the query that fetches the data. Otherwise, the operator is evaluated by Sclera's streaming evaluation engine -- the only exception is the sort operator, in which case the input is materialized in the cache store and sorted as a part of the query that fetches the result. Planning of multi-input operators is more involved. We take the example of JOIN . Recall that comma-separated from_items appear in the FROM clause are converted into a sequence of binary cross-products, which may later be converted into binary joins based on the predicates in the WHERE clause. In this section, we only consider planning of a join with two inputs. The inputs to the join are planned recursively; after planning, each input is either an expression that is to be pushed down to an underlying system as a query for evaluation, or a data stream that is the either the result of prior in-memory computations, or ingested from an external source. Further, we assume if an input is a data stream, it is the left input -- if the left input is not a data stream, but the right input is, the join is rewritten to commute its inputs. There are multiple cases: Case 1: Both inputs are data streams, the join is an inner or outer equi-join, and the input streams are sorted on the respective joining columns Sclera evaluates the join in its embedded engine using the merge-join algorithm. Case 2A: Left input is data stream, the join is an inner or outer equi-join, and the left input stream is sorted on its joining column The right input is evaluated with an ORDER BY on its joining column. This case then reduces to the Case 1 above and is evaluated accordingly. Case 2B: Left input is data stream, the join is an inner or left-outer equi-join, and the left input stream is not sorted on its joining column The right input is materialized at source location (if not a base table), and indexed on its joining column. The left input is processed in batches, and each batch probes for the joining right-input rows using the index. This is an indexed nested loop join with the left input in the outer loop. This evaluation strategy is chosen to avoid materializing the left input, which is assumed to be expensive (or impossible) to materialize. Note that a right-outer join cannot be evaluated using this strategy. Case 2C: Left input is data stream, but scenario not covered by the cases above The left input is materialized at the location of the right input (or the cache store if the right input is a stream). If right input is a stream, it is also materialized at the cache store. This reduces to the Case 3A below. This the most inefficient scenario, but we think it is rare. Nevertheless, please be careful when joining streams without the appropriate sort order, especially when the join is a right-outer join. Case 3A: Neither input is a data stream, and both inputs are present at the same location In this case, the join expression is pushed down to the common location, and is computed by the underlying system. Case 3B: Neither input is a data stream, and the inputs are present at different locations This is a cross-system join. To evaluate a cross-system join, Sclera needs all the inputs to be present at a single location; let us call this the \"target location\" for the join. This target location is decided as follows: For each input, Sclera finds the location of the underlying data. These locations are the candidates for the target location, and are listed in the order of appearance of the corresponding from_item in the FROM clause. The list may contain duplicates. From this list, Sclera then removes the cache store , if present, as well as the \"read-only\" locations . If the list is empty, Sclera assigns the cache store as the target location. This has the effect that cross-system joins across multiple read-only locations are evaluated by moving all the data to the cache store; the join is then computed at the cache store. If the list is not empty, Sclera assigns the location of the left input the target location. This has the effect that all the data from locations other than the assigned target location is moved to the target location, where the join is then computed. The ordering of the from_item s in a FROM clause thus matters when evaluating cross-system joins. While this enables you to control how data is moved while evaluating a query, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system join, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query. In the current version, Sclera moves data from a \"source\" to a \"target\" database system by reading in the data from the source and inserting it into a temporary table in the target. This transfer is done in a streaming (pipelined) manner wherever possible, to avoid reading the entire result in memory. This could be a bottleneck when large amounts of data (millions of rows) are transferred. More efficient data transfer mechanisms will be in place in later versions of Sclera.","title":"Relational Operators"},{"location":"intro/technical/#extension-operators","text":"These operators are evaluated using external libraries available through a component. If the input is not already available in memory (entirely, or as a stream/iterator from a datastore), it is fetched using the datastore's interface (e.g. JDBC/SQL for a RDBMS). The component then passes the input to the associated library (after appropriate transformations, if needed); the operator's result is then computed using the library's API. An operator could be evaluated using multiple alternative components . For instance, the \"classification\" operator could be evaluated using WEKA ( component: sclera-weka ), or any other machine learning plugin. The specific library/component used can be enforced by the query, or through defaults in the configuration. See the SQL documentation for details. Note that the way the input is prepared and/or the result is obtained could be very different for different libraries. Without Sclera, moving from one library to an alternative library with similar functionality would be a messy \"porting\" job. With Sclera, all that complexity is taken care of under the hood.","title":"Extension Operators"},{"location":"intro/technical/#command-processor","text":"The command processor is responsible for executing the non-query commands, such as creating tables, inserting, updating and deleting data. A command may or may not have an embedded query. If it does, Sclera makes use of the query processor discussed above to plan and execute the query, and translates the statement to work with the final result (details left out for brevity). In either case, Sclera interfaces with the underlying systems' APIs to get the task done. For instance, to create a table when the underlying system is a NoSQL datastore, Sclera makes use of the appropriate API functions to create the structure. Similar translation happens when inserting, updating or deleting rows, and so on.","title":"Command Processor"},{"location":"intro/technical/#schema-store","text":"The schema store contains the metadata that enables the SQL processor to plan the SQL statements for execution on the underlying systems. This metadata includes: The connection parameters for every database system that is connected to Sclera . The schema of the tables imported from the connected database systems Specification of the virtual tables By default, an embedded H2 database is used as a data store. This default can be changed by modifying the configuration .","title":"Schema Store"},{"location":"intro/technical/#cache-store","text":"The cache store is used by the query processor for evaluationg relational operators on intermediate results. By default, an embedded H2 main-memory database is used as a cache data store. This default can be changed by modifying the configuration .","title":"Cache Store"},{"location":"intro/whysclera/","text":"Designed with business intelligence professionals in mind, Sclera directly addresses the challenges an IT organization faces while implementing analytics applications. This means that you can now focus on generating insights from your data, and leave the rest to Sclera. Problem: Lack of time to develop systems/programming skills \u25c4 Moving to analytics requires developing a whole new set of skills -- learning new languages, new library APIs, and new architectures. Expertise is rare, and needs time to develop. Sclera's solution Sclera supports a large subset of standard SQL. So, if you know SQL, you can start firing cross-system queries rightaway. Later, when you need, you can start including complex analytics in your SQL queries. You do not need to know the complex APIs of the underlying systems and the analytics libraries. Sclera exposes analytics operations as straightforward SQL extensions, which are translated to appropriate API calls at runtime. As such, in the spirit of SQL, you only need to understand the analytics concepts -- not the implementation details of a specific system. Problem: Analytics is disruptive, expensive to setup \u25c4 Moving to analytics requires setting up new systems, allocating resources and setting up processes for ETL. This is not only disruptive, but also requires an upfront investment. Sclera's solution Sclera overlays over your existing infrastructure. You do not have to allocate additional resources. Sclera does not require you to move data or preprocess it (ETL). You just need to link Sclera to the data sources/database systems and Sclera takes care of executing the query across these sources/systems. You can update the infrastructure, add new systems later, if needed. Sclera will adapt to the changes with minimal disruption. Problem: Data is spread across silos, hard to consolidate \u25c4 Enterprise data is typically spread across multiple data sources. Consolidating this data for analysis is a major challenge. Typically, this implies physically moving data from each source to a centralized analytics platform (through a time and resource-intensive, multi-stage ETL process). These ETL processes are a processing bottleneck, and hinder the development of real-time analytics solutions. Sclera's solution Sclera overlays over the data silos and presents them as an unified environment. Currently, Sclera provides support for Oracle, PostgreSQL, and MySQL as well as read-only data sources such as web-services, flat files (plain text, CSV) through optional extensions. Support for additional sources is coming soon. At runtime, Sclera breaks up your queries based on the location of queried data, pushes down the computation to the respective platform and pipes the result to the appropriate analytics libraries; the resulting cross-platform workflow is executed under the hood without the need of ETL and data duplication -- the user just fires the query and gets the result. Problem: Multi-structured data \u25c4 Not all data is relational. Valuable data exists as unstructured text files (such as customer emails), or semi-structured logs (such as clickstreams, or output of a legacy system). Current systems are unable process this data. Sclera's solution Sclera comes with an extension that can extract structured data from unstructured text. Moreover, Sclera can read flat files (plain text, CSV) stored on your disk (or on HDFS), and also fetch data from web-services. Coming soon: Support for JSON data, log data Problem: Vendor Lock-In \u25c4 Analytics is a highly experimental activity. It is hard to pin down the exact infrastructure needs for an organization in advance. However, most analytics solutions are not agile. As such, enterprises are afraid of a vendor lock-in. Sclera's solution Sclera decouples the application from the backend. This means that you can replace the backend libraries based on your needs. In fact, Sclera can help you with vendor selection -- write your application first, and then pick the backend that works best for your needs. Problem: Tightly coupled data storage, analytics, and reporting \u25c4 Analytics solutions available today have a tight coupling of their database system, analytics libraries, and even the reporting (visualization) interface. This is a compromise, since it is unlikely that a product is the best at each of the three. Sclera's solution Sclera bridges across multiple database systems and multiple analytics libraries; each of these is an independent extension. This enables you to pick the best database system and the best analytics library independently. Currently, Sclera supports Oracle, MySQL, PostgreSQL (database systems), and Weka (analytics libraries). Sclera provides a standard JDBC interface, which means that you can use Sclera with the JDBC-compatible reporting/visualization engine of your choice. Problem: Lack of extensibility \u25c4 Each organization's analytics needs are different. A closed system with a specific set of analytics libraries might not be able to capture exactly what you need. For instance, you could have your data in some proprietary format, or need to interface with a legacy system with a proprietary API, or even have a proprietary algorithm to analyze your data. Sclera's solution Sclera is an open, extensible system. You can develop your own extensions for your proprietary data sources, or add your own analytics routines. These extensions can be called from within SQL queries.","title":"Why Sclera?"},{"location":"intro/whysclera/#problem-lack-of-time-to-develop-systemsprogramming-skills","text":"Moving to analytics requires developing a whole new set of skills -- learning new languages, new library APIs, and new architectures. Expertise is rare, and needs time to develop. Sclera's solution Sclera supports a large subset of standard SQL. So, if you know SQL, you can start firing cross-system queries rightaway. Later, when you need, you can start including complex analytics in your SQL queries. You do not need to know the complex APIs of the underlying systems and the analytics libraries. Sclera exposes analytics operations as straightforward SQL extensions, which are translated to appropriate API calls at runtime. As such, in the spirit of SQL, you only need to understand the analytics concepts -- not the implementation details of a specific system.","title":"Problem: Lack of time to develop systems/programming skills"},{"location":"intro/whysclera/#problem-analytics-is-disruptive-expensive-to-setup","text":"Moving to analytics requires setting up new systems, allocating resources and setting up processes for ETL. This is not only disruptive, but also requires an upfront investment. Sclera's solution Sclera overlays over your existing infrastructure. You do not have to allocate additional resources. Sclera does not require you to move data or preprocess it (ETL). You just need to link Sclera to the data sources/database systems and Sclera takes care of executing the query across these sources/systems. You can update the infrastructure, add new systems later, if needed. Sclera will adapt to the changes with minimal disruption.","title":"Problem: Analytics is disruptive, expensive to setup"},{"location":"intro/whysclera/#problem-data-is-spread-across-silos-hard-to-consolidate","text":"Enterprise data is typically spread across multiple data sources. Consolidating this data for analysis is a major challenge. Typically, this implies physically moving data from each source to a centralized analytics platform (through a time and resource-intensive, multi-stage ETL process). These ETL processes are a processing bottleneck, and hinder the development of real-time analytics solutions. Sclera's solution Sclera overlays over the data silos and presents them as an unified environment. Currently, Sclera provides support for Oracle, PostgreSQL, and MySQL as well as read-only data sources such as web-services, flat files (plain text, CSV) through optional extensions. Support for additional sources is coming soon. At runtime, Sclera breaks up your queries based on the location of queried data, pushes down the computation to the respective platform and pipes the result to the appropriate analytics libraries; the resulting cross-platform workflow is executed under the hood without the need of ETL and data duplication -- the user just fires the query and gets the result.","title":"Problem: Data is spread across silos, hard to consolidate"},{"location":"intro/whysclera/#problem-multi-structured-data","text":"Not all data is relational. Valuable data exists as unstructured text files (such as customer emails), or semi-structured logs (such as clickstreams, or output of a legacy system). Current systems are unable process this data. Sclera's solution Sclera comes with an extension that can extract structured data from unstructured text. Moreover, Sclera can read flat files (plain text, CSV) stored on your disk (or on HDFS), and also fetch data from web-services. Coming soon: Support for JSON data, log data","title":"Problem: Multi-structured data"},{"location":"intro/whysclera/#problem-vendor-lock-in","text":"Analytics is a highly experimental activity. It is hard to pin down the exact infrastructure needs for an organization in advance. However, most analytics solutions are not agile. As such, enterprises are afraid of a vendor lock-in. Sclera's solution Sclera decouples the application from the backend. This means that you can replace the backend libraries based on your needs. In fact, Sclera can help you with vendor selection -- write your application first, and then pick the backend that works best for your needs.","title":"Problem: Vendor Lock-In"},{"location":"intro/whysclera/#problem-tightly-coupled-data-storage-analytics-and-reporting","text":"Analytics solutions available today have a tight coupling of their database system, analytics libraries, and even the reporting (visualization) interface. This is a compromise, since it is unlikely that a product is the best at each of the three. Sclera's solution Sclera bridges across multiple database systems and multiple analytics libraries; each of these is an independent extension. This enables you to pick the best database system and the best analytics library independently. Currently, Sclera supports Oracle, MySQL, PostgreSQL (database systems), and Weka (analytics libraries). Sclera provides a standard JDBC interface, which means that you can use Sclera with the JDBC-compatible reporting/visualization engine of your choice.","title":"Problem: Tightly coupled data storage, analytics, and reporting"},{"location":"intro/whysclera/#problem-lack-of-extensibility","text":"Each organization's analytics needs are different. A closed system with a specific set of analytics libraries might not be able to capture exactly what you need. For instance, you could have your data in some proprietary format, or need to interface with a legacy system with a proprietary API, or even have a proprietary algorithm to analyze your data. Sclera's solution Sclera is an open, extensible system. You can develop your own extensions for your proprietary data sources, or add your own analytics routines. These extensions can be called from within SQL queries.","title":"Problem: Lack of extensibility"},{"location":"sclerasql/sqlcleaning/","text":"This document describes Sclera extensions for data cleaning. Automatic Type Inference \u25c4 Data sourced from web-services typically does not include the data types -- everything from numbers to dates are available as strings. These string values need to be cast to appropriate data types before they can be used in further computations. When the data types are known, the type-casting is can be hardcoded; but this is dangerous when the data format from an external service changes without notice. When the data types are not known, as in the case of ad-hoc data access, the data types need to be inferred manually. This is not only error-prone, it also does not scale to data sets with large number of columns. Sclera's TYPEINFER operator automates the type inference and casting. It intelligently infers the type of the specified columns from the data (you can optionally provide hints on how many rows are enough), and casts the string values accordingly. The syntax is as follows: table_expression TYPEINFER [ ( [ target_columns ] [ NULLS ( null_values ) ] [ LIMIT limit ] ) ] where: table_expression is an arbitrary table expression target_columns is a comma-separated list of columns in table_expression for which the type inference is to be done. If the type of any of these columns is not CHAR or VARCHAR , it is silently ignored. We can also specify a wildcard instead of an explicit column list. If the target_columns list is not specified, all columns of type CHAR or VARCHAR in the input are included. null_values is a list of values which, if seen in any of the target_columns , must be substituted for NULL . For instance, a common string used to mark unavailable values in datasets is \"N/A\" and \"not found\". Saying NULLS(\"N/A\", \"not found\") will replace all occurence of the strings with NULL in the output. limit is the number of rows that the operator should see before it decides on the column types. If not specified, all input rows will be scanned to infer the column types, and then a second scan will cast the values in each row to the inferred types. As an example, the following statement infers the types of the columns in the input file input.csv by looking at the first data row, and then casts all values to the inferred types on the fly as they are read from the file. EXTERNAL CSV(\"input.csv\") TYPEINFER(LIMIT 1) Automatic type inference discussed above works well when the input values (strings) contain the data in a standardised format that can be easily parsed. Integers and floating point numbers have standardized format, so running TYPEINFER on a CSV file containing only numeric values works well. However, dates and times come in all sorts of formats; TYPEINFER only understands the standard formats such as \"2016-01-06 09:15:59.0\". Anything non-standard, such as \"08/01\", and TYPEINFER will not know how to parse (actually, neither will a human - does \"08/01\" mean Jan 8 or Aug 1?). This is where the TEXT PARSE clause discussed next becomes useful. Text Parsing \u25c4 Test parsing involves parsing the values in an input column to generate one or more columns in the output. The syntax is: table_expression TEXT PARSE ( pattern ) IN input_column TO target_columns where: table_expression is an arbitrary table expression patterns is a Java regular expression , containing one or more capturing groups , which are subpatterns enclosed in parenthesis input_column is a column in the output of the table_expression , of type CHAR or VARCHAR target_columns is a list of column names, one for each capturing group in the pattern ; each column in the list is associated with a capturing group, in order of occurrence For each row in the output of table_expression (the \"input row\"), the operator parses the string in the column input_column using the specified patern . The output consists of all columns values in the input row, augmented with new columns of type VARCHAR , with names specified in the target_columns , containing the substrings extracted by matching the capturing groups in order. For instance, consider a table input_table with data as follows: id | month ---+------ 1 | '1/86' 2 | '2/86' Then, the following statement parses the string in column month in each row, separating the numbers before and after the / and places them in result columns m and y respectively. input_table TEXT parse \"(\\d+)/(\\d+)\" IN month TO (m, y) The result is: id | m | y ---+-----+---- 1 | '1' | '86' 2 | '2' | '86' The type of columns m and y is VARCHAR ; they can be cast to INT if needed. Data Imputation \u25c4 Consider a dataset with missing values in a column. A missing value is assumed as represented by NULL in the following discussion. Data imputation involves filling in the missing values with reasonable estimates. In this section, we describe the ways in which data imputation can be done in Sclera. Data Imputation with Regular SQL \u25c4 In the simplest case, we may want to fill in the missing data with a constant, or an value we know how to compute. This can be done using a trivial application of the COALESCE function . The CASE expression is more general, in that it allws you to fill in values conditionally. The harder case is when you do not know how to compute the value to fill in; this is discussed next. Data Imputation using Machine Learning \u25c4 Recall that classifiers learn how to compute the value of a given column (target) given the values of other columns in a row. The idea behind the machine learning approach to data imputation is to: Train a classifier on clean dataset. This clean dataset could be a subset of the input dataset, containing rows with all values available. Or, it could be a reference dataset available independently. Apply the classifier to the rows with missing values of the classifier's target column. This will generate estimates for the missing values. The syntax is as follows: table_expression IMPUTED WITH classifier_name ( target_column ) [ FLAG flag_column ) ] where: table_expression is an arbitrary table expression classifier_name is the name of classifier, already trained on a clean subset of the given dataset target_column is the column containing the missing values, which need to be estimated and filled in flag_column is an optional column name -- if specified, a column of this name in the result will contain a value true if the target_column was originally NULL and is now filled with an estimated value, or false otherwise.","title":"Data Cleaning"},{"location":"sclerasql/sqlcleaning/#automatic-type-inference","text":"Data sourced from web-services typically does not include the data types -- everything from numbers to dates are available as strings. These string values need to be cast to appropriate data types before they can be used in further computations. When the data types are known, the type-casting is can be hardcoded; but this is dangerous when the data format from an external service changes without notice. When the data types are not known, as in the case of ad-hoc data access, the data types need to be inferred manually. This is not only error-prone, it also does not scale to data sets with large number of columns. Sclera's TYPEINFER operator automates the type inference and casting. It intelligently infers the type of the specified columns from the data (you can optionally provide hints on how many rows are enough), and casts the string values accordingly. The syntax is as follows: table_expression TYPEINFER [ ( [ target_columns ] [ NULLS ( null_values ) ] [ LIMIT limit ] ) ] where: table_expression is an arbitrary table expression target_columns is a comma-separated list of columns in table_expression for which the type inference is to be done. If the type of any of these columns is not CHAR or VARCHAR , it is silently ignored. We can also specify a wildcard instead of an explicit column list. If the target_columns list is not specified, all columns of type CHAR or VARCHAR in the input are included. null_values is a list of values which, if seen in any of the target_columns , must be substituted for NULL . For instance, a common string used to mark unavailable values in datasets is \"N/A\" and \"not found\". Saying NULLS(\"N/A\", \"not found\") will replace all occurence of the strings with NULL in the output. limit is the number of rows that the operator should see before it decides on the column types. If not specified, all input rows will be scanned to infer the column types, and then a second scan will cast the values in each row to the inferred types. As an example, the following statement infers the types of the columns in the input file input.csv by looking at the first data row, and then casts all values to the inferred types on the fly as they are read from the file. EXTERNAL CSV(\"input.csv\") TYPEINFER(LIMIT 1) Automatic type inference discussed above works well when the input values (strings) contain the data in a standardised format that can be easily parsed. Integers and floating point numbers have standardized format, so running TYPEINFER on a CSV file containing only numeric values works well. However, dates and times come in all sorts of formats; TYPEINFER only understands the standard formats such as \"2016-01-06 09:15:59.0\". Anything non-standard, such as \"08/01\", and TYPEINFER will not know how to parse (actually, neither will a human - does \"08/01\" mean Jan 8 or Aug 1?). This is where the TEXT PARSE clause discussed next becomes useful.","title":"Automatic Type Inference"},{"location":"sclerasql/sqlcleaning/#text-parsing","text":"Test parsing involves parsing the values in an input column to generate one or more columns in the output. The syntax is: table_expression TEXT PARSE ( pattern ) IN input_column TO target_columns where: table_expression is an arbitrary table expression patterns is a Java regular expression , containing one or more capturing groups , which are subpatterns enclosed in parenthesis input_column is a column in the output of the table_expression , of type CHAR or VARCHAR target_columns is a list of column names, one for each capturing group in the pattern ; each column in the list is associated with a capturing group, in order of occurrence For each row in the output of table_expression (the \"input row\"), the operator parses the string in the column input_column using the specified patern . The output consists of all columns values in the input row, augmented with new columns of type VARCHAR , with names specified in the target_columns , containing the substrings extracted by matching the capturing groups in order. For instance, consider a table input_table with data as follows: id | month ---+------ 1 | '1/86' 2 | '2/86' Then, the following statement parses the string in column month in each row, separating the numbers before and after the / and places them in result columns m and y respectively. input_table TEXT parse \"(\\d+)/(\\d+)\" IN month TO (m, y) The result is: id | m | y ---+-----+---- 1 | '1' | '86' 2 | '2' | '86' The type of columns m and y is VARCHAR ; they can be cast to INT if needed.","title":"Text Parsing"},{"location":"sclerasql/sqlcleaning/#data-imputation","text":"Consider a dataset with missing values in a column. A missing value is assumed as represented by NULL in the following discussion. Data imputation involves filling in the missing values with reasonable estimates. In this section, we describe the ways in which data imputation can be done in Sclera.","title":"Data Imputation"},{"location":"sclerasql/sqlcleaning/#data-imputation-with-regular-sql","text":"In the simplest case, we may want to fill in the missing data with a constant, or an value we know how to compute. This can be done using a trivial application of the COALESCE function . The CASE expression is more general, in that it allws you to fill in values conditionally. The harder case is when you do not know how to compute the value to fill in; this is discussed next.","title":"Data Imputation with Regular SQL"},{"location":"sclerasql/sqlcleaning/#data-imputation-using-machine-learning","text":"Recall that classifiers learn how to compute the value of a given column (target) given the values of other columns in a row. The idea behind the machine learning approach to data imputation is to: Train a classifier on clean dataset. This clean dataset could be a subset of the input dataset, containing rows with all values available. Or, it could be a reference dataset available independently. Apply the classifier to the rows with missing values of the classifier's target column. This will generate estimates for the missing values. The syntax is as follows: table_expression IMPUTED WITH classifier_name ( target_column ) [ FLAG flag_column ) ] where: table_expression is an arbitrary table expression classifier_name is the name of classifier, already trained on a clean subset of the given dataset target_column is the column containing the missing values, which need to be estimated and filled in flag_column is an optional column name -- if specified, a column of this name in the result will contain a value true if the target_column was originally NULL and is now filled with an estimated value, or false otherwise.","title":"Data Imputation using Machine Learning"},{"location":"sclerasql/sqlcrosstab/","text":"Sclera supports standard SQL cross-tabulation function PIVOT and its inverse UNPIVOT . The semantics of these functions is the same as in Oracle 11g and MS SQL Server 2008 , but with a slightly modified (in our opinion, simplified) syntax. PIVOT \u25c4 The PIVOT operator creates a contingency table from raw input data. Examples \u25c4 For instance, consider the table vclicks containing (rather simplified) visitor clicks data. > vclicks; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 1 | 10:21:03 | login 1 | 10:24:39 | prodview 1 | 10:27:14 | logout 2 | 10:21:04 | login 2 | 10:22:10 | search 2 | 10:27:15 | logout 2 | 11:01:22 | login 1 | 11:01:23 | login 2 | 11:02:33 | prodview 2 | 11:04:10 | search 2 | 11:05:47 | prodview 1 | 11:05:48 | prodview 2 | 11:07:19 | checkout 2 | 11:09:52 | prodview 2 | 11:13:21 | logout 1 | 11:13:22 | logout -----------+-----------+---------- (16 rows) Lets say we want to see the counts of the visits for page types \"search\", \"checkout\" and \"prodview\" for each visitor. One way to compute these aggregates would be using GROUP BY aggregation: > SELECT visitorid, pagetype, COUNT(*) FROM vclicks WHERE pagetype IN (\"search\", \"checkout\", \"prodview\") GROUP BY visitorid, pagetype ORDER BY visitorid; -----------+----------+------- VISITORID | PAGETYPE | COUNT -----------+----------+------- 1 | prodview | 2 2 | prodview | 3 2 | checkout | 1 2 | search | 2 -----------+----------+------- (4 rows) This is good, but for a better view, we might want all the aggregates for a visitor to appear in the same row. This can be achieved using embedded CASE statements. > SELECT visitorid, COUNT(CASE pagetype WHEN \"search\" THEN 1 END) AS search, COUNT(CASE pagetype WHEN \"checkout\" THEN 1 END) AS checkout, COUNT(CASE pagetype WHEN \"prodview\" THEN 1 END) AS prodview FROM vclicks GROUP BY visitorid ORDER BY visitorid; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- (2 rows) With PIVOT , we can say, instead: > vclicks PARTITION BY visitorid PIVOT COUNT(*) FOR pagetype IN (\"search\", \"checkout\", \"prodview\") ORDER BY visitorid; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- (2 rows) If we want counts across all the visitors, we do not need the PARTITION BY clause. > vclicks PIVOT COUNT(*) FOR pagetype IN (\"search\", \"checkout\", \"prodview\"); --------+----------+---------- SEARCH | CHECKOUT | PRODVIEW --------+----------+---------- 2 | 1 | 5 --------+----------+---------- (1 row) We can use any aggregate in place of COUNT . To get the last visit time for the visit to the pages instead, we can say: > vclicks PARTITION BY visitorid PIVOT MAX(visittime) FOR pagetype IN (\"search\", \"checkout\", \"prodview\") ORDER BY visitorid; -----------+----------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+----------+----------+---------- 1 | | | 11:05:48 2 | 11:04:10 | 11:07:19 | 11:09:52 -----------+----------+----------+---------- (2 rows) Syntax \u25c4 The syntax of the operator is: table_expression [ PARTITION BY ( partn_columns ) ] PIVOT aggr_func ( aggr_params ) FOR target_column IN ( target_value [ AS alias ] [, ...] ) where: table_expression is an arbitrary table expression partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified: The result of table_expression is partitioned on this set of columns; the aggregation happens independently on the rows within each partition. These columns will be included in each output row, alongside the aggregates for the corresponding partition. aggr_func is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias . These are the parameters of the aggregate function aggr_func target_column is the GROUP BY column, but the values are restricted to the target_value list specified next target_value are values of target_column , these are the values on which the grouping of rows happens (within a partition). The respective aggregates will be included in the output rows as separate columns. The name of the column for target_value will be the associated alias , if present, or a string representation of target_value . UNPIVOT \u25c4 The UNPIVOT operator converts columns into rows. Examples \u25c4 Consider the result of the example above. We assume that the result is in a table pagecounts . > pagecounts; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- This table has all the counts for a visitor accumulated in a single row for the visitor. This might not always be convenient, and lets say we need a separate row for SEARCH, CHECKOUT and PRODVIEW counts for each visitor. Each row in the result needs to have three columns: a column visitorid containing the the visitor id, a column pagetype containg one of the values \"Search\", \"Checkout\" or \"ProdView\" indicating what count the row is for, and a column pagecount containing the correspoding count. The UNPIVOT operator does this very simply: > pagecounts UNPIVOT pagecount FOR pagetype IN ( search AS \"Search\", checkout AS \"Checkout\", prodview AS \"ProdView\" ); The result is: -----------+----------+---------- VISITORID | PAGETYPE | PAGECOUNT -----------+----------+---------- 1 | Search | 0 1 | Checkout | 0 1 | ProdView | 2 1 | Search | 2 1 | Checkout | 1 1 | ProdView | 3 -----------+----------+---------- Note that pagetype and pagecount are new columns. The values in pagetype correspond to the columns names in the input table, as specified in the IN clause. Syntax \u25c4 The syntax of the operator is: table_expression UNPIVOT value_column FOR label_column IN ( label_value_column [ AS label ] [, ...] ) where: table_expression is an arbitrary table expression . label_value_column is a column in the output of table_expression label is the string-valued label associated with the column label_value_column ; if unspecified, the name of label_value_column is taken as the label value_column is the column in the result that will contain the unpivoted values label_column is the column in the result that will contain the label of the column label_value_column whose value is placed in value_column Each of the columns label_value_column are assumed to be of the same type, and this common type is the type of the column value_column in the result. For each row in the output of the input table_expression , the operator will generate a row for each specified column label_value_column , consisting of: the value of label_value_column in the input row, placed in the result column value_column the label of label_value_column , placed in the result column label_column a copy of all columns in the input row except any of the columns label_value_column specified in the IN clause Overhead \u25c4 The PIVOT and UNPIVOT operators in Sclera are evaluated in a single pass over the input. The memory consumption is independent of the number of input rows.","title":"Using Cross-Tabulation"},{"location":"sclerasql/sqlcrosstab/#pivot","text":"The PIVOT operator creates a contingency table from raw input data.","title":"PIVOT"},{"location":"sclerasql/sqlcrosstab/#examples","text":"For instance, consider the table vclicks containing (rather simplified) visitor clicks data. > vclicks; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 1 | 10:21:03 | login 1 | 10:24:39 | prodview 1 | 10:27:14 | logout 2 | 10:21:04 | login 2 | 10:22:10 | search 2 | 10:27:15 | logout 2 | 11:01:22 | login 1 | 11:01:23 | login 2 | 11:02:33 | prodview 2 | 11:04:10 | search 2 | 11:05:47 | prodview 1 | 11:05:48 | prodview 2 | 11:07:19 | checkout 2 | 11:09:52 | prodview 2 | 11:13:21 | logout 1 | 11:13:22 | logout -----------+-----------+---------- (16 rows) Lets say we want to see the counts of the visits for page types \"search\", \"checkout\" and \"prodview\" for each visitor. One way to compute these aggregates would be using GROUP BY aggregation: > SELECT visitorid, pagetype, COUNT(*) FROM vclicks WHERE pagetype IN (\"search\", \"checkout\", \"prodview\") GROUP BY visitorid, pagetype ORDER BY visitorid; -----------+----------+------- VISITORID | PAGETYPE | COUNT -----------+----------+------- 1 | prodview | 2 2 | prodview | 3 2 | checkout | 1 2 | search | 2 -----------+----------+------- (4 rows) This is good, but for a better view, we might want all the aggregates for a visitor to appear in the same row. This can be achieved using embedded CASE statements. > SELECT visitorid, COUNT(CASE pagetype WHEN \"search\" THEN 1 END) AS search, COUNT(CASE pagetype WHEN \"checkout\" THEN 1 END) AS checkout, COUNT(CASE pagetype WHEN \"prodview\" THEN 1 END) AS prodview FROM vclicks GROUP BY visitorid ORDER BY visitorid; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- (2 rows) With PIVOT , we can say, instead: > vclicks PARTITION BY visitorid PIVOT COUNT(*) FOR pagetype IN (\"search\", \"checkout\", \"prodview\") ORDER BY visitorid; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- (2 rows) If we want counts across all the visitors, we do not need the PARTITION BY clause. > vclicks PIVOT COUNT(*) FOR pagetype IN (\"search\", \"checkout\", \"prodview\"); --------+----------+---------- SEARCH | CHECKOUT | PRODVIEW --------+----------+---------- 2 | 1 | 5 --------+----------+---------- (1 row) We can use any aggregate in place of COUNT . To get the last visit time for the visit to the pages instead, we can say: > vclicks PARTITION BY visitorid PIVOT MAX(visittime) FOR pagetype IN (\"search\", \"checkout\", \"prodview\") ORDER BY visitorid; -----------+----------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+----------+----------+---------- 1 | | | 11:05:48 2 | 11:04:10 | 11:07:19 | 11:09:52 -----------+----------+----------+---------- (2 rows)","title":"Examples"},{"location":"sclerasql/sqlcrosstab/#syntax","text":"The syntax of the operator is: table_expression [ PARTITION BY ( partn_columns ) ] PIVOT aggr_func ( aggr_params ) FOR target_column IN ( target_value [ AS alias ] [, ...] ) where: table_expression is an arbitrary table expression partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified: The result of table_expression is partitioned on this set of columns; the aggregation happens independently on the rows within each partition. These columns will be included in each output row, alongside the aggregates for the corresponding partition. aggr_func is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias . These are the parameters of the aggregate function aggr_func target_column is the GROUP BY column, but the values are restricted to the target_value list specified next target_value are values of target_column , these are the values on which the grouping of rows happens (within a partition). The respective aggregates will be included in the output rows as separate columns. The name of the column for target_value will be the associated alias , if present, or a string representation of target_value .","title":"Syntax"},{"location":"sclerasql/sqlcrosstab/#unpivot","text":"The UNPIVOT operator converts columns into rows.","title":"UNPIVOT"},{"location":"sclerasql/sqlcrosstab/#examples_1","text":"Consider the result of the example above. We assume that the result is in a table pagecounts . > pagecounts; -----------+--------+----------+---------- VISITORID | SEARCH | CHECKOUT | PRODVIEW -----------+--------+----------+---------- 1 | 0 | 0 | 2 2 | 2 | 1 | 3 -----------+--------+----------+---------- This table has all the counts for a visitor accumulated in a single row for the visitor. This might not always be convenient, and lets say we need a separate row for SEARCH, CHECKOUT and PRODVIEW counts for each visitor. Each row in the result needs to have three columns: a column visitorid containing the the visitor id, a column pagetype containg one of the values \"Search\", \"Checkout\" or \"ProdView\" indicating what count the row is for, and a column pagecount containing the correspoding count. The UNPIVOT operator does this very simply: > pagecounts UNPIVOT pagecount FOR pagetype IN ( search AS \"Search\", checkout AS \"Checkout\", prodview AS \"ProdView\" ); The result is: -----------+----------+---------- VISITORID | PAGETYPE | PAGECOUNT -----------+----------+---------- 1 | Search | 0 1 | Checkout | 0 1 | ProdView | 2 1 | Search | 2 1 | Checkout | 1 1 | ProdView | 3 -----------+----------+---------- Note that pagetype and pagecount are new columns. The values in pagetype correspond to the columns names in the input table, as specified in the IN clause.","title":"Examples"},{"location":"sclerasql/sqlcrosstab/#syntax_1","text":"The syntax of the operator is: table_expression UNPIVOT value_column FOR label_column IN ( label_value_column [ AS label ] [, ...] ) where: table_expression is an arbitrary table expression . label_value_column is a column in the output of table_expression label is the string-valued label associated with the column label_value_column ; if unspecified, the name of label_value_column is taken as the label value_column is the column in the result that will contain the unpivoted values label_column is the column in the result that will contain the label of the column label_value_column whose value is placed in value_column Each of the columns label_value_column are assumed to be of the same type, and this common type is the type of the column value_column in the result. For each row in the output of the input table_expression , the operator will generate a row for each specified column label_value_column , consisting of: the value of label_value_column in the input row, placed in the result column value_column the label of label_value_column , placed in the result column label_column a copy of all columns in the input row except any of the columns label_value_column specified in the IN clause","title":"Syntax"},{"location":"sclerasql/sqlcrosstab/#overhead","text":"The PIVOT and UNPIVOT operators in Sclera are evaluated in a single pass over the input. The memory consumption is independent of the number of input rows.","title":"Overhead"},{"location":"sclerasql/sqlexamples/","text":"These examples are meant to give a taste of ScleraSQL features. Please click on the \"Read More\" buttons for additional examples and extensive documentation. Data Access / Virtualization \u25c4 Working across systems is easy. The following links Sclera to the customer data in PostgreSQL and the order data in Oracle: > add location pgdb as postgresql('localhost/custdb'); > add table pgdb.customers; > add location oracledb as oracle('localhost/orderdb'); > add table oracledb.orders; Regular SQL over these tables executes across PostgreSQL and Oracle: > select location, sum(orders.total) from orders join customers on (orders.custid = customers.id) group by customers.location; Read More \u00bb Data Cleaning \u25c4 Data sourced from web-services typically does not include the data types -- everything from numbers to dates are available as strings. These string values need to be cast to appropriate data types before they can be used in further computations. ScleraSQL provides extensions for automatic type inference and text parsing for the purpose. As an example, the following statement infers the types of the columns in the input file input.csv by looking at the first ten data rows, and then casts all values to the inferred types on the fly as they are read from the file. Any occurrence of \"N/A\" is taken as NULL . > EXTERNAL CSV(\"input.csv\") TYPEINFER(NULLS(\"N/A\") LIMIT 10) As another example, the following statement parses the string (e.g. '08/12') in column month in each row of the table input_table , separating the numbers before and after the / and places them in result columns m and y respectively. > input_table TEXT PARSE \"(\\d+)/(\\d+)\" IN month TO (m, y) Sclera also provides extensions for filling in missing values in the input dataset (called \"imputation\"); this can be done through regular SQL or with the help of a classifier . Read More \u00bb Data Wrangling \u25c4 ScleraSQL supports a large subset of standard SQL. This means that you can explore and transform (e.g. filter, join, aggregate, pivot/unpivot) your data using familiar SQL. Several examples of data wrangling appear in the Sclera visualization examples , where it is used to transform the input external data into a table with one data point per row, as expected by the visualization component. For a specific illustration, click here . The code below is adapted from that example. In the following, the input data is fetched from a CSV file, and the column datatypes are determined using the TYPEINFER operator. The data, containing one row per state, is first sorted to get the states in decreasing order of the total population, and then transformed using SQL UNPIVOT to get one row per bar -- with one new column containing the population, and another containing the age group. > (EXTERNAL CSV(\"population.csv\") TYPEINFER(LIMIT 1) ORDER BY yunder5 + y5to13 + y14to17 + y18to24 + y25to44 + y45to64 + y65over DESC) UNPIVOT population FOR age IN ( yunder5 AS \"Under 5 Years\", y5to13 AS \"5 to 13 Years\", y14to17 AS \"14 to 17 Years\", y18to24 AS \"18 to 24 Years\", y25to44 AS \"25 to 44 Years\", y45to64 AS \"45 to 64 Years\", y65over AS \"65 Years and Over\" ) Compare the above with doing the same in D3/Javascript . For further examples, please see the documentation for the supported SQL subset and the PIVOT / UNPIVOT extensions . Read More \u00bb Machine Learning \u25c4 Machine learning computations are baked into ScleraSQL. The following statement trains a classifier for identifying prospects, using a survey on customers: > add table pgdb.survey; > create classifier myclassifier(isinterested) using select survey.isinterested, customers.* from survey join customers on (survey.custid = customers.id); Using the classifier is equally straightforward. The following query identifies prospects among target customers: > add table pgdb.targets; > select email, name, isprospect from (targets classified with myclassifier(isprospect)); Read More \u00bb Pattern Matching \u25c4 ScleraSQL also supports pattern matches over streaming data. The following query labels each clickstream log with the session's login time, and the log's position in the session, all in real-time: > select *, login.visittime as logintime, other.count() as pos from clicks partition by visitorid match \"login.other*\" on pagetype when \"login\" then \"login\" else \"other\"; Read More \u00bb Data Visualization \u25c4 Sclera's visualization component, ScleraViz, enables quick and easy visualization of your ScleraSQL query results. ScleraViz is integrated with ScleraSQL ; this means a few lines of ScleraSQL can fetch, clean, analyze and visualize your data in a single sweep. ScleraViz is inspired by Grammar of Graphics , specifically R's ggplot2 -- but is implemented as an extension to ScleraSQL and uses D3 as the rendering engine. Moreover, unlike ggplot2, ScleraViz can clean, analyze and plot streaming data. An online preview with a number of examples and their live demos is available at https://scleraviz.herokuapp.com . Read More \u00bb","title":"ScleraSQL Examples"},{"location":"sclerasql/sqlexamples/#data-access-virtualization","text":"Working across systems is easy. The following links Sclera to the customer data in PostgreSQL and the order data in Oracle: > add location pgdb as postgresql('localhost/custdb'); > add table pgdb.customers; > add location oracledb as oracle('localhost/orderdb'); > add table oracledb.orders; Regular SQL over these tables executes across PostgreSQL and Oracle: > select location, sum(orders.total) from orders join customers on (orders.custid = customers.id) group by customers.location; Read More \u00bb","title":"Data Access / Virtualization"},{"location":"sclerasql/sqlexamples/#data-cleaning","text":"Data sourced from web-services typically does not include the data types -- everything from numbers to dates are available as strings. These string values need to be cast to appropriate data types before they can be used in further computations. ScleraSQL provides extensions for automatic type inference and text parsing for the purpose. As an example, the following statement infers the types of the columns in the input file input.csv by looking at the first ten data rows, and then casts all values to the inferred types on the fly as they are read from the file. Any occurrence of \"N/A\" is taken as NULL . > EXTERNAL CSV(\"input.csv\") TYPEINFER(NULLS(\"N/A\") LIMIT 10) As another example, the following statement parses the string (e.g. '08/12') in column month in each row of the table input_table , separating the numbers before and after the / and places them in result columns m and y respectively. > input_table TEXT PARSE \"(\\d+)/(\\d+)\" IN month TO (m, y) Sclera also provides extensions for filling in missing values in the input dataset (called \"imputation\"); this can be done through regular SQL or with the help of a classifier . Read More \u00bb","title":"Data Cleaning"},{"location":"sclerasql/sqlexamples/#data-wrangling","text":"ScleraSQL supports a large subset of standard SQL. This means that you can explore and transform (e.g. filter, join, aggregate, pivot/unpivot) your data using familiar SQL. Several examples of data wrangling appear in the Sclera visualization examples , where it is used to transform the input external data into a table with one data point per row, as expected by the visualization component. For a specific illustration, click here . The code below is adapted from that example. In the following, the input data is fetched from a CSV file, and the column datatypes are determined using the TYPEINFER operator. The data, containing one row per state, is first sorted to get the states in decreasing order of the total population, and then transformed using SQL UNPIVOT to get one row per bar -- with one new column containing the population, and another containing the age group. > (EXTERNAL CSV(\"population.csv\") TYPEINFER(LIMIT 1) ORDER BY yunder5 + y5to13 + y14to17 + y18to24 + y25to44 + y45to64 + y65over DESC) UNPIVOT population FOR age IN ( yunder5 AS \"Under 5 Years\", y5to13 AS \"5 to 13 Years\", y14to17 AS \"14 to 17 Years\", y18to24 AS \"18 to 24 Years\", y25to44 AS \"25 to 44 Years\", y45to64 AS \"45 to 64 Years\", y65over AS \"65 Years and Over\" ) Compare the above with doing the same in D3/Javascript . For further examples, please see the documentation for the supported SQL subset and the PIVOT / UNPIVOT extensions . Read More \u00bb","title":"Data Wrangling"},{"location":"sclerasql/sqlexamples/#machine-learning","text":"Machine learning computations are baked into ScleraSQL. The following statement trains a classifier for identifying prospects, using a survey on customers: > add table pgdb.survey; > create classifier myclassifier(isinterested) using select survey.isinterested, customers.* from survey join customers on (survey.custid = customers.id); Using the classifier is equally straightforward. The following query identifies prospects among target customers: > add table pgdb.targets; > select email, name, isprospect from (targets classified with myclassifier(isprospect)); Read More \u00bb","title":"Machine Learning"},{"location":"sclerasql/sqlexamples/#pattern-matching","text":"ScleraSQL also supports pattern matches over streaming data. The following query labels each clickstream log with the session's login time, and the log's position in the session, all in real-time: > select *, login.visittime as logintime, other.count() as pos from clicks partition by visitorid match \"login.other*\" on pagetype when \"login\" then \"login\" else \"other\"; Read More \u00bb","title":"Pattern Matching"},{"location":"sclerasql/sqlexamples/#data-visualization","text":"Sclera's visualization component, ScleraViz, enables quick and easy visualization of your ScleraSQL query results. ScleraViz is integrated with ScleraSQL ; this means a few lines of ScleraSQL can fetch, clean, analyze and visualize your data in a single sweep. ScleraViz is inspired by Grammar of Graphics , specifically R's ggplot2 -- but is implemented as an extension to ScleraSQL and uses D3 as the rendering engine. Moreover, unlike ggplot2, ScleraViz can clean, analyze and plot streaming data. An online preview with a number of examples and their live demos is available at https://scleraviz.herokuapp.com . Read More \u00bb","title":"Data Visualization"},{"location":"sclerasql/sqlextarg/","text":"While computing the optimal ( MAX or MIN ) of an expression is straightforward in SQL, retrieving the rows that contain the optimal value is remarkably tedious and inefficient. As a simple example, consider the table table vhclicks containing (simplified) clickstream data for an e-commerce site. It contains a row for each page visited, containing the visitor identifier visitorid , time of visit visittime , page type pagetype , and the number of \"hovers\" by the visitor on the page hcount . > vhclicks; -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 1 | 10:21:03 | login | 1 1 | 10:24:39 | prodview | 10 1 | 10:27:14 | logout | 1 2 | 10:21:04 | login | 1 2 | 10:22:10 | search | 5 2 | 10:27:15 | logout | 1 2 | 11:01:22 | login | 1 1 | 11:01:23 | login | 1 2 | 11:02:33 | prodview | 7 2 | 11:04:10 | search | 10 2 | 11:05:47 | prodview | 5 1 | 11:05:48 | prodview | 9 2 | 11:07:19 | checkout | 3 2 | 11:09:52 | prodview | 10 2 | 11:13:21 | logout | 1 1 | 11:13:22 | logout | 1 -----------+-----------+----------+-------- (16 rows) Now, suppose we want to retrieve the rows for pages with the greatest hcount visited after 11:00 . Here is how we perform this simple task in standard SQL. > SELECT * FROM vhclicks WHERE hcount = ( SELECT MAX(hcount) FROM vhclicks WHERE visittime > '11:00:00'::time ) AND visittime > '11:00:00'::time; -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 2 | 11:09:52 | prodview | 10 -----------+-----------+----------+-------- (2 rows) Notice the redundancy -- we need to specify the condition twice, which could be painful and error-prone for a complex WHERE clause. Moreover, this takes two passes over the data, and the WHERE clause is evaluated twice. This is clearly undesirable for large datasets. Sclera provides a construct, called ARG , that eliminates the redundancy and inefficiencies in syntax as well as execution of the above query. In Sclera, we can frame the same query as: > (vhclicks WHERE visittime > '11:00:00'::time) ARG MAX(hcount) -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 2 | 11:09:52 | prodview | 10 -----------+-----------+----------+-------- (2 rows) Unlike the earlier query, this query is evaluated in a single pass over the data. We can think of ARG as a special \"filter\", similar to WHERE and HAVING . However, while WHERE and HAVING compute their output by applying a condition one row at a time, the output rows of ARG is computed based on the entire input (or the input partition ). We can get the rows containing optimal values for multiple aggregates as well -- the following query returns the row with the earliest visittime past 11:00 , in addition to the rows above. > (vhclicks WHERE visittime > '11:00:00'::time) ARG (MAX(hcount), MIN(visittime)); -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 2 | 11:09:52 | prodview | 10 2 | 11:01:22 | login | 1 -----------+-----------+----------+-------- (3 rows) It is easy to do a cascaded computation. For instance, to find the row with the earliest visittime among those with the greatest hcount across pages visited after 11:00 , we say: > (vhclicks WHERE visittime > '11:00:00'::time) ARG MAX(hcount) ARG MIN(visittime); -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 -----------+-----------+----------+-------- (1 row) We can use ARG with the MATCH operator as well. For instance, the following query finds the product views on which visitor id 1 hovered the most in a session: > (vhclicks WHERE visitorid = 1 ORDER BY visittime) ARG prodview.MAX(hcount) OVER MATCH \"login.(prodview | search | checkout)*.logout\" ON pagetype; -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 1 | 10:24:39 | prodview | 10 1 | 11:05:48 | prodview | 9 -----------+-----------+----------+-------- (2 rows) A set of rows will be output for every match of the MATCH regular expression . Note that the order of the input is relevant for the above query. ARG on Partitioned Input \u25c4 We can use ARG on partitioned input. The following query find the pages with the highest hcount for each visitor after 11:00 . > (vhclicks WHERE visittime > '11:00:00'::time) PARTITION BY visitorid ARG MAX(hcount); -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 2 | 11:09:52 | prodview | 10 1 | 11:05:48 | prodview | 9 -----------+-----------+----------+-------- (3 rows) Similarly, the following query finds, for each visitor, the product views on which the visitor hovered the most in a session: > (vhclicks ORDER BY visittime) PARTITION BY visitorid ARG prodview.MAX(hcount) OVER MATCH \"login.(prodview | search | checkout)*.logout\" ON pagetype; -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 1 | 10:24:39 | prodview | 10 2 | 11:09:52 | prodview | 10 1 | 11:05:48 | prodview | 9 -----------+-----------+----------+-------- (3 rows) ARG Syntax \u25c4 This section introduced a new table expression with the following syntax: table_expression [ PARTITION BY ( partn_columns ) ] ARG ( [ label . ] aggr_func ( aggr_params ) [, ...] ) [ [ OVER ] match_expression ] where: table_expression is an arbitrary table expression . partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified: The result of table_expression is partitioned on this set of columns; the aggregation happens independently on the rows within each partition. aggr_func is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias . These are the parameters of the aggregate function aggr_func . match_expression is an optional MATCH expression label is optional. When specified, it can be: When MATCH is not present, it is the table_alias for the table_expression , or When MATCH is present, it is a label identifying a subsequence in the MATCH regular expression .","title":"Selecting the Optimal"},{"location":"sclerasql/sqlextarg/#arg-on-partitioned-input","text":"We can use ARG on partitioned input. The following query find the pages with the highest hcount for each visitor after 11:00 . > (vhclicks WHERE visittime > '11:00:00'::time) PARTITION BY visitorid ARG MAX(hcount); -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 2 | 11:04:10 | search | 10 2 | 11:09:52 | prodview | 10 1 | 11:05:48 | prodview | 9 -----------+-----------+----------+-------- (3 rows) Similarly, the following query finds, for each visitor, the product views on which the visitor hovered the most in a session: > (vhclicks ORDER BY visittime) PARTITION BY visitorid ARG prodview.MAX(hcount) OVER MATCH \"login.(prodview | search | checkout)*.logout\" ON pagetype; -----------+-----------+----------+-------- VISITORID | VISITTIME | PAGETYPE | HCOUNT -----------+-----------+----------+-------- 1 | 10:24:39 | prodview | 10 2 | 11:09:52 | prodview | 10 1 | 11:05:48 | prodview | 9 -----------+-----------+----------+-------- (3 rows)","title":"ARG on Partitioned Input"},{"location":"sclerasql/sqlextarg/#arg-syntax","text":"This section introduced a new table expression with the following syntax: table_expression [ PARTITION BY ( partn_columns ) ] ARG ( [ label . ] aggr_func ( aggr_params ) [, ...] ) [ [ OVER ] match_expression ] where: table_expression is an arbitrary table expression . partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified: The result of table_expression is partitioned on this set of columns; the aggregation happens independently on the rows within each partition. aggr_func is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias . These are the parameters of the aggregate function aggr_func . match_expression is an optional MATCH expression label is optional. When specified, it can be: When MATCH is not present, it is the table_alias for the table_expression , or When MATCH is present, it is a label identifying a subsequence in the MATCH regular expression .","title":"ARG Syntax"},{"location":"sclerasql/sqlextdataaccess/","text":"In this section, we describe the extensions that enable access to data in CSV (Comma-Separated Value) files , free-form text files and web services within a SQL query. This data can be used just like a relational base table within the SQL query, and can be joined with data from relational/non-relational data stores (see the Data Platform Connection Reference document for details). These extensions are developed using the Sclera Connector Development SDK , the sources for each of these extensions is available on GitHub for reference. Accessing CSV Data \u25c4 Sclera can dynamically load CSV files and present the data as a table to a SQL query. Recall that a CSV file is a text file, structured into lines such that the first line contains the column names separated by commas, and the remaining lines contain the corresponding values separated by commas. This feature introduces a new table expression with the following syntax: EXTERNAL CSV( path [ , format [ , header_flag [ , path_col ] ] ] ) [ ORDERED BY ( expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ) ] path is one of the following: a URL that responds with CSV data (e.g. http://download.finance.yahoo.com/d/quotes.csv?s=AAPL&f=sl1d1t1c1ohgv&e=.csv ) the path to a CSV file the path to a directory containing multiple CSV files; each of these files is expected to have the same format and identical headers (if present). format is the CSV format, as specified by the Apache Commons CSV library . It can be one of (see the Apache Commons CSV documentation for details): DEFAULT , which specifies ',' as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and ignores empty lines in the input. RFC4180 , which specifies ',' as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and does not ignore empty lines in the input. EXCEL , which is the same as RFC4180 except that it allows missing column names. TDF , which specifies '\\t' (tab) as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and ignores spaces surrounding the values. MYSQL , which specifies '\\t' (tab) as the delimiter, does not quote the values (but escapes special characters with '\\' ), specifies '\\n' as the record separator, and does not ignore empty lines in the input. header_flag can be HEADER or NOHEADER indicating whether or not a header is present in the input CSV. If not specified, it defaults to HEADER path_col the name of an optional output column. If specified, a new column of the specified name is added to each row, and populated with the URL or file name from which that row has been read. This is useful when the path is a directory -- in this case, the specified column will contain the path to the relevant file under that directory. The ORDERED BY clause declares the sort order of the rows in the CSV file; note that this is an unverified declaration, and Sclera blindly relies on the same while planning further evaluation on the rows read. As an example, consider a CSV file \"/path/to/custinfo.csv\" containing columns email , location and age for each customer. You can view the data as a table in Sclera by saying: > EXTERNAL CSV(\"/path/to/custinfo.csv\"); This can be used in SQL queries just like a regular base table or view. The following query lists, for each distinct location, the number customers in that location. > SELECT location, COUNT(*) FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") GROUP BY location; The following query joins this table with a MySQL (connected as location myloc ) table defaulters , and computes the number of defaulters in that region: > SELECT location, COUNT(*) FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email) GROUP BY location; Note that since the CSV format does not include the type of the columns, each column in the table returned by CSV(_) has type VARCHAR . To enable automatic type inferencing, you can use the TYPEINFER operator . Exporting Query Results as CSV Files \u25c4 The EXTERNAL CSV can also be used to export the result of a query into a CSV file, using the following syntax: CREATE EXTERNAL CSV( file_path [ , format ] ) AS table_expression file_path is the path to the file to be created. If the file exists, it will be overwritten. format is as described earlier . table_expression is a table expression . The syntax is similar to the standard CREATE TABLE ... AS statement , except the TABLE is replaced by the EXTERNAL CSV(...) . For example, the following statement creates a CSV file custdefault.csv containing the JOIN of the data in custinfo.csv and the table defaulters in the location myloc : > CREATE EXTERNAL CSV(\"/path/to/custdefault.csv\") AS EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email); Similarly, the following statement creates a CSV file locdefault.csv containing the number of defaulters by location: > CREATE EXTERNAL CSV(\"/path/to/locdefault.csv\") AS SELECT location, COUNT(*) as defaulter_count FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email) GROUP BY location; Accessing Text Files \u25c4 Sclera can dynamically load raw text data from text files and present the same as a table to a SQL query. This feature introduces a new table expression with the following syntax: EXTERNAL TEXTFILES( filedir_path ) where filedir_path is the path to file to be loaded, or to a directory containing the files to be loaded. The resulting virtual table contains a row for each file, with two columns of type VARCHAR . The first column, called file contains the canonical path of the file, and the second column contents contains the textual contents of the file. For instance, the following query returns the path and contents of all files under the directory \"/tmp/mydir\" : > SELECT file, contents FROM TEXTFILES(\"/tmp/mydir\"); The resulting table can also be aggregated over, joined with other base or virtual tables, and so on, just like a base table or a view.","title":"External Data Access"},{"location":"sclerasql/sqlextdataaccess/#accessing-csv-data","text":"Sclera can dynamically load CSV files and present the data as a table to a SQL query. Recall that a CSV file is a text file, structured into lines such that the first line contains the column names separated by commas, and the remaining lines contain the corresponding values separated by commas. This feature introduces a new table expression with the following syntax: EXTERNAL CSV( path [ , format [ , header_flag [ , path_col ] ] ] ) [ ORDERED BY ( expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ) ] path is one of the following: a URL that responds with CSV data (e.g. http://download.finance.yahoo.com/d/quotes.csv?s=AAPL&f=sl1d1t1c1ohgv&e=.csv ) the path to a CSV file the path to a directory containing multiple CSV files; each of these files is expected to have the same format and identical headers (if present). format is the CSV format, as specified by the Apache Commons CSV library . It can be one of (see the Apache Commons CSV documentation for details): DEFAULT , which specifies ',' as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and ignores empty lines in the input. RFC4180 , which specifies ',' as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and does not ignore empty lines in the input. EXCEL , which is the same as RFC4180 except that it allows missing column names. TDF , which specifies '\\t' (tab) as the delimiter, '\"' as the quote, '\\r\\n' as the record separator, and ignores spaces surrounding the values. MYSQL , which specifies '\\t' (tab) as the delimiter, does not quote the values (but escapes special characters with '\\' ), specifies '\\n' as the record separator, and does not ignore empty lines in the input. header_flag can be HEADER or NOHEADER indicating whether or not a header is present in the input CSV. If not specified, it defaults to HEADER path_col the name of an optional output column. If specified, a new column of the specified name is added to each row, and populated with the URL or file name from which that row has been read. This is useful when the path is a directory -- in this case, the specified column will contain the path to the relevant file under that directory. The ORDERED BY clause declares the sort order of the rows in the CSV file; note that this is an unverified declaration, and Sclera blindly relies on the same while planning further evaluation on the rows read. As an example, consider a CSV file \"/path/to/custinfo.csv\" containing columns email , location and age for each customer. You can view the data as a table in Sclera by saying: > EXTERNAL CSV(\"/path/to/custinfo.csv\"); This can be used in SQL queries just like a regular base table or view. The following query lists, for each distinct location, the number customers in that location. > SELECT location, COUNT(*) FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") GROUP BY location; The following query joins this table with a MySQL (connected as location myloc ) table defaulters , and computes the number of defaulters in that region: > SELECT location, COUNT(*) FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email) GROUP BY location; Note that since the CSV format does not include the type of the columns, each column in the table returned by CSV(_) has type VARCHAR . To enable automatic type inferencing, you can use the TYPEINFER operator .","title":"Accessing CSV Data"},{"location":"sclerasql/sqlextdataaccess/#exporting-query-results-as-csv-files","text":"The EXTERNAL CSV can also be used to export the result of a query into a CSV file, using the following syntax: CREATE EXTERNAL CSV( file_path [ , format ] ) AS table_expression file_path is the path to the file to be created. If the file exists, it will be overwritten. format is as described earlier . table_expression is a table expression . The syntax is similar to the standard CREATE TABLE ... AS statement , except the TABLE is replaced by the EXTERNAL CSV(...) . For example, the following statement creates a CSV file custdefault.csv containing the JOIN of the data in custinfo.csv and the table defaulters in the location myloc : > CREATE EXTERNAL CSV(\"/path/to/custdefault.csv\") AS EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email); Similarly, the following statement creates a CSV file locdefault.csv containing the number of defaulters by location: > CREATE EXTERNAL CSV(\"/path/to/locdefault.csv\") AS SELECT location, COUNT(*) as defaulter_count FROM EXTERNAL CSV(\"/path/to/custinfo.csv\") JOIN myloc.defaulters USING (email) GROUP BY location;","title":"Exporting Query Results as CSV Files"},{"location":"sclerasql/sqlextdataaccess/#accessing-text-files","text":"Sclera can dynamically load raw text data from text files and present the same as a table to a SQL query. This feature introduces a new table expression with the following syntax: EXTERNAL TEXTFILES( filedir_path ) where filedir_path is the path to file to be loaded, or to a directory containing the files to be loaded. The resulting virtual table contains a row for each file, with two columns of type VARCHAR . The first column, called file contains the canonical path of the file, and the second column contents contains the textual contents of the file. For instance, the following query returns the path and contents of all files under the directory \"/tmp/mydir\" : > SELECT file, contents FROM TEXTFILES(\"/tmp/mydir\"); The resulting table can also be aggregated over, joined with other base or virtual tables, and so on, just like a base table or a view.","title":"Accessing Text Files"},{"location":"sclerasql/sqlextml/","text":"Sclera provides machine learning via first-class constructs in SQL. You can create objects such as classifiers and clusterers as easily as you create tables -- using a single CREATE ... AS statement. Further, Sclera provides SQL operators that enable classification/clustering of rows as a part of a SQL query. Off the shelf libraries such as Weka enable you to write applications with embedded machine learning. But to use these libraries without Sclera, you need to learn the proprietary APIs, and write code that complies with the same. This takes a lot of preparation and background, and is highly disruptive. Sclera provides a set of machine learning operators; these are a part of the SQL language, just like JOIN , WHERE , ORDER BY , GROUP BY , HAVING and LIMIT in regular SQL. These new operators are evaluated by calling the analytics library of choice -- reformatting the data, calling the right API functions, collating and reformatting the results and all the other boilerplate happens automatically, behind the scenes. The result is a highly compact \"declarative\" way of doing analytics, something that you can get started on almost immediately if you know basic SQL. Classification \u25c4 Consider a table survey with columns: age int gender char(1) region char(4) income real married boolean children int car boolean save_act boolean current_act boolean mortgage boolean isinterested boolean This table contains data from a customer survey, with one row per response. Apart from the customer attributes, it includes a column isinterested on whether or not the customer is interested in your product. Now, suppose you want to consider a new group of people and identify which of them might be interested in your product versus not. A classifier is the right tool to help you do this. A classifier estimates the value of a discrete-valued \"target\" variable given the values of certain \"feature\" variables. The classifier can be trained given the sample target and feature values for a set of sample instances, called the \"training data\". In the example above, the table survey is the training data; each row in the table (representing a unique customer) is a data instance, with the column isinterested as the target variable and the other columns as the feature variables. The trained classifier can be used to \"classify\" new data instances -- that is, estimate target values given the feature values in these data instances. In terms of the example, this means identifying the value of isinterested for a new customer, which is what we started out to do. In this section, we show how to train classifiers in Sclera, and use them to classify new data instances within a SQL query. Classifier Training \u25c4 The formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] CLASSIFIER classifier_name ( target_column_name ) USING table_expression This creates a classifier with the name specified by classifier_name . The classifier is trained using the result of the table expression table_expression , with the column with the name specified by target_column_name as the target, and all the remaining columns in the result as features. The optional TEMPORARY (shortened form: TEMP ) modifier creates a classifier that exists for the duration of the Sclera session only; when the session ends, the classifier is deleted. Notice the similarity with the CREATE TABLE AS statement . In our running example, the following creates the classifier myclassifier and trains it on the table survey , with the column isinterested as target and the other columns as features: > CREATE CLASSIFIER myclassifier(isinterested) USING survey; After the classifier is created, you can see the classifier description using the DESCRIBE command : > DESCRIBE myclassifier; Classifier Application \u25c4 The classifier training learns a function that estimates the value of the target column given the values of the feature columns. Applying the classifier on a table involves computing this function on each row of the table. This feature introduces a new table expression with the following syntax: table_expression CLASSIFIED WITH classifier_name ( class_column_name ) In this expression, the classifier with the name specified by classifier_name is used to classify the rows in the result of the table expression table_expression , which must include all the feature columns present in the data on which the classifier was trained (it can include additional columns). The result contains one row per input row. Each row contains all the columns in the input table_expressions and a new column, with the name as specified in class_column_name , containing the classifier output -- which is the classifier's estimate of the target variable given the feature values in the row. Continuing our example, consider a new table profiles containing profile of a new group of people. The table profiles includes all the feature columns in of the classifier myclassifier . We apply the classifier as follows: > profiles CLASSIFIED WITH myclassifier(isprospect); The output is a table with all columns in profiles , and a new column isprospect which, for each row, contains the result of applying the classifier function given the feature column values in that row. This expression can be used in a query just like a table or a view. For instance, the following query gives the count of the prospects and non-prospects in the profiles table. > SELECT isprospect, COUNT(*) FROM (profiles CLASSIFIED WITH myclassifier(isprospect)) GROUP BY isprospect; Clustering \u25c4 Consider a table customers , with one row per customer, and columns: age int income real married boolean children int car boolean To understand your customers better, you may want to group these customers based on similarity across these attributes. Note that, unlike the classifier, we do not have a apriori input on these groups; there is no \"target\", and all the columns in the input are \"feature\" columns. A clusterer is the right tool for this task. A clusterer maps data instances to one among a finite set of \"clusters\", such that data instances with similar features belong to the same cluster, and data instances with dissimilar features belong to different clusters. The clusterer can be trained given the feature values of a set of representative data instances, called the \"training data\". In the example above, the table customers is the training data; each row in the table (representing a unique customer) is a data instance, with the columns as the feature variables. In business terminology, the clusters are market segments , and this task is an instance of market segmentation . The trained clusterer can be used to assign the data instances to clusters. Since these data instances are assumed to be representative, we can use the same clusterer to map additional customers to clusters. In terms of the example, this means identifying the right market segment for a new customer. In this section, we show how to train clusterers in Sclera, and use them to assign clusters to new data instances within a SQL query. Clusterer Training \u25c4 The formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] CLUSTERER clusterer_name USING table_expression This creates a classifier with the name specified by clusterer_name . The clusterer is trained using the result of the table expression table_expression , with all the columns in the result as features. The optional TEMPORARY (shortened form: TEMP ) modifier creates a clusterer that exists for the duration of the Sclera session only; when the session ends, the clusterer is deleted. Notice the similarity with the CREATE TABLE AS statement and CREATE CLASSIFIER statement . In our running example, the following creates the clusterer myclusterer and trains it on the table customers : > CREATE CLUSTERER myclusterer USING customers; After the clusterer is created, you can see the clusterer description using the DESCRIBE command : > DESCRIBE myclusterer; Clusterer Application \u25c4 The clusterer training learns a function that computes the cluster for a data instance given the values of all the feature columns. Applying the clusterer on a table involves computing this function on each row of the table. This feature introduces a new table expression with the following syntax: table_expression CLUSTERED WITH clusterer_name ( cluster_column_name ) In this expression, the clusterer with the name specified by clusterer_name is used to assign clusters to the rows in the result of the table expression table_expression , which must include all the feature columns present in the data on which the clusterer was trained (it can include additional columns). The result contains one row per input row. Each row contains all the columns in the input table_expressions and a new column, with the name as specified in cluster_column_name , containing the clusterer output. Continuing our example, the clusterer can be applied to the table used to train the clusterer ( customers in the above example), or to any other table or output of a query, as long as it includes all the feature columns of the training data. We apply the clusterer to the table customers as follows: > customers CLUSTERED WITH myclusterer(clusterid); The output is a table with all columns in customers , and a new column clusterid which, for each row, contains the id of the cluster (an integer) assigned to that row. This expression can be used in a query just like a table or a view. For instance, the following query gives the count of the customers in each cluster. > SELECT clusterid, COUNT(*) FROM (customers CLUSTERED WITH myclusterer(clusterid)) GROUP BY clusterid; Extended Syntax for Using Specific Libraries and Algorithms \u25c4 The classifier/clusterer syntax above is agnostic of the underlying library. Sclera uses Weka as the default library, and specific classification/clustering algorithms as default. The default library is specified in the configuration file , and can be changed if required to an alternative supported library and the algorithms therein. The default library can be overriden by explicitly mentioning the library (currently, WEKA ) in the CREATE statements. Moreover, you can select the specific algorithms to use for classification/clustering, and even provide the parameters. The algorithm names and parameters depend on the specific library. Sclera does not interpret the specified algorithm parameters, and merely passes them on to the appropriate APIs of the chosen library. For instance, the following uses FOOBARML as the underlying library for creating the classifier (instead of the default, as earlier ): > CREATE FOOBARML CLASSIFIER myclassifier(isinterested) USING survey; (The above assumes that a plugin for FOOBAR has been installed.) The following specifies the use of SIMPLEKMEANS algorithm for clustering in Weka: > CREATE WEKA CLUSTERER(\"SIMPLEKMEANS\") myclusterer USING customers; To further specify cluster counts as 3 (overriding the default 2): > CREATE WEKA CLUSTERER(\"SIMPLEKMEANS\", \"-N 3\") myclusterer USING customers; The extended formal syntax for classification/clustering, incorporating these overrides, is discussed below. Extended Syntax for Classification \u25c4 The extended formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] [ library_name ] CLASSIFIER [ ( algorithm_name [ , algorithm_options ] ) ] classifier_name ( target_column_name ) USING table_expression The optional library_name specifies the machine learning library to use for the task. If not specified, the WEKA is used (this default can be modified using the sclera.service.default.mlservice configuration parameter ). The optional algorithm_name identifies the algorithm to be used in training the classifier. The following are supported: for WEKA : J48 ( documentation ) HOEFFDINGTREE ( documentation ) LMT ( documentation ) M5P ( documentation ) RANDOMFOREST ( documentation ) REPTREE ( documentation ) CLASSIFICATIONVIAREGRESSION ( documentation ) DECISIONTABLE ( documentation ) M5RULES ( documentation ) ONER ( documentation ) LOGISTIC ( documentation ) NAIVEBAYES ( documentation ) If not specified, J48 (for WEKA ) is used as the default. The optional algorithm_options provides the configuration options (parameters) for the algorithm identified by algorithm_name . These options are passed as a single string, just as in a command line. Please refer to the Weka documentation the respective algorithms, linked above, for details of the accepted options and defaults. If not specified, the default parameters for the specified algorithm are used. Remaining parameters are as in the abridged syntax discussed earlier. Extended Syntax for Clustering \u25c4 The extended formal syntax for creating an clusterer is as follows: CREATE [ { TEMPORARY | TEMP } ] [ library_name ] CLUSTERER [ ( algorithm_name [ , algorithm_options ] ) ] clusterer_name USING table_expression The optional library_name specifies the machine learning library to use for the task. In the current version, only WEKA is accepted as Sclera currently only interfaces with Weka for clustering. If not specified, WEKA is used (this default can be modified using the sclera.service.default.mlservice configuration parameter ). The optional algorithm_name identifies the algorithm to be used in training the clusterer. The following are supported for WEKA : SIMPLEKMEANS ( documentation ) COBWEB ( documentation ) EM ( documentation ) FARTHESTFIRST ( documentation ) HIERARCHICAL ( documentation ) If not specified, SIMPLEKMEANS is used as the default. The optional algorithm_options provides the configuration options (parameters) for the algorithm identified by algorithm_name . These options are passed as a single string, just as in a command line. Please refer to the Weka documentation the respective algorithms, linked above, for details of the accepted options and defaults. If not specified, the default parameters for the specified algorithm are used. Remaining parameters are as in the abridged syntax discussed earlier.","title":"Using Machine Learning"},{"location":"sclerasql/sqlextml/#classification","text":"Consider a table survey with columns: age int gender char(1) region char(4) income real married boolean children int car boolean save_act boolean current_act boolean mortgage boolean isinterested boolean This table contains data from a customer survey, with one row per response. Apart from the customer attributes, it includes a column isinterested on whether or not the customer is interested in your product. Now, suppose you want to consider a new group of people and identify which of them might be interested in your product versus not. A classifier is the right tool to help you do this. A classifier estimates the value of a discrete-valued \"target\" variable given the values of certain \"feature\" variables. The classifier can be trained given the sample target and feature values for a set of sample instances, called the \"training data\". In the example above, the table survey is the training data; each row in the table (representing a unique customer) is a data instance, with the column isinterested as the target variable and the other columns as the feature variables. The trained classifier can be used to \"classify\" new data instances -- that is, estimate target values given the feature values in these data instances. In terms of the example, this means identifying the value of isinterested for a new customer, which is what we started out to do. In this section, we show how to train classifiers in Sclera, and use them to classify new data instances within a SQL query.","title":"Classification"},{"location":"sclerasql/sqlextml/#classifier-training","text":"The formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] CLASSIFIER classifier_name ( target_column_name ) USING table_expression This creates a classifier with the name specified by classifier_name . The classifier is trained using the result of the table expression table_expression , with the column with the name specified by target_column_name as the target, and all the remaining columns in the result as features. The optional TEMPORARY (shortened form: TEMP ) modifier creates a classifier that exists for the duration of the Sclera session only; when the session ends, the classifier is deleted. Notice the similarity with the CREATE TABLE AS statement . In our running example, the following creates the classifier myclassifier and trains it on the table survey , with the column isinterested as target and the other columns as features: > CREATE CLASSIFIER myclassifier(isinterested) USING survey; After the classifier is created, you can see the classifier description using the DESCRIBE command : > DESCRIBE myclassifier;","title":"Classifier Training"},{"location":"sclerasql/sqlextml/#classifier-application","text":"The classifier training learns a function that estimates the value of the target column given the values of the feature columns. Applying the classifier on a table involves computing this function on each row of the table. This feature introduces a new table expression with the following syntax: table_expression CLASSIFIED WITH classifier_name ( class_column_name ) In this expression, the classifier with the name specified by classifier_name is used to classify the rows in the result of the table expression table_expression , which must include all the feature columns present in the data on which the classifier was trained (it can include additional columns). The result contains one row per input row. Each row contains all the columns in the input table_expressions and a new column, with the name as specified in class_column_name , containing the classifier output -- which is the classifier's estimate of the target variable given the feature values in the row. Continuing our example, consider a new table profiles containing profile of a new group of people. The table profiles includes all the feature columns in of the classifier myclassifier . We apply the classifier as follows: > profiles CLASSIFIED WITH myclassifier(isprospect); The output is a table with all columns in profiles , and a new column isprospect which, for each row, contains the result of applying the classifier function given the feature column values in that row. This expression can be used in a query just like a table or a view. For instance, the following query gives the count of the prospects and non-prospects in the profiles table. > SELECT isprospect, COUNT(*) FROM (profiles CLASSIFIED WITH myclassifier(isprospect)) GROUP BY isprospect;","title":"Classifier Application"},{"location":"sclerasql/sqlextml/#clustering","text":"Consider a table customers , with one row per customer, and columns: age int income real married boolean children int car boolean To understand your customers better, you may want to group these customers based on similarity across these attributes. Note that, unlike the classifier, we do not have a apriori input on these groups; there is no \"target\", and all the columns in the input are \"feature\" columns. A clusterer is the right tool for this task. A clusterer maps data instances to one among a finite set of \"clusters\", such that data instances with similar features belong to the same cluster, and data instances with dissimilar features belong to different clusters. The clusterer can be trained given the feature values of a set of representative data instances, called the \"training data\". In the example above, the table customers is the training data; each row in the table (representing a unique customer) is a data instance, with the columns as the feature variables. In business terminology, the clusters are market segments , and this task is an instance of market segmentation . The trained clusterer can be used to assign the data instances to clusters. Since these data instances are assumed to be representative, we can use the same clusterer to map additional customers to clusters. In terms of the example, this means identifying the right market segment for a new customer. In this section, we show how to train clusterers in Sclera, and use them to assign clusters to new data instances within a SQL query.","title":"Clustering"},{"location":"sclerasql/sqlextml/#clusterer-training","text":"The formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] CLUSTERER clusterer_name USING table_expression This creates a classifier with the name specified by clusterer_name . The clusterer is trained using the result of the table expression table_expression , with all the columns in the result as features. The optional TEMPORARY (shortened form: TEMP ) modifier creates a clusterer that exists for the duration of the Sclera session only; when the session ends, the clusterer is deleted. Notice the similarity with the CREATE TABLE AS statement and CREATE CLASSIFIER statement . In our running example, the following creates the clusterer myclusterer and trains it on the table customers : > CREATE CLUSTERER myclusterer USING customers; After the clusterer is created, you can see the clusterer description using the DESCRIBE command : > DESCRIBE myclusterer;","title":"Clusterer Training"},{"location":"sclerasql/sqlextml/#clusterer-application","text":"The clusterer training learns a function that computes the cluster for a data instance given the values of all the feature columns. Applying the clusterer on a table involves computing this function on each row of the table. This feature introduces a new table expression with the following syntax: table_expression CLUSTERED WITH clusterer_name ( cluster_column_name ) In this expression, the clusterer with the name specified by clusterer_name is used to assign clusters to the rows in the result of the table expression table_expression , which must include all the feature columns present in the data on which the clusterer was trained (it can include additional columns). The result contains one row per input row. Each row contains all the columns in the input table_expressions and a new column, with the name as specified in cluster_column_name , containing the clusterer output. Continuing our example, the clusterer can be applied to the table used to train the clusterer ( customers in the above example), or to any other table or output of a query, as long as it includes all the feature columns of the training data. We apply the clusterer to the table customers as follows: > customers CLUSTERED WITH myclusterer(clusterid); The output is a table with all columns in customers , and a new column clusterid which, for each row, contains the id of the cluster (an integer) assigned to that row. This expression can be used in a query just like a table or a view. For instance, the following query gives the count of the customers in each cluster. > SELECT clusterid, COUNT(*) FROM (customers CLUSTERED WITH myclusterer(clusterid)) GROUP BY clusterid;","title":"Clusterer Application"},{"location":"sclerasql/sqlextml/#extended-syntax-for-using-specific-libraries-and-algorithms","text":"The classifier/clusterer syntax above is agnostic of the underlying library. Sclera uses Weka as the default library, and specific classification/clustering algorithms as default. The default library is specified in the configuration file , and can be changed if required to an alternative supported library and the algorithms therein. The default library can be overriden by explicitly mentioning the library (currently, WEKA ) in the CREATE statements. Moreover, you can select the specific algorithms to use for classification/clustering, and even provide the parameters. The algorithm names and parameters depend on the specific library. Sclera does not interpret the specified algorithm parameters, and merely passes them on to the appropriate APIs of the chosen library. For instance, the following uses FOOBARML as the underlying library for creating the classifier (instead of the default, as earlier ): > CREATE FOOBARML CLASSIFIER myclassifier(isinterested) USING survey; (The above assumes that a plugin for FOOBAR has been installed.) The following specifies the use of SIMPLEKMEANS algorithm for clustering in Weka: > CREATE WEKA CLUSTERER(\"SIMPLEKMEANS\") myclusterer USING customers; To further specify cluster counts as 3 (overriding the default 2): > CREATE WEKA CLUSTERER(\"SIMPLEKMEANS\", \"-N 3\") myclusterer USING customers; The extended formal syntax for classification/clustering, incorporating these overrides, is discussed below.","title":"Extended Syntax for Using Specific Libraries and Algorithms"},{"location":"sclerasql/sqlextml/#extended-syntax-for-classification","text":"The extended formal syntax for creating a classifier is as follows: CREATE [ { TEMPORARY | TEMP } ] [ library_name ] CLASSIFIER [ ( algorithm_name [ , algorithm_options ] ) ] classifier_name ( target_column_name ) USING table_expression The optional library_name specifies the machine learning library to use for the task. If not specified, the WEKA is used (this default can be modified using the sclera.service.default.mlservice configuration parameter ). The optional algorithm_name identifies the algorithm to be used in training the classifier. The following are supported: for WEKA : J48 ( documentation ) HOEFFDINGTREE ( documentation ) LMT ( documentation ) M5P ( documentation ) RANDOMFOREST ( documentation ) REPTREE ( documentation ) CLASSIFICATIONVIAREGRESSION ( documentation ) DECISIONTABLE ( documentation ) M5RULES ( documentation ) ONER ( documentation ) LOGISTIC ( documentation ) NAIVEBAYES ( documentation ) If not specified, J48 (for WEKA ) is used as the default. The optional algorithm_options provides the configuration options (parameters) for the algorithm identified by algorithm_name . These options are passed as a single string, just as in a command line. Please refer to the Weka documentation the respective algorithms, linked above, for details of the accepted options and defaults. If not specified, the default parameters for the specified algorithm are used. Remaining parameters are as in the abridged syntax discussed earlier.","title":"Extended Syntax for Classification"},{"location":"sclerasql/sqlextml/#extended-syntax-for-clustering","text":"The extended formal syntax for creating an clusterer is as follows: CREATE [ { TEMPORARY | TEMP } ] [ library_name ] CLUSTERER [ ( algorithm_name [ , algorithm_options ] ) ] clusterer_name USING table_expression The optional library_name specifies the machine learning library to use for the task. In the current version, only WEKA is accepted as Sclera currently only interfaces with Weka for clustering. If not specified, WEKA is used (this default can be modified using the sclera.service.default.mlservice configuration parameter ). The optional algorithm_name identifies the algorithm to be used in training the clusterer. The following are supported for WEKA : SIMPLEKMEANS ( documentation ) COBWEB ( documentation ) EM ( documentation ) FARTHESTFIRST ( documentation ) HIERARCHICAL ( documentation ) If not specified, SIMPLEKMEANS is used as the default. The optional algorithm_options provides the configuration options (parameters) for the algorithm identified by algorithm_name . These options are passed as a single string, just as in a command line. Please refer to the Weka documentation the respective algorithms, linked above, for details of the accepted options and defaults. If not specified, the default parameters for the specified algorithm are used. Remaining parameters are as in the abridged syntax discussed earlier.","title":"Extended Syntax for Clustering"},{"location":"sclerasql/sqlextordered/","text":"Ordered data is data that has an underlying sort order -- examples include website clickstreams, tweets, stock tickers, system logs, and so on. The sort order is typically time (in which case, we also call it a time series ), but other orders are also possible. Such data is hard to analyze in standard SQL. Sclera provides expressive constructs to efficiently and flexibly analyze ordered data. As we shall see, these constructs are far more powerful than SQL window functions , and are also much simpler and intuitive to use; they not only obviate the use of expensive self-joins and nested queries in a majority of queries, but also enable queries that so far were really hard to express in standard SQL. Since evaluation of the constructs discussed in this section takes a single pass over the data and uses a bounded amount of memory irrespective of the number of rows processed (which means that no intermediate results are materialized), these constructs can be readily applied over streaming data as well. We describe these constructs in the sections below, and also illustrate their utility using a number of use cases. We first present the constructs that enable queries to access the prior rows of a base table or intermediate result, and define running aggregates over them . This generalizes the window functions of standard SQL. We then generalize the above, and show how to use regular expressions to form aggregation groups within a base table or an intermediate result , based on column values along with positional constraints. This generalizes the window functions as well as the GROUP BY clause of SQL. Accessing History Without Self-Joins \u25c4 Most operations on ordered data involve current as well as prior rows in a stream. SQL is good at accessing the current row, not so much in accessing the prior rows. Consider ticker data for a stock, one row per day, and say we need to find the opening prices for the days where the closing price in the previous session was 5% or more above the average closing price so far. In Sclera, this query can be specified as: SELECT T.ts, T.openprice FROM (ticker ORDER BY ts) T WHERE T[-1].closeprice >= T.avg(closeprice) * 1.05 The same query can be specified in standard SQL (not supported in Sclera) as: SELECT T.ts, T.openprice FROM ticker T WINDOW w AS (ORDER BY ts) WHERE LAG(closeprice) OVER w >= AVG(closeprice) OVER w * 1.05 In standard SQL, we only get \"first class\" access to T 's current row -- the earlier rows, and running aggregates across them, are available only through specialized functions. Also, the sort order of the rows in T is specified in a WINDOW clause that is separate from T . In Sclera, we get first class access to T 's current row, as well as all prior rows, through an optional index notation that is baked into the SQL syntax. Also, the running aggregates are associated with T which represents both the sequence of rows and their sort order. This leads to simpler and more readable queries, as is apparent from the example above. These constructs are described in further detail below. Table Alias as an Array of Rows \u25c4 Recall that a table alias in a standard SQL query identifies a table expression in the FROM clause . In standard SQL, each column reference is explicitly or implicitly associated with a table alias. Operationally, the table alias stands for the current row of the result of the table expression identified by the alias (hereafter called the intermediate result associated with the table alias). Sclera generalizes the use of table alias by introducing an optional index to retrieve prior rows for the associated intermediate result. In other words, the history is considered a list, starting at the first emitted row, and ending at the previous emitted row. For instance, the following query returns, for each input row, the difference of the a day's openprice and the previous closeprice. SELECT T.closeprice - T[-1].openprice FROM (ticker ORDER BY ts) T The use of negative indexes as offsets from the end of the list is borrowed from Python . For the \"previous row\" of T to be well defined in the above query, the intermediate result associated with T needs to be ordered -- since the data here is coming from a base table on disk, an ORDER BY ts is needed; this is not needed if the input is already sorted . The index is optional, and can be an arbitrary integer. Specifically, given the table alias T : T represents the current row (this is consistent with standard SQL ) T[0] is the first row, T[1] is the second row, and so on. T[-1] is the previous row, T[-2] is one previous to that, and so on. If the index is out of range, the corresponding values are taken as NULL . As an example, consider the table FOO : > FOO; ---+--- A | B ---+--- 1 | X 2 | Y 3 | Z ---+--- (3 rows) Then, the query > SELECT T.A AS A, T[-2].B AS PPREVB, T[-1].B AS PREVB, T.B AS B, T[0].B AS FIRSTB, T[1].B AS SECONDB, T[2].B AS THIRDB FROM (FOO ORDER BY A) T; gives the following result: ---+--------+-------+---+--------+---------+-------- A | PPREVB | PREVB | B | FIRSTB | SECONDB | THIRDB ---+--------+-------+---+--------+---------+-------- 1 | NULL | NULL | X | X | NULL | NULL 2 | NULL | X | Y | X | Y | NULL 3 | X | Y | Z | X | Y | Z ---+--------+-------+---+--------+---------+-------- (3 rows) In the above: The column B is the input column B . The column PREVB is the input column B with a lag of one row. The column PPREVB is the input column B with a lag of two rows. The column FIRSTB contains the value of B in the first input row. The column SECONDB contains the value of B in the second input row. It is NULL when the input so far has less than two rows. The column THIRDB contains the value of B in the third input row. It is NULL when the input so far has less than three rows. The indexed table aliases can be used in any expression in the scope of the alias -- in SELECT , WHERE , GROUP BY , HAVING or ORDER BY clauses -- they are accepted wherever the regular table alias is in standard SQL. Running Aggregates \u25c4 The table alias can also be used to retrieve the running aggregates over the result of the table or subquery associated with the alias. As an example, again consider the table FOO : > FOO; ---+--- A | B ---+--- 1 | X 2 | Y 3 | Z ---+--- (3 rows) The query > SELECT T.A AS A, T.SUM(A) AS SUMA FROM (FOO ORDER BY A) T; gives the following result: ---+------ A | SUMA ---+------ 1 | 1 2 | 3 3 | 6 ---+------ (3 rows) Formally, the syntax of the running aggregate is: table_alias.aggregate_function ( aggr_params ) where: table_alias identifies a table expression in the FROM clause aggregate_function is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias . Indexed Table Alias and Running Aggregates on Partitioned Input \u25c4 The input rows can be partitioned by specifying a set of columns in a PARTITION BY clause. The history is then maintained independently for each partition. Given the table alias T : T represents the current row, as earlier and consistent with standard SQL . T[0] is the first row with the same values of the partition columns as the current row, T[1] is the second such row, and so on. T[-1] is the previous row with the same values of the partition columns as the current row, T[-2] is such a row previous to that, and so on. As an example, consider the table BAR : > BAR; ---+---+--- A | B | C ---+---+--- 1 | X | 0 2 | Z | 1 3 | Y | 0 5 | Z | 0 4 | Y | 1 6 | X | 1 ---+---+--- (6 rows) Then, the query > SELECT T.A AS A, T.C AS C, T[-2].B AS PPREVB, T[-1].B AS PREVB, T.B AS B, T[0].B AS FIRSTB, T[1].B AS SECONDB, T[2].B AS THIRDB FROM (BAR ORDER BY A) T PARTITION BY C; gives the following result: ---+---+--------+-------+---+--------+---------+-------- A | C | PPREVB | PREVB | B | FIRSTB | SECONDB | THIRDB ---+---+--------+-------+---+--------+---------+-------- 1 | 0 | NULL | NULL | X | X | NULL | NULL 2 | 1 | NULL | NULL | Z | Z | NULL | NULL 3 | 0 | NULL | X | Y | X | Y | NULL 4 | 0 | X | Y | Z | X | Y | Z 5 | 1 | NULL | Z | Y | Z | Y | NULL 6 | 1 | Z | Y | X | Z | Y | X ---+---+--------+-------+---+--------+---------+-------- (6 rows) This result is computed using the same logic as earlier , but separately for the two partitions identified by C = 0 and C = 1 respectively. Similarly, running aggregates on a partitioned input are computed independently for each partition. The query > SELECT T.A AS A, T.C AS C, T.SUM(A) AS SUMA FROM (BAR ORDER BY A) T PARTITION BY C; gives the following result: ---+---+------ A | C | SUMA ---+---+------ 1 | 0 | 1 2 | 1 | 2 3 | 0 | 4 4 | 0 | 8 5 | 1 | 7 6 | 1 | 13 ---+---+------ (6 rows) Partitioning is useful, for instance, when we need to analyze rows for a particular visitor in a multi-visitor clickstream. Pattern Matching with MATCH \u25c4 Regular expressions are commonplace mechanism for text search, supported almost all text editors. Standard SQL uses regular expressions in the SIMILAR TO operator, used to match patterns over string values. In Sclera, regular expressions can also be used to match, query and aggregate a sequence of rows. The idea is to see a column of the input as a sequence of symbols -- just like text -- and match regular expression over this sequence. This allows us to \"parse\" the input sequence of rows and work with them at a granularity that is extremely tough to emulate in standard SQL. The regular expression is matched progressively with the incoming sequence of tuples. Specifically, a match occurs at a row when a segment of rows upto that row match the regular expression. Multiple segments upto the row may match the regular expression -- in this case, the longest matching segment is taken as the match and the others are ignored. As soon as a match occurs, a row is emitted to the output. The output rows are constructed by evaluating aggregates on the subsequence of rows matching the respective labels in the regular expression. We illustrate with a number of examples. Pattern Matching Examples \u25c4 Consider the clickstream data for an e-commerce site. For simplicity, we assume that the data is for a single visitor (we will remove this constraint soon ). The data, simplified to focus on the ideas below, is as follows: > clicks; -----------+---------- VISITTIME | PAGETYPE -----------+---------- 10:21:03 | login 10:22:09 | search 10:24:39 | prodview 10:27:14 | logout 11:01:22 | login 11:02:33 | prodview 11:04:09 | search 11:05:47 | prodview 11:07:19 | checkout 11:09:51 | prodview 11:13:21 | logout -----------+---------- (11 rows) For this data, let us define a \"session\" as the segment starting at a \"login\" upto the following \"logout\". This is an extremely simplified clickstream -- but this simplification helps us to focus on the features being discussed in the example queries below. Example 1 Suppose we want to add a new column sessionstart to each row, containing the start time of the session to which that row belongs. This is a straightforward task -- but in standard SQL, will need a complex query with nested subqueries. With Sclera, the query is as simple as: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM (clicks ORDER BY visittime) MATCH \"login.(prodview | search | checkout | logout)*\" ON pagetype The query progressively matches the regular expression login.(prodview | search | checkout | logout)* against the sequence of values in the column pagetype , and emits a row for each match. Note that this regular expression matches every prefix of a session starting at its \"login\" row. Every row in a session thus gets associated with the first row (i.e. the \"login\" row) of the session. The unqualified column visittime and pageType get values from the last row of the match, while login.visittime gets the value from the row that matches login in the match. Notice how the symbols in the regular expression also assume the role of a table_alias representing the subsequence of rows that match that symbol. The result of the query is, as expected: -----------+----------+-------------- VISITTIME | PAGETYPE | SESSIONSTART -----------+----------+-------------- 10:21:03 | login | 10:21:03 10:22:09 | search | 10:21:03 10:24:39 | prodview | 10:21:03 10:27:14 | logout | 10:21:03 11:01:22 | login | 11:01:22 11:02:33 | prodview | 11:01:22 11:04:09 | search | 11:01:22 11:05:47 | prodview | 11:01:22 11:07:19 | checkout | 11:01:22 11:09:51 | prodview | 11:01:22 11:13:21 | logout | 11:01:22 -----------+----------+-------------- (11 rows) The ON pagetype qualifier in the query above specifies that the values in the column pagetype are to be used used to label the input rows. This means that if we want a (sub)expression specifying \"all labels expect login \", as in the above queries, we need to know all the values the column pagetype can possibly take -- in a real scenario, this might be a large set of values, might not be available, or might be hard to compute. One workaround is to use a CASE expression to generate the labels and apply MATCH on the generated labels. The query can be rewritten as: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM ( SELECT visittime, pagetype, CASE pagetype WHEN \"login\" THEN \"login\" ELSE \"other\" END AS pagelabel FROM clicks ORDER BY visittime ) MATCH \"login.other*\" ON pagelabel; Alternatively, we can use the extended ON clause, as follows: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM (clicks ORDER BY visittime) MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; The extended ON clause performs the same function as the CASE statement above for this query; later , we will see that it is a bit more general. The same idea can be applied to match over numeric fields -- generate discrete-valued labels based on a condition and match over the generated labels. Example 2 In this example, we want to associate the sequence number of the session with each row, with the first session as 1 , the second session as 2 , and so on. In Sclera, we can easily compute this by matching all the sessions in a single regular expression, and assigning the session identifier as the number of logins seen so far. The query is as follows: > SELECT visittime, pagetype, login.count() AS sessionseq FROM (clicks ORDER BY visittime) MATCH \"(login.other*)+\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; and the result is: -----------+----------+------------ VISITTIME | PAGETYPE | SESSIONSEQ -----------+----------+------------ 10:21:03 | login | 1 10:22:09 | search | 1 10:24:39 | prodview | 1 10:27:14 | logout | 1 11:01:22 | login | 2 11:02:33 | prodview | 2 11:04:09 | search | 2 11:05:47 | prodview | 2 11:07:19 | checkout | 2 11:09:51 | prodview | 2 11:13:21 | logout | 2 -----------+----------+------------ (11 rows) Example 3 In this example, we want to find, for each session in which the visitor does a \"checkout\", the number of product views by a visitor between the \"login\" and the \"checkout\". Again, the standard SQL will be extremely complex. In Sclera, the query works out to far simpler: > SELECT login.visittime as sessionstart, prodview.count() AS prodviews FROM (clicks ORDER BY visittime) MATCH \"login.(prodview | search)*.checkout\" ON pagetype; As in the previous query, this query progressively matches the regular expression login.(prodview | search)*.checkout against the sequence of values in the column pagetype . This regular expression, however, only matches segments starting at a \"login\" row and ending at a \"checkout\" row. Each match associates each symbol in the regular expression with the subsequence of rows matching the symbol; the symbol becomes the table alias for the associated rows, and can be used to aggregate the rows as with regular table aliases . The expression prodview.count() thus counts the \"prodview\" rows that lie between the \"login\" row and the \"checkout\" row. The result is: --------------+----------- SESSIONSTART | PRODVIEWS --------------+----------- 11:01:22 | 2 --------------+----------- (1 row) Since we ignore the \"search\" rows, we can eliminate them in the input, getting the equivalent query: > SELECT login.visittime as sessionstart, prodview.count() AS prodviews FROM (clicks WHERE pagetype <> \"search\" ORDER BY visittime) MATCH \"login.prodview*.checkout\" ON pagetype; Example 4 Sometimes, we may need the same value in the column being matched to be represented by multiple symbols in the regular expression -- this is needed to differentiate rows with the same value of the column based on their positional context. In this example, we want to know the number of product views before and after a checkout within a session. Since the query differentiates rows with the same value of the matched column based on the positional context, we need a mechanism to associate multiple symbols with rows. This is achieved by the extended ON clause used earlier in Example 1 , and is illustrated in the query below: > SELECT login.visittime as sessionstart, before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"login.(before | search)*.checkout.(after | search)*.logout\" ON pagetype WHEN \"prodview\" THEN (\"before\", \"after\"); The WHEN clause specifies \"before\" and \"after\" as labels for \"prodview\" rows (the label \"prodview\" is not recognized anymore). The other rows retain their default label. In a match, the \"prodview\" rows appearing before a checkout row get assigned the label \"before\", and \"prodview\" rows appearing after a checkout row get assigned the label \"after\". As earlier, these labels can be used to aggregate upon the associated rows -- thus, before.count() is the number of product views between \"login\" and \"checkout\", while after.count() is the number of product views between \"checkout\" and \"logout\". The result of the query is: --------------+-------------+------------ SESSIONSTART | VIEWSBEFORE | VIEWSAFTER --------------+-------------+------------ 11:01:22 | 2 | 1 --------------+-------------+------------ (1 row) Again, since \"search\" rows are ignored in the above query, we can safely eliminate them in the input, getting the equivalent query: > SELECT login.visittime as sessionstart, before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks WHERE pagetype <> \"search\" ORDER BY visittime) MATCH \"login.before*.checkout.after*.logout\" ON pagetype WHEN \"prodview\" THEN (\"before\", \"after\"); Example 5 Sclera also supports start and end anchors in the regular expression. The start-anchor restricts the match to a prefix of the input row sequence, while the end-anchor restricts the match to a suffix of the input row sequence. The following query returns the number of searches, across all sessions, before the first checkout by a visitor: > SELECT search.count() AS searches FROM (clicks WHERE pagetype in (\"search\", \"checkout\") ORDER BY visittime) MATCH \"^search*.checkout\" ON pagetype; ---------- SEARCHES ---------- 2 ---------- (1 row) Similarly, the following query returns the number of searches, across all sessions, after the last checkout by a visitor: > SELECT search.count() AS searches FROM (clicks WHERE pagetype in (\"search\", \"checkout\") ORDER BY visittime) MATCH \"checkout.search*$\" ON pagetype; ---------- SEARCHES ---------- 0 ---------- (1 row) Note that a query with an end-anchor can only emit the output after consuming all the input. The two anchors can be used together. The following query counts the non-search rows before and after the last search. > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 5 | 4 -------------+------------ (1 row) If we want to include the counts for \"search\" rows as well in before , we can use a generalized qualifier: > SELECT LABEL(before, search).count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 7 | 4 -------------+------------ (1 row) Of course, since the labels \"search\" and \"before\" are mutually exclusive, we could also have added before.count() and search.count() to get the same result. > SELECT before.count() + search.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 7 | 4 -------------+------------ (1 row) If we want the symbol before to include the searches before the last, we can say: > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^before*.search.after*$\" ON pagetype WHEN \"search\" THEN (\"before\", \"search\") ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 6 | 4 -------------+------------ (1 row) The difference with the previous query is that the last search is not included in the VIEWSBEFORE above. Example 6 Sclera also supports wild cards using the ALL clause in the extended ON syntax. The following query outputs, for each \"search\" row, the pagetype of previous and next rows, if any. > SELECT search.visittime, prevpg.pagetype AS prevtype, nextpg.pagetype AS nexttype FROM (clicks ORDER BY visittime) MATCH \"prevpg.search.nextpg\" ON pagetype ALL(prevpg, nextpg) -----------+----------+---------- VISITTIME | PREVTYPE | NEXTTYPE -----------+----------+---------- 10:22:09 | login | prodview 11:04:09 | prodview | prodview -----------+----------+---------- (2 rows) The ALL labels apply to all rows, in addition to the labels assigned by the ON / WHEN / THEN / ELSE clauses. For instance, in the last query in the previous example , notice that the symbol \"before\" was assigned to all rows. Rather than include \"before\" in all THEN and ELSE lists, we can say: > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^before*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE \"after\" ALL \"before\"; -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 6 | 4 -------------+------------ (1 row) MATCH on Partitioned Input \u25c4 The input can be partitioned on one or more columns before applying MATCH , using a PARTITION BY clause. The regular expression is then matched independently on the rows within each partition. The examples in the previous section were based on clickstream data of a single visitor. Let us now assume that the data has multiple visitors. In the following table a new column visitorid contains the identifier of the visitor for each row. > vclicks; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 1 | 10:21:03 | login 1 | 10:24:39 | prodview 1 | 10:27:14 | logout 2 | 10:21:04 | login 2 | 10:22:10 | search 2 | 10:27:15 | logout 2 | 11:01:22 | login 1 | 11:01:23 | login 2 | 11:02:33 | prodview 2 | 11:04:10 | search 2 | 11:05:47 | prodview 1 | 11:05:48 | prodview 2 | 11:07:19 | checkout 2 | 11:09:52 | prodview 2 | 11:13:21 | logout 1 | 11:13:22 | logout -----------+-----------+---------- (16 rows) For instance, the query in Example 1 can be rewritten as: > SELECT visitorid, visittime, pagetype, login.visittime AS sessionstart FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; -----------+-----------+----------+-------------- VISITORID | VISITTIME | PAGETYPE | SESSIONSTART -----------+-----------+----------+-------------- 2 | 10:21:04 | login | 10:21:04 1 | 10:21:03 | login | 10:21:03 1 | 10:24:39 | prodview | 10:21:03 2 | 10:22:10 | search | 10:21:04 2 | 10:27:15 | logout | 10:21:04 1 | 10:27:14 | logout | 10:21:03 2 | 11:01:22 | login | 11:01:22 2 | 11:02:33 | prodview | 11:01:22 2 | 11:04:10 | search | 11:01:22 1 | 11:01:23 | login | 11:01:23 2 | 11:05:47 | prodview | 11:01:22 2 | 11:07:19 | checkout | 11:01:22 2 | 11:09:52 | prodview | 11:01:22 1 | 11:05:48 | prodview | 11:01:23 2 | 11:13:21 | logout | 11:01:22 1 | 11:13:22 | logout | 11:01:23 -----------+-----------+----------+-------------- (16 rows) The evaluation now builds a MATCH processor for each unique visitorid -- each processor only sees the rows that belong to the same visitorid . The memory overhead thus depends upon the number of unique visitors. If the data contains a large number of visitors, it may be more efficient to sort the input on ( visitorid , visittime ) -- this will ensure that only one visitor's data is processed at a time, making the memory overhead of MATCH independent of the number of input rows. > SELECT visitorid, visittime, pagetype, login.visittime AS sessionstart FROM (vclicks ORDER BY visitorid, visittime) PARTITION BY visitorid MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; -----------+-----------+----------+-------------- VISITORID | VISITTIME | PAGETYPE | SESSIONSTART -----------+-----------+----------+-------------- 1 | 10:21:03 | login | 10:21:03 1 | 10:24:39 | prodview | 10:21:03 1 | 10:27:14 | logout | 10:21:03 1 | 11:01:23 | login | 11:01:23 1 | 11:05:48 | prodview | 11:01:23 1 | 11:13:22 | logout | 11:01:23 2 | 10:21:04 | login | 10:21:04 2 | 10:22:10 | search | 10:21:04 2 | 10:27:15 | logout | 10:21:04 2 | 11:01:22 | login | 11:01:22 2 | 11:02:33 | prodview | 11:01:22 2 | 11:04:10 | search | 11:01:22 2 | 11:05:47 | prodview | 11:01:22 2 | 11:07:19 | checkout | 11:01:22 2 | 11:09:52 | prodview | 11:01:22 2 | 11:13:21 | logout | 11:01:22 -----------+-----------+----------+-------------- (16 rows) MATCH Syntax \u25c4 This section introduced a new table expression with the following syntax: table_expression [ PARTITION BY ( partn_columns ) ] MATCH regular_expression [ON labeler] where: table_expression is an arbitrary table expression partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified, the result of table_expression is partitioned on this set of columns; the matching happens independently on the rows within each partition. regular_expression , with: alphabet: the labels used by the labeler to tag the input rows operators: \" . \" (concatenation), \" | \" (disjunction), \" * \" (kleene star), \" + \" (kleene plus), and \" ? \" (option). Used to compose more complex regular expressions from the alphabet. anchors: a start anchor \" ^ \" or an end anchor \" $ \". Required when we need only a prefix or a suffix match, respectively, and not all the matches as is the default. Note that: In the syntax above, the parenthesis on partn_colums can be omitted when the list is a singleton. When the regular_expression contains only one symbol, the labeler specification is optional; when the labeler is not specified, each input row will be labeled with the symbol in the regular expresion. Labeler Syntax \u25c4 The labeler tags each input row with a set of labels. The labeler is specified with a label_col , optional multiple WHEN / THEN clauses with an optional ELSE clause, and an optional ALL clause. label_col [ WHEN ( when_values ) THEN ( then_labels ) [ WHEN ... ] [ ELSE ( else_labels ) ] ] [ ALL ( all_labels ) ] where: label_col is a column in the result of table_expression mentioned in the MATCH syntax above. when_values is a comma-separated list of values of the same SQL type as label_col (to be extra sure in case of numeric types, we can use explicit casts) then_labels , else_labels and all_labels are comma-separated lists of labels In the syntax above, the parenthesis on lists can be omitted when the list is a singleton. Consider a row, and let the value of the column label_col in the row be X . The set of labels that tag the row are determined as follows: Case 1: ALL clause is not present When WHEN / THEN / ELSE clauses are not present, the row is labeled with X . If the WHEN / THEN clauses are present, and X is present in at least one when_values list, then the row is labeled by the union of the then_labels associated with the when_values that contain X . If the WHEN / THEN clauses are present, but X is not present in any when_values , then: If ELSE clause is present, the row is labeled with the else_labels . If ELSE clause is not present, the row is labeled with an empty set. Case 2: ALL clause is present The row is labeled with the union of all_labels and the label set computed as in Case 1 above. Accessing Matched Subsequences as an Array of Rows \u25c4 Recall our earlier discussion on how Sclera enables a table alias can be interpreted as an array of rows . The MATCH construct contains a generalization of the same -- we can interpret any symbol appearing in a regular expression (which represents a subsequence) as an array as well. As a simple example, the query on the table clicks : > SELECT T.*, (T.visittime - T[0].visittime)::INT AS timediff FROM (clicks ORDER BY visittime) T; can also be written as: > SELECT T.*, (T.visittime - T[0].visittime)::INT AS timediff FROM (clicks ORDER BY visittime) MATCH \"T+\"; In the first query, the table alias T is interpreted as an array over the entire prior sequence of input rows. In the second query, since the regular expression \" T+ \" contains only one symbol T , the default labeler labels each input row with T . The regular expression \" T+ \" matches each non-empty prefix of the input sequence -- in other words, \" T+ \" matches the entire history at any point. The two queries above are therefore equivalent, and have the output: -----------+----------+---------- VISITTIME | PAGETYPE | TIMEDIFF -----------+----------+---------- 10:21:03 | login | 0 10:22:09 | search | 66000 10:24:39 | prodview | 216000 10:27:14 | logout | 371000 11:01:22 | login | 2419000 11:02:33 | prodview | 2490000 11:04:09 | search | 2586000 11:05:47 | prodview | 2684000 11:07:19 | checkout | 2776000 11:09:51 | prodview | 2928000 11:13:21 | logout | 3138000 -----------+----------+---------- (11 rows) The array construct with MATCH becomes more useful when used along with more complex regular expressions. For instance, the following query retrieves the second visit (i.e. the row at index 1 ) in each visitor in the table vclicks . > SELECT T[1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 2 | 10:22:10 | search 1 | 10:24:39 | prodview -----------+-----------+---------- (2 rows) Similarly, the following query retrieves the second-last visit (i.e. the row at index -1 ) in each visitor in the table vclicks . > SELECT T[-1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 2 | 11:09:52 | prodview 1 | 11:05:48 | prodview -----------+-----------+---------- (2 rows) We can also have rows at multiple indexes juxtaposed in the same row. > SELECT T[1].*, T[-1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+----------+-------------+-------------+------------ VISITORID | VISITTIME | PAGETYPE | VISITORID_1 | VISITTIME_1 | PAGETYPE_1 -----------+-----------+----------+-------------+-------------+------------ 2 | 10:22:10 | search | 2 | 11:09:52 | prodview 1 | 10:24:39 | prodview | 1 | 11:05:48 | prodview -----------+-----------+----------+-------------+-------------+------------ (2 rows) We can use arbitrary regular expressions on multiple symbols. Each symbol determines a subsequence that can be accessed as an array of rows. > SELECT visitorid, prodview[0].visittime AS firstprod, prodview.visittime AS lastprod, others[0].visittime AS firstothers, others.visittime AS lastothers FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"(prodview | others)*$\" ON pagetype WHEN \"prodview\" THEN \"prodview\" ELSE \"others\"; -----------+-----------+----------+-------------+------------ VISITORID | FIRSTPROD | LASTPROD | FIRSTOTHERS | LASTOTHERS -----------+-----------+----------+-------------+------------ 2 | 11:02:33 | 11:09:52 | 10:21:04 | 11:13:21 1 | 10:24:39 | 11:05:48 | 10:21:03 | 11:13:22 -----------+-----------+----------+-------------+------------ (2 rows) Scalability of Ordered Data Processing in Sclera \u25c4 At a first look, the computations in the constructs described in this section seem expensive, raising issues of scalability. In this section, we briefly mention the overheads, and assert that the evaluation is highly scalable -- in fact, the evaluation happens in a single pass on the input, and in bounded memory that is independent of the number of rows in the input without any external I/O. Use of a positive index for a column requires storing the value in the row at the specific offset for the duration of the query; the overhead of a positive index is thus the amount of memory needed to store the single value. Use of a negative index for a column, on the other hand, requires storing the previous values of the column upto the index. The overhead of a negative index thus depends upon the absolute value of the index. Since the query only allows constants as index, this overhead is bounded for a given query. The memory overhead of a running aggregate depends on the state being maintained incrementally by the aggregate -- this is a constant (i.e. independent of the number of rows in the input), with the singular exception of string_agg , in which case the overhead is linear in the number of input rows. When there is no PARTITION BY , or when the input is coming sorted on the partition columns, all rows of the same partition are processed together, before the rows of the next partition -- the overheads therefore are independent of the number of partitions as well. When the input is not sorted on the partition columns specified in the PARTITION BY clause, Sclera needs to process all the partitions simultaneously -- this implies a memory overhead poportional to the number of partitions. When the input is sorted on a subset of the partition columns, the number of partitions that need to be maintained concurrently reduce, and the memory overhead reduces proportionately.","title":"Processing Ordered Data"},{"location":"sclerasql/sqlextordered/#accessing-history-without-self-joins","text":"Most operations on ordered data involve current as well as prior rows in a stream. SQL is good at accessing the current row, not so much in accessing the prior rows. Consider ticker data for a stock, one row per day, and say we need to find the opening prices for the days where the closing price in the previous session was 5% or more above the average closing price so far. In Sclera, this query can be specified as: SELECT T.ts, T.openprice FROM (ticker ORDER BY ts) T WHERE T[-1].closeprice >= T.avg(closeprice) * 1.05 The same query can be specified in standard SQL (not supported in Sclera) as: SELECT T.ts, T.openprice FROM ticker T WINDOW w AS (ORDER BY ts) WHERE LAG(closeprice) OVER w >= AVG(closeprice) OVER w * 1.05 In standard SQL, we only get \"first class\" access to T 's current row -- the earlier rows, and running aggregates across them, are available only through specialized functions. Also, the sort order of the rows in T is specified in a WINDOW clause that is separate from T . In Sclera, we get first class access to T 's current row, as well as all prior rows, through an optional index notation that is baked into the SQL syntax. Also, the running aggregates are associated with T which represents both the sequence of rows and their sort order. This leads to simpler and more readable queries, as is apparent from the example above. These constructs are described in further detail below.","title":"Accessing History Without Self-Joins"},{"location":"sclerasql/sqlextordered/#table-alias-as-an-array-of-rows","text":"Recall that a table alias in a standard SQL query identifies a table expression in the FROM clause . In standard SQL, each column reference is explicitly or implicitly associated with a table alias. Operationally, the table alias stands for the current row of the result of the table expression identified by the alias (hereafter called the intermediate result associated with the table alias). Sclera generalizes the use of table alias by introducing an optional index to retrieve prior rows for the associated intermediate result. In other words, the history is considered a list, starting at the first emitted row, and ending at the previous emitted row. For instance, the following query returns, for each input row, the difference of the a day's openprice and the previous closeprice. SELECT T.closeprice - T[-1].openprice FROM (ticker ORDER BY ts) T The use of negative indexes as offsets from the end of the list is borrowed from Python . For the \"previous row\" of T to be well defined in the above query, the intermediate result associated with T needs to be ordered -- since the data here is coming from a base table on disk, an ORDER BY ts is needed; this is not needed if the input is already sorted . The index is optional, and can be an arbitrary integer. Specifically, given the table alias T : T represents the current row (this is consistent with standard SQL ) T[0] is the first row, T[1] is the second row, and so on. T[-1] is the previous row, T[-2] is one previous to that, and so on. If the index is out of range, the corresponding values are taken as NULL . As an example, consider the table FOO : > FOO; ---+--- A | B ---+--- 1 | X 2 | Y 3 | Z ---+--- (3 rows) Then, the query > SELECT T.A AS A, T[-2].B AS PPREVB, T[-1].B AS PREVB, T.B AS B, T[0].B AS FIRSTB, T[1].B AS SECONDB, T[2].B AS THIRDB FROM (FOO ORDER BY A) T; gives the following result: ---+--------+-------+---+--------+---------+-------- A | PPREVB | PREVB | B | FIRSTB | SECONDB | THIRDB ---+--------+-------+---+--------+---------+-------- 1 | NULL | NULL | X | X | NULL | NULL 2 | NULL | X | Y | X | Y | NULL 3 | X | Y | Z | X | Y | Z ---+--------+-------+---+--------+---------+-------- (3 rows) In the above: The column B is the input column B . The column PREVB is the input column B with a lag of one row. The column PPREVB is the input column B with a lag of two rows. The column FIRSTB contains the value of B in the first input row. The column SECONDB contains the value of B in the second input row. It is NULL when the input so far has less than two rows. The column THIRDB contains the value of B in the third input row. It is NULL when the input so far has less than three rows. The indexed table aliases can be used in any expression in the scope of the alias -- in SELECT , WHERE , GROUP BY , HAVING or ORDER BY clauses -- they are accepted wherever the regular table alias is in standard SQL.","title":"Table Alias as an Array of Rows"},{"location":"sclerasql/sqlextordered/#running-aggregates","text":"The table alias can also be used to retrieve the running aggregates over the result of the table or subquery associated with the alias. As an example, again consider the table FOO : > FOO; ---+--- A | B ---+--- 1 | X 2 | Y 3 | Z ---+--- (3 rows) The query > SELECT T.A AS A, T.SUM(A) AS SUMA FROM (FOO ORDER BY A) T; gives the following result: ---+------ A | SUMA ---+------ 1 | 1 2 | 3 3 | 6 ---+------ (3 rows) Formally, the syntax of the running aggregate is: table_alias.aggregate_function ( aggr_params ) where: table_alias identifies a table expression in the FROM clause aggregate_function is an aggregate function aggr_params is a comma-separated list of scalar expressions , all of whose column references are contained in the result of table_alias .","title":"Running Aggregates"},{"location":"sclerasql/sqlextordered/#indexed-table-alias-and-running-aggregates-on-partitioned-input","text":"The input rows can be partitioned by specifying a set of columns in a PARTITION BY clause. The history is then maintained independently for each partition. Given the table alias T : T represents the current row, as earlier and consistent with standard SQL . T[0] is the first row with the same values of the partition columns as the current row, T[1] is the second such row, and so on. T[-1] is the previous row with the same values of the partition columns as the current row, T[-2] is such a row previous to that, and so on. As an example, consider the table BAR : > BAR; ---+---+--- A | B | C ---+---+--- 1 | X | 0 2 | Z | 1 3 | Y | 0 5 | Z | 0 4 | Y | 1 6 | X | 1 ---+---+--- (6 rows) Then, the query > SELECT T.A AS A, T.C AS C, T[-2].B AS PPREVB, T[-1].B AS PREVB, T.B AS B, T[0].B AS FIRSTB, T[1].B AS SECONDB, T[2].B AS THIRDB FROM (BAR ORDER BY A) T PARTITION BY C; gives the following result: ---+---+--------+-------+---+--------+---------+-------- A | C | PPREVB | PREVB | B | FIRSTB | SECONDB | THIRDB ---+---+--------+-------+---+--------+---------+-------- 1 | 0 | NULL | NULL | X | X | NULL | NULL 2 | 1 | NULL | NULL | Z | Z | NULL | NULL 3 | 0 | NULL | X | Y | X | Y | NULL 4 | 0 | X | Y | Z | X | Y | Z 5 | 1 | NULL | Z | Y | Z | Y | NULL 6 | 1 | Z | Y | X | Z | Y | X ---+---+--------+-------+---+--------+---------+-------- (6 rows) This result is computed using the same logic as earlier , but separately for the two partitions identified by C = 0 and C = 1 respectively. Similarly, running aggregates on a partitioned input are computed independently for each partition. The query > SELECT T.A AS A, T.C AS C, T.SUM(A) AS SUMA FROM (BAR ORDER BY A) T PARTITION BY C; gives the following result: ---+---+------ A | C | SUMA ---+---+------ 1 | 0 | 1 2 | 1 | 2 3 | 0 | 4 4 | 0 | 8 5 | 1 | 7 6 | 1 | 13 ---+---+------ (6 rows) Partitioning is useful, for instance, when we need to analyze rows for a particular visitor in a multi-visitor clickstream.","title":"Indexed Table Alias and Running Aggregates on Partitioned Input"},{"location":"sclerasql/sqlextordered/#pattern-matching-with-match","text":"Regular expressions are commonplace mechanism for text search, supported almost all text editors. Standard SQL uses regular expressions in the SIMILAR TO operator, used to match patterns over string values. In Sclera, regular expressions can also be used to match, query and aggregate a sequence of rows. The idea is to see a column of the input as a sequence of symbols -- just like text -- and match regular expression over this sequence. This allows us to \"parse\" the input sequence of rows and work with them at a granularity that is extremely tough to emulate in standard SQL. The regular expression is matched progressively with the incoming sequence of tuples. Specifically, a match occurs at a row when a segment of rows upto that row match the regular expression. Multiple segments upto the row may match the regular expression -- in this case, the longest matching segment is taken as the match and the others are ignored. As soon as a match occurs, a row is emitted to the output. The output rows are constructed by evaluating aggregates on the subsequence of rows matching the respective labels in the regular expression. We illustrate with a number of examples.","title":"Pattern Matching with MATCH"},{"location":"sclerasql/sqlextordered/#pattern-matching-examples","text":"Consider the clickstream data for an e-commerce site. For simplicity, we assume that the data is for a single visitor (we will remove this constraint soon ). The data, simplified to focus on the ideas below, is as follows: > clicks; -----------+---------- VISITTIME | PAGETYPE -----------+---------- 10:21:03 | login 10:22:09 | search 10:24:39 | prodview 10:27:14 | logout 11:01:22 | login 11:02:33 | prodview 11:04:09 | search 11:05:47 | prodview 11:07:19 | checkout 11:09:51 | prodview 11:13:21 | logout -----------+---------- (11 rows) For this data, let us define a \"session\" as the segment starting at a \"login\" upto the following \"logout\". This is an extremely simplified clickstream -- but this simplification helps us to focus on the features being discussed in the example queries below. Example 1 Suppose we want to add a new column sessionstart to each row, containing the start time of the session to which that row belongs. This is a straightforward task -- but in standard SQL, will need a complex query with nested subqueries. With Sclera, the query is as simple as: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM (clicks ORDER BY visittime) MATCH \"login.(prodview | search | checkout | logout)*\" ON pagetype The query progressively matches the regular expression login.(prodview | search | checkout | logout)* against the sequence of values in the column pagetype , and emits a row for each match. Note that this regular expression matches every prefix of a session starting at its \"login\" row. Every row in a session thus gets associated with the first row (i.e. the \"login\" row) of the session. The unqualified column visittime and pageType get values from the last row of the match, while login.visittime gets the value from the row that matches login in the match. Notice how the symbols in the regular expression also assume the role of a table_alias representing the subsequence of rows that match that symbol. The result of the query is, as expected: -----------+----------+-------------- VISITTIME | PAGETYPE | SESSIONSTART -----------+----------+-------------- 10:21:03 | login | 10:21:03 10:22:09 | search | 10:21:03 10:24:39 | prodview | 10:21:03 10:27:14 | logout | 10:21:03 11:01:22 | login | 11:01:22 11:02:33 | prodview | 11:01:22 11:04:09 | search | 11:01:22 11:05:47 | prodview | 11:01:22 11:07:19 | checkout | 11:01:22 11:09:51 | prodview | 11:01:22 11:13:21 | logout | 11:01:22 -----------+----------+-------------- (11 rows) The ON pagetype qualifier in the query above specifies that the values in the column pagetype are to be used used to label the input rows. This means that if we want a (sub)expression specifying \"all labels expect login \", as in the above queries, we need to know all the values the column pagetype can possibly take -- in a real scenario, this might be a large set of values, might not be available, or might be hard to compute. One workaround is to use a CASE expression to generate the labels and apply MATCH on the generated labels. The query can be rewritten as: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM ( SELECT visittime, pagetype, CASE pagetype WHEN \"login\" THEN \"login\" ELSE \"other\" END AS pagelabel FROM clicks ORDER BY visittime ) MATCH \"login.other*\" ON pagelabel; Alternatively, we can use the extended ON clause, as follows: > SELECT visittime, pagetype, login.visittime AS sessionstart FROM (clicks ORDER BY visittime) MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; The extended ON clause performs the same function as the CASE statement above for this query; later , we will see that it is a bit more general. The same idea can be applied to match over numeric fields -- generate discrete-valued labels based on a condition and match over the generated labels. Example 2 In this example, we want to associate the sequence number of the session with each row, with the first session as 1 , the second session as 2 , and so on. In Sclera, we can easily compute this by matching all the sessions in a single regular expression, and assigning the session identifier as the number of logins seen so far. The query is as follows: > SELECT visittime, pagetype, login.count() AS sessionseq FROM (clicks ORDER BY visittime) MATCH \"(login.other*)+\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; and the result is: -----------+----------+------------ VISITTIME | PAGETYPE | SESSIONSEQ -----------+----------+------------ 10:21:03 | login | 1 10:22:09 | search | 1 10:24:39 | prodview | 1 10:27:14 | logout | 1 11:01:22 | login | 2 11:02:33 | prodview | 2 11:04:09 | search | 2 11:05:47 | prodview | 2 11:07:19 | checkout | 2 11:09:51 | prodview | 2 11:13:21 | logout | 2 -----------+----------+------------ (11 rows) Example 3 In this example, we want to find, for each session in which the visitor does a \"checkout\", the number of product views by a visitor between the \"login\" and the \"checkout\". Again, the standard SQL will be extremely complex. In Sclera, the query works out to far simpler: > SELECT login.visittime as sessionstart, prodview.count() AS prodviews FROM (clicks ORDER BY visittime) MATCH \"login.(prodview | search)*.checkout\" ON pagetype; As in the previous query, this query progressively matches the regular expression login.(prodview | search)*.checkout against the sequence of values in the column pagetype . This regular expression, however, only matches segments starting at a \"login\" row and ending at a \"checkout\" row. Each match associates each symbol in the regular expression with the subsequence of rows matching the symbol; the symbol becomes the table alias for the associated rows, and can be used to aggregate the rows as with regular table aliases . The expression prodview.count() thus counts the \"prodview\" rows that lie between the \"login\" row and the \"checkout\" row. The result is: --------------+----------- SESSIONSTART | PRODVIEWS --------------+----------- 11:01:22 | 2 --------------+----------- (1 row) Since we ignore the \"search\" rows, we can eliminate them in the input, getting the equivalent query: > SELECT login.visittime as sessionstart, prodview.count() AS prodviews FROM (clicks WHERE pagetype <> \"search\" ORDER BY visittime) MATCH \"login.prodview*.checkout\" ON pagetype; Example 4 Sometimes, we may need the same value in the column being matched to be represented by multiple symbols in the regular expression -- this is needed to differentiate rows with the same value of the column based on their positional context. In this example, we want to know the number of product views before and after a checkout within a session. Since the query differentiates rows with the same value of the matched column based on the positional context, we need a mechanism to associate multiple symbols with rows. This is achieved by the extended ON clause used earlier in Example 1 , and is illustrated in the query below: > SELECT login.visittime as sessionstart, before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"login.(before | search)*.checkout.(after | search)*.logout\" ON pagetype WHEN \"prodview\" THEN (\"before\", \"after\"); The WHEN clause specifies \"before\" and \"after\" as labels for \"prodview\" rows (the label \"prodview\" is not recognized anymore). The other rows retain their default label. In a match, the \"prodview\" rows appearing before a checkout row get assigned the label \"before\", and \"prodview\" rows appearing after a checkout row get assigned the label \"after\". As earlier, these labels can be used to aggregate upon the associated rows -- thus, before.count() is the number of product views between \"login\" and \"checkout\", while after.count() is the number of product views between \"checkout\" and \"logout\". The result of the query is: --------------+-------------+------------ SESSIONSTART | VIEWSBEFORE | VIEWSAFTER --------------+-------------+------------ 11:01:22 | 2 | 1 --------------+-------------+------------ (1 row) Again, since \"search\" rows are ignored in the above query, we can safely eliminate them in the input, getting the equivalent query: > SELECT login.visittime as sessionstart, before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks WHERE pagetype <> \"search\" ORDER BY visittime) MATCH \"login.before*.checkout.after*.logout\" ON pagetype WHEN \"prodview\" THEN (\"before\", \"after\"); Example 5 Sclera also supports start and end anchors in the regular expression. The start-anchor restricts the match to a prefix of the input row sequence, while the end-anchor restricts the match to a suffix of the input row sequence. The following query returns the number of searches, across all sessions, before the first checkout by a visitor: > SELECT search.count() AS searches FROM (clicks WHERE pagetype in (\"search\", \"checkout\") ORDER BY visittime) MATCH \"^search*.checkout\" ON pagetype; ---------- SEARCHES ---------- 2 ---------- (1 row) Similarly, the following query returns the number of searches, across all sessions, after the last checkout by a visitor: > SELECT search.count() AS searches FROM (clicks WHERE pagetype in (\"search\", \"checkout\") ORDER BY visittime) MATCH \"checkout.search*$\" ON pagetype; ---------- SEARCHES ---------- 0 ---------- (1 row) Note that a query with an end-anchor can only emit the output after consuming all the input. The two anchors can be used together. The following query counts the non-search rows before and after the last search. > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 5 | 4 -------------+------------ (1 row) If we want to include the counts for \"search\" rows as well in before , we can use a generalized qualifier: > SELECT LABEL(before, search).count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 7 | 4 -------------+------------ (1 row) Of course, since the labels \"search\" and \"before\" are mutually exclusive, we could also have added before.count() and search.count() to get the same result. > SELECT before.count() + search.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^(before | search)*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 7 | 4 -------------+------------ (1 row) If we want the symbol before to include the searches before the last, we can say: > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^before*.search.after*$\" ON pagetype WHEN \"search\" THEN (\"before\", \"search\") ELSE (\"before\", \"after\"); -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 6 | 4 -------------+------------ (1 row) The difference with the previous query is that the last search is not included in the VIEWSBEFORE above. Example 6 Sclera also supports wild cards using the ALL clause in the extended ON syntax. The following query outputs, for each \"search\" row, the pagetype of previous and next rows, if any. > SELECT search.visittime, prevpg.pagetype AS prevtype, nextpg.pagetype AS nexttype FROM (clicks ORDER BY visittime) MATCH \"prevpg.search.nextpg\" ON pagetype ALL(prevpg, nextpg) -----------+----------+---------- VISITTIME | PREVTYPE | NEXTTYPE -----------+----------+---------- 10:22:09 | login | prodview 11:04:09 | prodview | prodview -----------+----------+---------- (2 rows) The ALL labels apply to all rows, in addition to the labels assigned by the ON / WHEN / THEN / ELSE clauses. For instance, in the last query in the previous example , notice that the symbol \"before\" was assigned to all rows. Rather than include \"before\" in all THEN and ELSE lists, we can say: > SELECT before.count() AS viewsbefore, after.count() as viewsafter FROM (clicks ORDER BY visittime) MATCH \"^before*.search.after*$\" ON pagetype WHEN \"search\" THEN \"search\" ELSE \"after\" ALL \"before\"; -------------+------------ VIEWSBEFORE | VIEWSAFTER -------------+------------ 6 | 4 -------------+------------ (1 row)","title":"Pattern Matching Examples"},{"location":"sclerasql/sqlextordered/#match-on-partitioned-input","text":"The input can be partitioned on one or more columns before applying MATCH , using a PARTITION BY clause. The regular expression is then matched independently on the rows within each partition. The examples in the previous section were based on clickstream data of a single visitor. Let us now assume that the data has multiple visitors. In the following table a new column visitorid contains the identifier of the visitor for each row. > vclicks; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 1 | 10:21:03 | login 1 | 10:24:39 | prodview 1 | 10:27:14 | logout 2 | 10:21:04 | login 2 | 10:22:10 | search 2 | 10:27:15 | logout 2 | 11:01:22 | login 1 | 11:01:23 | login 2 | 11:02:33 | prodview 2 | 11:04:10 | search 2 | 11:05:47 | prodview 1 | 11:05:48 | prodview 2 | 11:07:19 | checkout 2 | 11:09:52 | prodview 2 | 11:13:21 | logout 1 | 11:13:22 | logout -----------+-----------+---------- (16 rows) For instance, the query in Example 1 can be rewritten as: > SELECT visitorid, visittime, pagetype, login.visittime AS sessionstart FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; -----------+-----------+----------+-------------- VISITORID | VISITTIME | PAGETYPE | SESSIONSTART -----------+-----------+----------+-------------- 2 | 10:21:04 | login | 10:21:04 1 | 10:21:03 | login | 10:21:03 1 | 10:24:39 | prodview | 10:21:03 2 | 10:22:10 | search | 10:21:04 2 | 10:27:15 | logout | 10:21:04 1 | 10:27:14 | logout | 10:21:03 2 | 11:01:22 | login | 11:01:22 2 | 11:02:33 | prodview | 11:01:22 2 | 11:04:10 | search | 11:01:22 1 | 11:01:23 | login | 11:01:23 2 | 11:05:47 | prodview | 11:01:22 2 | 11:07:19 | checkout | 11:01:22 2 | 11:09:52 | prodview | 11:01:22 1 | 11:05:48 | prodview | 11:01:23 2 | 11:13:21 | logout | 11:01:22 1 | 11:13:22 | logout | 11:01:23 -----------+-----------+----------+-------------- (16 rows) The evaluation now builds a MATCH processor for each unique visitorid -- each processor only sees the rows that belong to the same visitorid . The memory overhead thus depends upon the number of unique visitors. If the data contains a large number of visitors, it may be more efficient to sort the input on ( visitorid , visittime ) -- this will ensure that only one visitor's data is processed at a time, making the memory overhead of MATCH independent of the number of input rows. > SELECT visitorid, visittime, pagetype, login.visittime AS sessionstart FROM (vclicks ORDER BY visitorid, visittime) PARTITION BY visitorid MATCH \"login.other*\" ON pagetype WHEN \"login\" THEN \"login\" ELSE \"other\"; -----------+-----------+----------+-------------- VISITORID | VISITTIME | PAGETYPE | SESSIONSTART -----------+-----------+----------+-------------- 1 | 10:21:03 | login | 10:21:03 1 | 10:24:39 | prodview | 10:21:03 1 | 10:27:14 | logout | 10:21:03 1 | 11:01:23 | login | 11:01:23 1 | 11:05:48 | prodview | 11:01:23 1 | 11:13:22 | logout | 11:01:23 2 | 10:21:04 | login | 10:21:04 2 | 10:22:10 | search | 10:21:04 2 | 10:27:15 | logout | 10:21:04 2 | 11:01:22 | login | 11:01:22 2 | 11:02:33 | prodview | 11:01:22 2 | 11:04:10 | search | 11:01:22 2 | 11:05:47 | prodview | 11:01:22 2 | 11:07:19 | checkout | 11:01:22 2 | 11:09:52 | prodview | 11:01:22 2 | 11:13:21 | logout | 11:01:22 -----------+-----------+----------+-------------- (16 rows)","title":"MATCH on Partitioned Input"},{"location":"sclerasql/sqlextordered/#match-syntax","text":"This section introduced a new table expression with the following syntax: table_expression [ PARTITION BY ( partn_columns ) ] MATCH regular_expression [ON labeler] where: table_expression is an arbitrary table expression partn_columns is an optional comma-separated list of columns in the result of table_expression . When specified, the result of table_expression is partitioned on this set of columns; the matching happens independently on the rows within each partition. regular_expression , with: alphabet: the labels used by the labeler to tag the input rows operators: \" . \" (concatenation), \" | \" (disjunction), \" * \" (kleene star), \" + \" (kleene plus), and \" ? \" (option). Used to compose more complex regular expressions from the alphabet. anchors: a start anchor \" ^ \" or an end anchor \" $ \". Required when we need only a prefix or a suffix match, respectively, and not all the matches as is the default. Note that: In the syntax above, the parenthesis on partn_colums can be omitted when the list is a singleton. When the regular_expression contains only one symbol, the labeler specification is optional; when the labeler is not specified, each input row will be labeled with the symbol in the regular expresion.","title":"MATCH Syntax"},{"location":"sclerasql/sqlextordered/#labeler-syntax","text":"The labeler tags each input row with a set of labels. The labeler is specified with a label_col , optional multiple WHEN / THEN clauses with an optional ELSE clause, and an optional ALL clause. label_col [ WHEN ( when_values ) THEN ( then_labels ) [ WHEN ... ] [ ELSE ( else_labels ) ] ] [ ALL ( all_labels ) ] where: label_col is a column in the result of table_expression mentioned in the MATCH syntax above. when_values is a comma-separated list of values of the same SQL type as label_col (to be extra sure in case of numeric types, we can use explicit casts) then_labels , else_labels and all_labels are comma-separated lists of labels In the syntax above, the parenthesis on lists can be omitted when the list is a singleton. Consider a row, and let the value of the column label_col in the row be X . The set of labels that tag the row are determined as follows: Case 1: ALL clause is not present When WHEN / THEN / ELSE clauses are not present, the row is labeled with X . If the WHEN / THEN clauses are present, and X is present in at least one when_values list, then the row is labeled by the union of the then_labels associated with the when_values that contain X . If the WHEN / THEN clauses are present, but X is not present in any when_values , then: If ELSE clause is present, the row is labeled with the else_labels . If ELSE clause is not present, the row is labeled with an empty set. Case 2: ALL clause is present The row is labeled with the union of all_labels and the label set computed as in Case 1 above.","title":"Labeler Syntax"},{"location":"sclerasql/sqlextordered/#accessing-matched-subsequences-as-an-array-of-rows","text":"Recall our earlier discussion on how Sclera enables a table alias can be interpreted as an array of rows . The MATCH construct contains a generalization of the same -- we can interpret any symbol appearing in a regular expression (which represents a subsequence) as an array as well. As a simple example, the query on the table clicks : > SELECT T.*, (T.visittime - T[0].visittime)::INT AS timediff FROM (clicks ORDER BY visittime) T; can also be written as: > SELECT T.*, (T.visittime - T[0].visittime)::INT AS timediff FROM (clicks ORDER BY visittime) MATCH \"T+\"; In the first query, the table alias T is interpreted as an array over the entire prior sequence of input rows. In the second query, since the regular expression \" T+ \" contains only one symbol T , the default labeler labels each input row with T . The regular expression \" T+ \" matches each non-empty prefix of the input sequence -- in other words, \" T+ \" matches the entire history at any point. The two queries above are therefore equivalent, and have the output: -----------+----------+---------- VISITTIME | PAGETYPE | TIMEDIFF -----------+----------+---------- 10:21:03 | login | 0 10:22:09 | search | 66000 10:24:39 | prodview | 216000 10:27:14 | logout | 371000 11:01:22 | login | 2419000 11:02:33 | prodview | 2490000 11:04:09 | search | 2586000 11:05:47 | prodview | 2684000 11:07:19 | checkout | 2776000 11:09:51 | prodview | 2928000 11:13:21 | logout | 3138000 -----------+----------+---------- (11 rows) The array construct with MATCH becomes more useful when used along with more complex regular expressions. For instance, the following query retrieves the second visit (i.e. the row at index 1 ) in each visitor in the table vclicks . > SELECT T[1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 2 | 10:22:10 | search 1 | 10:24:39 | prodview -----------+-----------+---------- (2 rows) Similarly, the following query retrieves the second-last visit (i.e. the row at index -1 ) in each visitor in the table vclicks . > SELECT T[-1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+---------- VISITORID | VISITTIME | PAGETYPE -----------+-----------+---------- 2 | 11:09:52 | prodview 1 | 11:05:48 | prodview -----------+-----------+---------- (2 rows) We can also have rows at multiple indexes juxtaposed in the same row. > SELECT T[1].*, T[-1].* FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"T+$\"; -----------+-----------+----------+-------------+-------------+------------ VISITORID | VISITTIME | PAGETYPE | VISITORID_1 | VISITTIME_1 | PAGETYPE_1 -----------+-----------+----------+-------------+-------------+------------ 2 | 10:22:10 | search | 2 | 11:09:52 | prodview 1 | 10:24:39 | prodview | 1 | 11:05:48 | prodview -----------+-----------+----------+-------------+-------------+------------ (2 rows) We can use arbitrary regular expressions on multiple symbols. Each symbol determines a subsequence that can be accessed as an array of rows. > SELECT visitorid, prodview[0].visittime AS firstprod, prodview.visittime AS lastprod, others[0].visittime AS firstothers, others.visittime AS lastothers FROM (vclicks ORDER BY visittime) PARTITION BY visitorid MATCH \"(prodview | others)*$\" ON pagetype WHEN \"prodview\" THEN \"prodview\" ELSE \"others\"; -----------+-----------+----------+-------------+------------ VISITORID | FIRSTPROD | LASTPROD | FIRSTOTHERS | LASTOTHERS -----------+-----------+----------+-------------+------------ 2 | 11:02:33 | 11:09:52 | 10:21:04 | 11:13:21 1 | 10:24:39 | 11:05:48 | 10:21:03 | 11:13:22 -----------+-----------+----------+-------------+------------ (2 rows)","title":"Accessing Matched Subsequences as an Array of Rows"},{"location":"sclerasql/sqlextordered/#scalability-of-ordered-data-processing-in-sclera","text":"At a first look, the computations in the constructs described in this section seem expensive, raising issues of scalability. In this section, we briefly mention the overheads, and assert that the evaluation is highly scalable -- in fact, the evaluation happens in a single pass on the input, and in bounded memory that is independent of the number of rows in the input without any external I/O. Use of a positive index for a column requires storing the value in the row at the specific offset for the duration of the query; the overhead of a positive index is thus the amount of memory needed to store the single value. Use of a negative index for a column, on the other hand, requires storing the previous values of the column upto the index. The overhead of a negative index thus depends upon the absolute value of the index. Since the query only allows constants as index, this overhead is bounded for a given query. The memory overhead of a running aggregate depends on the state being maintained incrementally by the aggregate -- this is a constant (i.e. independent of the number of rows in the input), with the singular exception of string_agg , in which case the overhead is linear in the number of input rows. When there is no PARTITION BY , or when the input is coming sorted on the partition columns, all rows of the same partition are processed together, before the rows of the next partition -- the overheads therefore are independent of the number of partitions as well. When the input is not sorted on the partition columns specified in the PARTITION BY clause, Sclera needs to process all the partitions simultaneously -- this implies a memory overhead poportional to the number of partitions. When the input is sorted on a subset of the partition columns, the number of partitions that need to be maintained concurrently reduce, and the memory overhead reduces proportionately.","title":"Scalability of Ordered Data Processing in Sclera"},{"location":"sclerasql/sqlexttext/","text":"Sclera can identify and extract entities, such as person and location names, from free-form text. This is done using the Sclera - Apache OpenNLP Connector which builds on the Apache OpenNLP library. While we may do not know the context in which these entities were mentioned in the text, this is still very useful as it enables you to identify related information from your database by doing joins with tables containing product information, and also filter and/or aggregate the messages based on location, persons mentioned, and so on. This feature introduces a new table expression with the following syntax: table_expression TEXT [ ( language_code ) ] EXTRACT( entity_type [, ...] ) IN input_column_name [ TO ( entity_column_name [, label_column_name ] ) ] The mandatory table_expression is a table expression The optional language_code specifies the language of the text; it can be \"da\" (for Danish), \"de\" (for German), \"en\" (for English), \"dl\" (for Dutch), \"pt\" (for Portuguese) or \"se\" (for Swedish). If not specified, it is taken to be \"en\" (i.e. English). The mandatory input_column_name is the name of a column in table_expression containing free-form text. This forms the input to the text extractors. Each entity_type in the mandatory comma-separated list specifies the type of entity to extract. The possible values for this parameter depends upon the available entity extraction models; see the setup notes for details. The optional entity_column_name is the name of a new column that contains the extracted entities. If not specified, Sclera assigns the name {input_column_name}_entity to the column. The optional label_column_name is the name of a new column that contains the labels of the extracted entities in entity_column_name ; label of an extraction is the entity_type associated with the extracted entity. If not specified, Sclera assigns the name {input_column_name}_label to the column. The result contains one row per entity found. Each row contains all the columns in the input table_expressions and the two new columns containing the extracted entity and its label respectively. As an example, consider a table messages containing customer messages, with columns custid containing the customer id, msgdate containing the message date, and msgcontent containing the content of the message from the customer. To identify the locations and persons in the messages in the column msgcontent in the table message : > messages TEXT EXTRACT(\"location\", \"person\") IN msgcontent TO (entity, entitylabel); The output of this expression is a table with all columns in messages , and two new columns entity and entitylabel which contain the extracted entity and its label ( \"location\" or \"person\" ) respectively. This table contains one row in the output for each entity discovered; the columns in messages are repeated in each row that contains an associated entity. The table expression can be used in a query just like a base table or a view. For instance, the following query joins the messages table with a stores table and find the unique number of messages for each store: > SELECT S.storeid, COUNT(distinct M.msgid) FROM (messages TEXT EXTRACT \"location\" IN msgcontent TO location) M JOIN stores S ON (M.location = S.location) GROUP BY S.storeid; Model Setup \u25c4 To be able to extract an entity_type in the language specified by language_code , the following files need to exist: \"$SCLERA_ASSETS/opennlp/{language_code}-sent.bin\" , containing the sentence detector model for the language \"$SCLERA_ASSETS/opennlp/{language_code}-ner-{entity_type}.bin\" , containing the extraction model for the entity type in the language In the above, $SCLERA_ASSETS is the directory given by the configuration parameter sclera.services.assetdir . For instance, to use a location model for English ( \"en\" ), the sentence detector file \"$SCLERA_ASSETS/opennlp/en-sent.bin\" and the extraction model file \"$SCLERA_ASSETS/opennlp/en-ner-location.bin\" need to exist. The text mining engine used by Sclera is Apache OpenNLP version 1.5 , and the pre-trained models are available for download . The available pre-trained models include person , location , organization , date , time , money and percentage models for Danish, German, English, Dutch, Portuguese and Swedish. Please download the required models and place them in the \"$SCLERA_ASSETS/opennlp\" directory. Model Accuracy \u25c4 Text extraction models are not perfect. The accuracy of an extraction model depends on how similar the document on which the extraction is being applied is to the set of documents used to train the extraction model. As such, the extraction may occasionally extract erroneous entities, or miss certain entities. If the accuracy with the available models on your data is not acceptable, you may want to generate your own models. New models can be generated on custom text collections using the Apache OpenNLP command line tools for training sentence detectors and entity extractors .","title":"Using Text Analytics"},{"location":"sclerasql/sqlexttext/#model-setup","text":"To be able to extract an entity_type in the language specified by language_code , the following files need to exist: \"$SCLERA_ASSETS/opennlp/{language_code}-sent.bin\" , containing the sentence detector model for the language \"$SCLERA_ASSETS/opennlp/{language_code}-ner-{entity_type}.bin\" , containing the extraction model for the entity type in the language In the above, $SCLERA_ASSETS is the directory given by the configuration parameter sclera.services.assetdir . For instance, to use a location model for English ( \"en\" ), the sentence detector file \"$SCLERA_ASSETS/opennlp/en-sent.bin\" and the extraction model file \"$SCLERA_ASSETS/opennlp/en-ner-location.bin\" need to exist. The text mining engine used by Sclera is Apache OpenNLP version 1.5 , and the pre-trained models are available for download . The available pre-trained models include person , location , organization , date , time , money and percentage models for Danish, German, English, Dutch, Portuguese and Swedish. Please download the required models and place them in the \"$SCLERA_ASSETS/opennlp\" directory.","title":"Model Setup"},{"location":"sclerasql/sqlexttext/#model-accuracy","text":"Text extraction models are not perfect. The accuracy of an extraction model depends on how similar the document on which the extraction is being applied is to the set of documents used to train the extraction model. As such, the extraction may occasionally extract erroneous entities, or miss certain entities. If the accuracy with the available models on your data is not acceptable, you may want to generate your own models. New models can be generated on custom text collections using the Apache OpenNLP command line tools for training sentence detectors and entity extractors .","title":"Model Accuracy"},{"location":"sclerasql/sqlintro/","text":"This section introduces the SQL constructs -- standard and proprietary -- supported by Sclera. Detailed documentation on these constructs, including the syntax and example usage, appear in the later sections; please follow the links in the descriptions below. The SQL constructs supported by Sclera come in the following flavors: Regular SQL constructs These standard constructs enable you to create, populate, query, update and delete base tables. Sclera supports a large subset of standard SQL. In addition, Sclera makes the language modern and succint by adding shortcuts to several commonly used constructs. See the detailed documentation for further information. SQL extensions for Cross-tabulation These constructs are used to convert data in multiple rows to columns in a single row ( PIVOT ), and vice-versa ( UNPIVOT ). The semantics of these functions is the same as in Oracle 11g and MS SQL Server 2008 , but with a slightly modified (in our opinion, simplified) syntax. See the detailed documentation for further information. SQL extensions for ad-hoc external data access These extensions enable you to access external data -- on-disk or streaming, local or over the internet -- from within SQL. See the detailed documentation for further information. Note that Sclera's connectivity to external data sources is not limited to the above -- Sclera includes an SDK that can be used to develop extensions for arbitrary external datasources. The Sclera Connector Development SDK document describes the SDK and includes working examples. SQL extensions for processing ordered data These extensions enable powerful constructs that simplify processing of ordered data. The features include immediate access to prior tuples and running aggregates, input splitting based on positional criteria, regular expression matches across rows, and much more -- available without any materialization and with bounded memory overheads. These features eliminate the need of self-joins in most queries, leading to orders of magnitude gains in query performance. Furthermore, given the single-pass processing with minimal memory overheads, these constructs can be used to process streaming data as well. See the detailed documentation for further information. SQL extensions for machine learning These extensions enable you to incorporate classification , clustering and association rule learning in your SQL queries. Sclera interfaces with well-established external libraries (currently Weka ) for the task; the query can optionally specify the specific library to use, the specific algorithm within the library and the associated parameter values. See the detailed documentation for further information. SQL extensions for text analytics This extension enables Sclera to identify and extract named entities from free-form text present in a table column. A simple example is extracting person names, product names and locations from customer tweets; the extracted information can be used by companies to join with the \"structured\" customer and product data; this enriched data can be analyzed using the extensions mentioned earlier for helpful insights. See the detailed documentation for further information.","title":"ScleraSQL Introduction"},{"location":"sclerasql/sqlmisc/","text":"This section lists the supported aggregate functions , data types and reserved words in Sclera' SQL. Aggregate Functions \u25c4 The aggregate functions supported in Sclera can be order-insensitive , which apply to both ordered and unordered data , or order-sensitive , which apply only to ordered data . Order-Insensitive Aggregate Functions \u25c4 These functions can be used in standard SQL aggregates as well as in running aggregates on ordered data (table adapted from PostgreSQL documentation ): Aggregate Argument Type Return Type Description avg(expression) any numeric type FLOAT the average (arithmetic mean) of all input values bool_and(expression) BOOLEAN BOOLEAN true if all input values are true, otherwise false bool_or(expression) BOOLEAN BOOLEAN true if at least one input value is true, otherwise false count(*) BIGINT number of input rows count(expression) any type BIGINT number of input rows for which the value of expression is not null every(expression) BOOLEAN BOOLEAN equivalent to bool_and max(expression) any numeric, string, or date/time type same as argument type maximum value of expression across all input values min(expression) any numeric, string, or date/time type same as argument type minimum value of expression across all input values sum(expression) any numeric type BIGINT for SMALLINT or INTEGER arguments, FLOAT for floating-point arguments sum of expression across all input values corr(Y, X) any numeric type FLOAT correlation coefficient covar_pop(Y, X) any numeric type FLOAT population covariance covar_samp(Y, X) any numeric type FLOAT sample covariance regr_avgx(Y, X) any numeric type FLOAT average of the independent variable (sum(X)/N) regr_avgy(Y, X) any numeric type FLOAT average of the dependent variable (sum(Y)/N) regr_count(Y, X) any numeric type BIGINT number of input rows in which both expressions are nonnull regr_intercept(Y, X) any numeric type FLOAT y-intercept of the least-squares-fit linear equation determined by the (X, Y) pairs regr_r2(Y, X) any numeric type FLOAT the coefficient of determination (also called R-squared or goodness of fit) regr_slope(Y, X) any numeric type FLOAT slope of the least-squares-fit linear equation determined by the (X, Y) pairs regr_sxx(Y, X) any numeric type FLOAT sum(X^2) - sum(X)^2/N (\"sum of squares\" of the independent variable) regr_sxy(Y, X) any numeric type FLOAT sum(X*Y) - sum(X)*sum(Y)/N (\"sum of products\" of independent times dependent variable) regr_syy(Y, X) any numeric type FLOAT sum(Y^2) - sum(Y)^2/N (\"sum of squares\" of the dependent variable) stddev(expression) any numeric type FLOAT historical alias for stddev_samp stddev_pop(expression) any numeric type FLOAT population standard deviation of the input values stddev_samp(expression) any numeric type FLOAT sample standard deviation of the input values variance(expression) any numeric type FLOAT historical alias for var_samp var_pop(expression) any numeric type FLOAT population variance of the input values (square of the population standard deviation) var_samp(expression) any numeric type FLOAT sample variance of the input values (square of the sample standard deviation) Order-Sensitive Aggregate Functions \u25c4 These functions can only be used in running aggregates on ordered data (table adapted from PostgreSQL documentation ): Function Return Type Description row_number() BIGINT number of the current row within its partition, counting from 1 rank() BIGINT rank of the current row with gaps; same as row_number of its first peer dense_rank() BIGINT rank of the current row without gaps; this function counts peer groups percent_rank() FLOAT relative rank of the current row: (rank - 1) / (total rows - 1) cume_dist() FLOAT relative rank of the current row: (number of rows preceding or peer with current row) / (total rows) ntile(num_buckets INTEGER) INTEGER integer ranging from 1 to the argument value, dividing the partition as equally as possible lag(value ANY [, offset INTEGER [, default ANY ]]) same type as value returns value evaluated at the row that is offset rows before the current row within the partition; if there is no such row, instead return default. Both offset and default are evaluated with respect to the current row. If omitted, offset defaults to 1 and default to null first_value(value ANY) same type as value returns value evaluated at the row that is the first row of the window frame last_value(value ANY) same type as value returns value evaluated at the row that is the last row of the window frame nth_value(value ANY, nth INTEGER) same type as value returns value evaluated at the row that is the nth row of the window frame (counting from 1); null if no such row string_agg(expression ANY, delimiter VARCHAR) CHAR(n) input values concatenated into a string, separated by delimiter Data Types \u25c4 Sclera supports a subset of PostgreSQL data types : BIGINT , BOOL or BOOLEAN , CHAR(n) , CHAR , DATE , DECIMAL(prec) , DECIMAL(prec, scale) , DECIMAL , FLOAT(prec) , FLOAT , INT or INTEGER , NUMERIC(prec) , NUMERIC(prec, scale) , NUMERIC , REAL , SMALLINT , TEXT , TIMESTAMP , TIME , VARCHAR(n) , VARCHAR . Note that ARRAY and other composite data types are not supported. Reserved Keywords \u25c4 The following cannot be used as a name or alias of any object within a SQL command: ADD , ALL , ALTER , AND , ANY , ARG , AS , ASC , ASSOCIATOR , BETWEEN , BIGINT , BIT , BOOL , BOOLEAN , BPCHAR , BY , CASE , CAST , CHAR , CLASSIFIED , CLASSIFIER , CLUSTERED , CLUSTERER , COLUMN , CONNECTED , CREATE , CROSS , DATE , DAY , DECIMAL , DELETE , DESC , DISTINCT , DROP , ELSE , END , ESCAPE , EXCEPT , EXISTS , EXTERNAL , FALSE , FETCH , FIRST , FLAG , FLOAT , FOREIGN , FROM , FULL , GRAPH , GROUP , HAVING , HOUR , ILIKE , IMPUTED , IN , INNER , INSERT , INT , INTEGER , INTERSECT , INTERVAL , INTO , IS , ISNULL , JOIN , KEY , LABEL , LAST , LEFT , LIKE , LIMIT , LOCATION , MATCH , MINUTE , MONTH , NATURAL , NEXT , NOT , NOTNULL , NULL , NULLS , NUMERIC , OF , OFFSET , ON , ONLY , OR , ORDER , ORDERED , OUTER , OVER , PARTITION , PIVOT , PREDICTOR , PRIMARY , READONLY , REAL , REFERENCES , REMOVE , RIGHT , ROW , ROWS , SCHEMA , SECOND , SELECT , SET , SIMILAR , SMALLINT , SOME , SYMMETRIC , TABLE , TEMP , TEMPORARY , TEXT , THEN , TIME , TIMESTAMP , TO , TRUE , UNION , UNKNOWN , UPDATE , USING , VALUES , VARBIT , VARCHAR , VARCHAR2 , VARYING , VIEW , WHEN , WHERE , WITH , WITHOUT , YEAR , ZONE .","title":"Miscellaneous Information"},{"location":"sclerasql/sqlmisc/#aggregate-functions","text":"The aggregate functions supported in Sclera can be order-insensitive , which apply to both ordered and unordered data , or order-sensitive , which apply only to ordered data .","title":"Aggregate Functions"},{"location":"sclerasql/sqlmisc/#order-insensitive-aggregate-functions","text":"These functions can be used in standard SQL aggregates as well as in running aggregates on ordered data (table adapted from PostgreSQL documentation ): Aggregate Argument Type Return Type Description avg(expression) any numeric type FLOAT the average (arithmetic mean) of all input values bool_and(expression) BOOLEAN BOOLEAN true if all input values are true, otherwise false bool_or(expression) BOOLEAN BOOLEAN true if at least one input value is true, otherwise false count(*) BIGINT number of input rows count(expression) any type BIGINT number of input rows for which the value of expression is not null every(expression) BOOLEAN BOOLEAN equivalent to bool_and max(expression) any numeric, string, or date/time type same as argument type maximum value of expression across all input values min(expression) any numeric, string, or date/time type same as argument type minimum value of expression across all input values sum(expression) any numeric type BIGINT for SMALLINT or INTEGER arguments, FLOAT for floating-point arguments sum of expression across all input values corr(Y, X) any numeric type FLOAT correlation coefficient covar_pop(Y, X) any numeric type FLOAT population covariance covar_samp(Y, X) any numeric type FLOAT sample covariance regr_avgx(Y, X) any numeric type FLOAT average of the independent variable (sum(X)/N) regr_avgy(Y, X) any numeric type FLOAT average of the dependent variable (sum(Y)/N) regr_count(Y, X) any numeric type BIGINT number of input rows in which both expressions are nonnull regr_intercept(Y, X) any numeric type FLOAT y-intercept of the least-squares-fit linear equation determined by the (X, Y) pairs regr_r2(Y, X) any numeric type FLOAT the coefficient of determination (also called R-squared or goodness of fit) regr_slope(Y, X) any numeric type FLOAT slope of the least-squares-fit linear equation determined by the (X, Y) pairs regr_sxx(Y, X) any numeric type FLOAT sum(X^2) - sum(X)^2/N (\"sum of squares\" of the independent variable) regr_sxy(Y, X) any numeric type FLOAT sum(X*Y) - sum(X)*sum(Y)/N (\"sum of products\" of independent times dependent variable) regr_syy(Y, X) any numeric type FLOAT sum(Y^2) - sum(Y)^2/N (\"sum of squares\" of the dependent variable) stddev(expression) any numeric type FLOAT historical alias for stddev_samp stddev_pop(expression) any numeric type FLOAT population standard deviation of the input values stddev_samp(expression) any numeric type FLOAT sample standard deviation of the input values variance(expression) any numeric type FLOAT historical alias for var_samp var_pop(expression) any numeric type FLOAT population variance of the input values (square of the population standard deviation) var_samp(expression) any numeric type FLOAT sample variance of the input values (square of the sample standard deviation)","title":"Order-Insensitive Aggregate Functions"},{"location":"sclerasql/sqlmisc/#order-sensitive-aggregate-functions","text":"These functions can only be used in running aggregates on ordered data (table adapted from PostgreSQL documentation ): Function Return Type Description row_number() BIGINT number of the current row within its partition, counting from 1 rank() BIGINT rank of the current row with gaps; same as row_number of its first peer dense_rank() BIGINT rank of the current row without gaps; this function counts peer groups percent_rank() FLOAT relative rank of the current row: (rank - 1) / (total rows - 1) cume_dist() FLOAT relative rank of the current row: (number of rows preceding or peer with current row) / (total rows) ntile(num_buckets INTEGER) INTEGER integer ranging from 1 to the argument value, dividing the partition as equally as possible lag(value ANY [, offset INTEGER [, default ANY ]]) same type as value returns value evaluated at the row that is offset rows before the current row within the partition; if there is no such row, instead return default. Both offset and default are evaluated with respect to the current row. If omitted, offset defaults to 1 and default to null first_value(value ANY) same type as value returns value evaluated at the row that is the first row of the window frame last_value(value ANY) same type as value returns value evaluated at the row that is the last row of the window frame nth_value(value ANY, nth INTEGER) same type as value returns value evaluated at the row that is the nth row of the window frame (counting from 1); null if no such row string_agg(expression ANY, delimiter VARCHAR) CHAR(n) input values concatenated into a string, separated by delimiter","title":"Order-Sensitive Aggregate Functions"},{"location":"sclerasql/sqlmisc/#data-types","text":"Sclera supports a subset of PostgreSQL data types : BIGINT , BOOL or BOOLEAN , CHAR(n) , CHAR , DATE , DECIMAL(prec) , DECIMAL(prec, scale) , DECIMAL , FLOAT(prec) , FLOAT , INT or INTEGER , NUMERIC(prec) , NUMERIC(prec, scale) , NUMERIC , REAL , SMALLINT , TEXT , TIMESTAMP , TIME , VARCHAR(n) , VARCHAR . Note that ARRAY and other composite data types are not supported.","title":"Data Types"},{"location":"sclerasql/sqlmisc/#reserved-keywords","text":"The following cannot be used as a name or alias of any object within a SQL command: ADD , ALL , ALTER , AND , ANY , ARG , AS , ASC , ASSOCIATOR , BETWEEN , BIGINT , BIT , BOOL , BOOLEAN , BPCHAR , BY , CASE , CAST , CHAR , CLASSIFIED , CLASSIFIER , CLUSTERED , CLUSTERER , COLUMN , CONNECTED , CREATE , CROSS , DATE , DAY , DECIMAL , DELETE , DESC , DISTINCT , DROP , ELSE , END , ESCAPE , EXCEPT , EXISTS , EXTERNAL , FALSE , FETCH , FIRST , FLAG , FLOAT , FOREIGN , FROM , FULL , GRAPH , GROUP , HAVING , HOUR , ILIKE , IMPUTED , IN , INNER , INSERT , INT , INTEGER , INTERSECT , INTERVAL , INTO , IS , ISNULL , JOIN , KEY , LABEL , LAST , LEFT , LIKE , LIMIT , LOCATION , MATCH , MINUTE , MONTH , NATURAL , NEXT , NOT , NOTNULL , NULL , NULLS , NUMERIC , OF , OFFSET , ON , ONLY , OR , ORDER , ORDERED , OUTER , OVER , PARTITION , PIVOT , PREDICTOR , PRIMARY , READONLY , REAL , REFERENCES , REMOVE , RIGHT , ROW , ROWS , SCHEMA , SECOND , SELECT , SET , SIMILAR , SMALLINT , SOME , SYMMETRIC , TABLE , TEMP , TEMPORARY , TEXT , THEN , TIME , TIMESTAMP , TO , TRUE , UNION , UNKNOWN , UPDATE , USING , VALUES , VARBIT , VARCHAR , VARCHAR2 , VARYING , VIEW , WHEN , WHERE , WITH , WITHOUT , YEAR , ZONE .","title":"Reserved Keywords"},{"location":"sclerasql/sqlregular/","text":"In this section, we describe the standard SQL constructs and query clauses supported by Sclera. The extensions are covered in later sections . The SQL constructs discussed below enable you to create, populate, query, update and delete base tables; these lie at the core of Sclera, and are supported across all editions. Sclera supports a significant subset of standard SQL. The specific SQL dialect is largely compatible with PostgreSQL . In addition, Sclera supports a few proprietary short-cuts that, in our opinion, make the scripts more compact and readable. This section is neither a tutorial, nor a formal specification of the supported SQL, and only serves as a guide illustrating the differences between Sclera's SQL and the PostgreSQL's SQL commands . If you need help with the basics, or want a more formal description, we suggest that you read this section alongside PostgreSQL's excellent documentation . Syntax Notation: [ .. ] means optional, | separates the alternatives, [ .. | .. ] lists alternatives at most one of which should be included, { .. | .. } lists alternatives exactly one of which should be included, and [, ...] stands for comma-separated repeats of the immediately preceding expression. In the following, the keywords appear in upper case to distinguish them from the other terms; however, Sclera is case-insensitive and keywords in actual commands and queries can be in upper or lower case. We start with describing the syntax of a scalar expression , followed with that of a select expression (or, query) . Next, we describe the syntax of SQL commands for creating base tables , inserting , updating , deleting data in base tables, and finally dropping base tables. This is followed with commands for creating and dropping views. Finally, we describe the data types supported by ScleraSQL, and the reserved words that cannot be used except as a part of the SQL syntax. Scalar Expressions \u25c4 A scalar expression (equivalently, a scalar_expression ) can be any of the following: A NULL or a constant A column reference to a column in any of the from_item elements in the FROM clause . The column reference can be qualified by the associated table name or alias. An expression with a type cast . The type cast can be used to convert the type of an expression to a supported data type . A scalar subquery , which is a SQL query , enclosed in parenthesis, whose result can have exactly one column and at most one row. If the result is empty, the value of the subquery is NULL , otherwise it gets the value of the column in the returned row. Sclera does not support correlated subqueries; in other words, you should be able to execute the scalar subquery as a stand-alone SQL query. The SQL standard is more general as it allows subqueries with expressions containing parameters coupling it to the enclosing query (see PostgreSQL's documentation ). An aggregate expression . Aggregate expressions can only appear in the SELECT clause , ORDER BY clause , or HAVING clause . A complex scalar expression constructed by applying supported functions and operators on other scalar expressions . The expressions in Sclera are mostly compliant with the supported subset of the SQL standard and PostgreSQL . The difference is in lack of support for correlated subqueries , extended data types and associated functions and operators . Querying Data using Select Expressions \u25c4 A SELECT statement (equivalently, a select_expression ) has the following syntax: SELECT [ ALL | DISTINCT [ ON ( expression [, ...] ) ] ] { expression [ [ AS ] output_name ] | abbreviation } [, ...] [ FROM from_item [, ...] ] [ WHERE condition ] [ GROUP BY expression [, ...] ] [ HAVING condition [, ...] ] [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select_expression ] [ ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ] [ LIMIT count [ ROW | ROWS ] ] [ OFFSET start [ ROW | ROWS ] ] [ FETCH [ FIRST | NEXT ] [ count ] [ ROW | ROWS ] ONLY ] This is a restriction of PostgreSQL's select statement syntax . The semantics of a select_expression (that is, SQL engine's interpretation of what is to be evaluated), and description of the various clauses appears below. The clauses are explained roughly in the order in which they are executed by Sclera's execution engine. FROM Clause \u25c4 The FROM clause specifies the input to the query as a list of from_item elements. Each from_item in the FROM clause can be one of: [location_name.]table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] table_name is a base table at the data location location_name . If the table_name is unique across the locations, you can omit the location_name . alias , when specified, becomes the substitute name for table_name within the scope of the query. If alias is specified, table_name can no longer be used in any expression . column_alias , when specified, becomes the substitute name for the corresponding column in the table table_name . If column_alias is specified for a column, the original column name can no longer be used in any expression . If the number of column aliases is less than the number of columns in table_name , the remaining columns of table_name retain their original names. If the number of column aliases is more than the number of columns in table_name , the remaining columns aliases are ignored. ( select_expression ) [ AS ] alias [ ( column_alias [, ...] ) ] Defines a virtual table computed using the nested select_expression and referenced using the name alias . By default, the output columns of the nested select_expression become the columns of the virtual table. However, if a column_alias list is specified, it overrides these default names, as earlier . VALUES ( constant [, ...] ) [, ...] [ AS ] alias [ ( column_alias [, ...] ) ] Defines a virtual table containing the rows enumerated explicitly as a comma-separated list of tuples, and referenced using the name alias . Note that all the rows must confirm to the same schema -- that is, each row must have the same number of elements, and the element at a given position in each row must have the same data type. By default, the Sclera assigns unique names to the columns in the rows. However, if a column_alias list is specified, it overrides these default names, as earlier . from_item [ NATURAL ] join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ] Defines a composite from_item formed by joining two from_item elements, hereafter referred to as inputs . join_type can be one of [ INNER ] JOIN , LEFT [ OUTER ] JOIN , RIGHT [ OUTER ] JOIN , FULL [ OUTER ] JOIN or CROSS JOIN (recall that words in [ .. ] are optional -- here, the words INNER and OUTER are just syntactic sugar). If the join_type is not a CROSS JOIN , you can specify at most one of the ON join_condition , USING ( join_column [, ...] ) ] or NATURAL . The join_condition in ON is a boolean-valued expression on the consolidated output columns of the two inputs. The join_column list in USING is shorthand for a join condition which is a conjunct of equalities among columns with the same name. Also, the output contains only one copy of the common columns in the join_column list. Example: X JOIN Y USING (A, B) is shorthand for X JOIN Y ON (X.A = Y.A AND X.B = Y.B) . The output contains only one copy of columns A and B . The NATURAL is a shorthand for a USING with the join_column list containing all columns with the same name across the two inputs; these column names need not be specified explicitly when NATURAL is used. Example: Given table X with columns A , B , C and table Y with columns B , C , D , the from_item X NATURAL JOIN Y is shorthand for X JOIN Y USING (B, C) . If none of the ON , USING or NATURAL are specified, the join is considered a CROSS JOIN . If the join_type is a CROSS JOIN , none of the ON , USING or NATURAL should be specified. A comma-separated list of more than one from_item elements translates to a cross-product of these elements, right to left; this cross-product is rewritten to an INNER JOIN by the optimizer if the WHERE clause includes cross-input predicates. The FROM clause is optional. If the clause is absent, the input is implicit as a single row with a single NULL -valued column. This clause is compliant with the supported subset of the SQL standard and PostgreSQL . Cross-System Joins \u25c4 When a join contains inputs from more than one database system, it is a cross-system join. While planning the evaluation of cross-system joins, Sclera considers the left input of a join larger than the right input, and prefers moving the right input to the left input's location. The ordering of the from_item s in a FROM clause thus matters when evaluating cross-system joins (see the technical documentation for details). While this enables you to control how data is moved while evaluating a query, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system join, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query. WHERE Clause \u25c4 The WHERE clause specifies a boolean expression on the column names in the output of the FROM clause . This boolean expression is used to filter the rows in the input. The boolean expression can only contain column names in the output of the FROM clause , and cannot contain an aggregate expression . The WHERE clause is optional. If the clause is absent, the input is not filtered. This clause is compliant with the SQL standard and PostgreSQL . GROUP BY Clause \u25c4 The GROUP BY clause groups the input rows, creating a group for each distinct combination of the GROUP BY expressions . The clause works with aggregate functions in the SELECT clause , ORDER BY clause , and/or HAVING clause . These clauses generate a row for each group, which can then be output , or used for ordering or filtering the result. Each expression in SELECT , ORDER BY or HAVING clauses not appearing within an aggregate function must build on the GROUP BY expressions or constants. The output is a row for each group. The GROUP BY clause in Sclera is mostly compliant with the SQL standard and PostgreSQL . In SQL standard and PostgreSQL , you can include an expression in the SELECT clause in the GROUP BY list by specifying its position number. This is not allowed in Sclera; instead, you can associate the SELECT expression with an alias ( output_name in the syntax stated above) and use that in the GROUP BY list. SELECT Clause \u25c4 The mandatory SELECT clause specifies the output of the query. The specification consists of a list of scalar expressions , optionally associated with alias names. Each scalar expression in the clause corresponds to a column in the output; the name of the column is the expression's alias name, or an automatically generated name. The SELECT expressions can only contain column names in the output of the FROM clause , and may include aggregate expressions . Abbreviations \u25c4 SQL provides certain abbreviations when specifying the SELECT expression list. An abbreviation is a special expression that expands to an expression list on compilation. In Sclera, an abbreviation can be one of the following: * [EXCEPT column_list] : The * expands to an expression list consists of all the columns in the output of the FROM clause . If EXCEPT column_list is specified, then the columns in the column_list are not included in the expansion. alias.* [EXCEPT column_list] : The alias.* expands to an expression list consists of all the columns in the from_item with alias alias in the FROM clause . If EXCEPT column_list is specified, then the columns in the column_list are not included in the expansion. In the above, column_list is either a column_alias , or a comma-separated column_alias list in parenthesis. The EXCEPT modifier is a Sclera addition to the syntax, it is not supported in the SQL standard or in PostgreSQL . Abbreviations cannot be used in the SELECT clause if a GROUP BY clause is present in the query. Distinct \u25c4 The select clause also has an optional DISTINCT clause, included between SELECT and the expression list, which comes in two variants. The first variant, specified by the modifier DISTINCT , eliminates duplicate rows in the output. This clause is part of the SQL standard. The second variant, specified by DISTINCT ON (expression [, ...]) , is a generalization of the first variant. The rows are grouped based on the distinct values of the DISTINCT ON expressions, and one row from each row is output. If more than one rows exist in a group, this choice is arbitrary. However, if an ORDER BY clause is present in the query, the rows in each group are first ordered, and the first row (in the resulting sorted order of rows) is chosen for output. This variant is also supported by PostgreSQL , but is not a part of the SQL standard. Aggregation \u25c4 If the SELECT expressions include aggregate expressions , then the input rows are aggregated. If a GROUP BY clause is present, there is a group for each distinct combination of the GROUP BY expression values; otherwise all the input rows form a single group. The rows in each group are aggregated based on the aggregate expressions, resulting in one output row for each group. For the above computation to be valid, all column names that do not appear as expressions in the GROUP BY clause (and are hence not constant within a group), must appear within an aggregation function . The SELECT clause in Sclera is compliant with the SQL standard and PostgreSQL . HAVING Clause \u25c4 The HAVING clause specifies a boolean-valued aggregate expression that is evaluated in the same way as the aggregate expressions in SELECT clause . However, unlike the SELECT clause , the resulting value is not output. Instead, it is used to filter the aggregation result. This clause is compliant with the SQL standard and PostgreSQL . UNION / INTERSECT / EXCEPT Clause \u25c4 The UNION , INTERSECT and EXCEPT clauses are used to compute the set union, intersection or difference, respectively, on the result of two queries. The modifier DISTINCT removes duplicate rows in the computed result; this is also the default when no modifier is specified. To disable the removal of duplicate rows, you can specify ALL instead. These clauses in Sclera are compliant with the SQL standard and PostgreSQL . Cross-System Set Operations \u25c4 When the clause contains queries evaluated at more than one system, it specifies a cross-system set operation. To evaluate a cross-system set operation, Sclera needs both the query results to be present at a single location; let us call this the \"target location\" for the set operation. This target location is decided as follows: For each query, Sclera finds the location of the query result after evaluation. These locations are the candidates for the target location, and are listed in the order of appearance of the corresponding queries. From this list, Sclera then removes the Cache Store , if present, as well as the \"read-only\" locations . If the list is empty, Sclera assigns the Cache Store as the target location. This has the effect that cross-system set operations across read-only locations are evaluated by evaluating both the queries, and moving the query results to the cache store; the set operation is then computed at the cache store. If the list is not empty, Sclera assigns the location on the left in the list as the target location. This has the effect that the query in the other location is evaluated, and its result is moved to the target location. The set operation is then computed along with the evaluation of the query in the target location. The ordering of the queries thus matters when evaluating cross-system set operations. While this enables you to control how data is moved while evaluating the queries containing set operations, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system set operation, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query. In the current version, Sclera moves data from a \"source\" to a \"target\" database system by reading in the data from the source and inserting it into a temporary table in the target. This transfer is done in a streaming (pipelined) manner wherever possible, to avoid reading the entire result in memory. This could be a bottleneck when large amounts of data (millions of rows) are transferred. More efficient data transfer mechanisms will be in place in later versions of Sclera. ORDER BY Clause \u25c4 The ORDER BY clause specifies a list of scalar expressions that determines the sort order of the computed result. The optional modifiers ASC (for ascending ) or DESC (for descending ) associated with each expression in the ORDER BY list determine how that particular expression should be considered while determining the overall sort order. When ASC is specified, the larger values are ordered after the smaller values, and the NULLS are ordered after all values. When DESC is specified, the smaller values are ordered after the larger values, and the NULLS are ordered before all values. The default is ASC . The optional NULLS FIRST and NULLS LAST modifiers force the NULLS to be ordered before or after all values. The ORDER BY clause in Sclera is mostly compliant with the SQL standard and PostgreSQL . In SQL standard and PostgreSQL , you can include an expression in the SELECT clause in the ORDER BY list by specifying its position number. This is not allowed in Sclera; instead, you can associate the SELECT expression with an alias ( output_name in the syntax stated above) and use that in the ORDER BY list. LIMIT , FETCH and OFFSET Clauses \u25c4 The LIMIT clause retains the specified number of initial rows in the result. If the result has less than the specified number of rows, this clause has no effect. The FETCH clause is an alternative to LIMIT , with a relatively verbose syntax. Here, again, you can specify the number of initial rows to retain in the result; but you can also say FETCH FIRST ROW ONLY to retain only the first row, if present. In general, the words FIRST , NEXT , ROW and ROWS are optional syntactic sugar; the number of initial rows, if not specified, defaults to 1 . A query can have either the LIMIT clause or the FETCH clause. The OFFSET clause discards the specified number of initial rows in the result. If the result has less than the specified number of rows, all rows are discarded. When both the OFFSET and the LIMIT (or FETCH ) clauses are present in a query, the OFFSET clause applies before the LIMIT (or FETCH ) clause. In absence of an ORDER BY clause , the query result is in arbitrary order and consequently, the retained/discarded row set is not deterministic (i.e. a different set of rows may be retained/discarded on each invocation). As such, the recommended use of these clauses is in conjunction with the ORDER BY clause . The FETCH and OFFSET clauses in Sclera are compliant with the SQL standard and PostgreSQL . The LIMIT clause is supported in PostgreSQL , but not in the SQL standard. Querying Data Using Generalized Table Expressions \u25c4 In standard SQL, as well as in PostgreSQL and other relational databases, a query is restricted to the SELECT statement described above . Sclera generalizes the query syntax such that anything that can appear in the FROM clause , possibly without the alias, is accepted as a query. We call this a table_expression . To recap, this includes: A standard SELECT statement . As a result, any query in the earlier query syntax remains admissible; this is indeed a generalization of the \"standard\" select expression / query discussed above . A base table name, possibly qualified with a location. You can rename the table and the columns the same way you could in the FROM clause . Syntax: [location_name.]table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] (see above for explanation) A values expression, which explicitly lists out the rows. Syntax: VALUES ( constant [, ...] ) [, ...] [ AS ] alias [ ( column_alias [, ...] ) ] (see above for explanation) A join expression involving nested table_expression elements. Syntax: table_expression [ NATURAL ] join_type table_expression [ ON join_condition | USING ( join_column [, ...] ) ] (see above for explanation) A table expression in parenthesis, with optional alias. Syntax: ( table_expression ) [ [ AS ] alias [ ( column_alias [, ...] ) ] ] In addition, a table_expression can have additional WHERE , ORDER BY and LIMIT / OFFSET / FETCH clauses. This is not supported in the standard SQL FROM clause. A table expression with a WHERE clause . Syntax: ( table_expression ) WHERE condition A table expression with an ORDER BY clause . Syntax: ( table_expression ) ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ] A table expression with LIMIT , FETCH and/or OFFSET clauses Syntax: ( table_expression ) [ LIMIT count [ ROW | ROWS ] ] [ OFFSET start [ ROW | ROWS ] ] [ FETCH [ FIRST | NEXT ] [ count ] [ ROW | ROWS ] ONLY ] Using this generalized syntax, to get the contents of the table ORDERS , you just need to say > ORDERS; instead of the unnecessarily verbose > SELECT * FROM ORDERS; To filter and limit, you can say > ORDERS WHERE CUSTID = 10 LIMIT 3 instead of > SELECT * FROM ORDERS WHERE CUSTID = 10 LIMIT 3 Similarly, to get the join of the tables ORDERS and CUSTOMERS , you just need to say > ORDERS JOIN CUSTOMERS USING (CUSTID); instead of the verbose > SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID); Further, we can union the above with another table PREV by saying > ORDERS JOIN CUSTOMERS USING (CUSTID) UNION PREV; instead of the standard > SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID) UNION SELECT * FROM PREV; The queries in the verbose standard SELECT syntax are still valid, but the equivalent queries in the extended syntax are more compact. As we can see, the difference is more pronounced in more complex queries. Base Tables \u25c4 In this section we describe Sclera's SQL commands for creating, updating and deleting base tables. Creating Base Tables \u25c4 Creating tables in Sclera is different from adding tables to Sclera using the ADD TABLE command . The latter merely imports the metadata of an existing table into Sclera. The create table commands, described below, create a new table. The net effect of these commands is (a) creating the table on the underlying database system, and (b) adding the table metadata to Sclera's metadata store. If the database system understands SQL (e.g. MySQL ), Sclera creates the table by generating the appropriate CREATE TABLE command, taking care of dialect and datatype differences; if the database system does not understand SQL (e.g. NoSQL datastores), Sclera uses the system's API (or any other available mechanism) to create an underlying structure that is compatible with the metadata specified in the command. The CREATE TABLE command has two variants. The first variant creates an empty table. The second variant creates a table, and also populates it with the result of a query. Creating Empty Tables \u25c4 The syntax of the CREATE TABLE command is as follows: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) This creates a new, empty table (persistent or temporary ) with the name table_name at location location_name (or the default location , if location_name is omitted), consisting of columns with names and types given by the associated column_name and column_type , and with column and table constraints specified by column_constraint and table_constraint . These terms are described in turn below. Temporary Modifier \u25c4 The optional TEMPORARY (shortened form: TEMP ) modifier in CREATE TEMPORARY TABLE creates a table that exists for the duration of the Sclera session only. When the session ends, the table is deleted from the underlying database system and its metadata is removed from Sclera's metadata store. Table Name \u25c4 The mandatory table_name is the name of the table to be created. You can optionally qualify the name with location_name , the name of the location where the table is to be created. If the location_name is omitted, the table is created at the default location . Columns and Their Types \u25c4 A table needs to have at least one column. You can specify the list of columns and their types as a comma-separated list of column_name and data_type pairs. The data_type must be one of the data types supported by Sclera . Column Constraints \u25c4 With each column in the column list, you can optionally specify a set of column constraints. Each column_constraint can be one of: NOT NULL : This column can never be NULL . The constraint is checked while inserting rows into the underlying table. PRIMARY KEY : This column is the primary key of the table -- as such, it must never be NULL , and a value of the column must be unique across all the rows in the table. This constraint can be specified for at most one column, and cannot be specified along with a PRIMARY KEY table constraint . REFERENCES [location_name.]table_name [ (column_name) ] : This column references the table table_name . The table with the name specified by table_name must be present in an underlying database system (if table_name is not unique across locations, the location_name must be specified). Further: If the column_name is specified, the table table_name must contain a column with the name specified by column_name of the same type as this column and with a PRIMARY KEY constraint. Also, for each row in this table, either this column is NULL , or there exists a row in table table_name with a matching value in the column column_name . If the column_name is not specified, the table table_name must have a primary key, which must be of the same type as this column. Also, for each row in this table, either this column is NULL , or there exists a row in table table_name with a matching primary key value. When the column_name is not specified, or equivalently, when it is the primary key of the table table_name , this column is called a foreign key (to table table_name ). Table Constraints \u25c4 A table constraint is a constraint on an entire row of the table, rather than a single column. Table constraints are more general than column constraints; each column_constraint mentioned above can be equivalently stated as a table_constraint . However, table constraint also allows you to specify multi-column primary keys and references/foreign keys. The optional table_constraint list appears as a continuation of the column list . Each table_constraint can be one of: PRIMARY KEY ( column_name [, ...] ) : The set of columns within the parenthesis the primary key of the table. As such, each of these columns can never be NULL , and the combined set of values of these columns must be unique across all the rows in the table. This constraint cannot be specified along with a PRIMARY KEY column constraint . FOREIGN KEY ( column_name [, ...] ) REFERENCES [location_name.]table_name [ ( column_name [, ...] ) ] : The list of columns references the table table_name . The table with the name specified by table_name must be present in an underlying database system (if table_name is not unique across locations, the location_name must be specified). Further: If the reference table's column_name list is specified, (a) it must have the same number of columns as the foreign key list, (b) the table table_name must contain each column in the reference list and these columns must collectively have a PRIMARY KEY constraint, and (c) each column of the reference list must have the same type as the corresponding column (i.e. at the same position) in the foreign key list. Also, for each row in this table, either some column in the foreign-key list is NULL , or there exists a row in table table_name with a matching set of values of the respective columns in the reference list. If the reference table's column_name list is not specified, the table table_name must have a primary key, which (a) must have the same number of columns as the foreign key list, and (b) each column of this primary key must have the same type as the corresponding column (i.e. at the same position) in the foreign key list. Also, for each row in this table, either some column in the foreign-key list is NULL , or there exists a row in table table_name with a matching set of values of the respective columns in the primary key. Note that Sclera does not enforce the column or table constraints described above. Sclera passes on these constraints to the underlying database system, if possible, and it is the responsibility of the underlying database system to check for these constraints. Nevertheless, Sclera assumes that these constraints hold while querying the data in the tables. While creating tables on database systems which do not enforce such constraints, it is the responsibility of the user to make sure that these constraints hold while inserting rows into the table. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL . Creating Tables with Query Results \u25c4 This variant of the CREATE TABLE statement enables you to create a table, and populates it with the result of the given query. This is more efficient than performing the two tasks one after another, especially in transactional systems such as MySQL and PostgreSQL . Also, this is sometimes more convenient because you need not specify the schema explicitly; if not specified, the schema is derived from the output schema of the given query. The standard syntax of the CREATE TABLE AS command is as follows: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] AS select_expression where select_expression is as described earlier . This is almost identical to the CREATE TABLE syntax , except the new select_expression , and that the schema description (column definitions, column and table constraint specifications) is now optional. Sclera generalizes this expression to accept a table_expression instead of a select_expression . The generalized syntax is: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] AS table_expression Note that since select_expression is a table_expression , statements in the standard syntax continues to be valid in this updated syntax. As in the case of queries , this generalization simplifies the CREATE TABLE AS statements. For instance, you can say: > CREATE TABLE CUSTORDERS AS ORDERS JOIN CUSTOMERS USING (CUSTID); instead of the standard: > CREATE TABLE CUSTORDERS AS SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID); This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL . Inserting Rows into a Base Table \u25c4 The INSERT statement inserts rows into an existing base table. The standard syntax of the INSERT statement is as follows: INSERT INTO [location_name.]table_name [ ( column_name [, ...] ) ] (select_expression | values_expression) Here, table_name is a base table at location location_name , into which the rows in the result of select_expression or value_expression will be inserted. The select_expression stands for the SELECT statement described earlier , and the value_expression stands for the VALUES clause discussed earlier , possibly without the alias. In the above, the location_name can be omitted if table_name is unique across locations. Sclera generalizes the syntax to: INSERT INTO [location_name.]table_name [ ( column_name [, ...] ) ] table_expression wherein the select_expression or value_expression have been replaced with table_expression . Recall that a select_expression or a value_expression is a table_expression as well; so the standard syntax continues to be valid. The optional column_name explicitly lists the table columns; each column in the list must be present in the table schema. If this list is not specified, it is taken to be the list of columns in the schema of the table table_name , in the order in which they were listed while creating the table. Given this explicit/implicit list, the following should hold: It must have the same number of columns as the result of the table_expression . For each column in the list, the type of a column in the table table_name must be compatible with the type of the corresponding (i.e. at the same position) column in the result. If the column_name list is explicitly specified, any column in the table table_name which does not appear in the list gets a NULL for every row inserted. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL . Updating Rows in a Base Table \u25c4 The UPDATE statement updates the value of one or more columns in rows in a base table. The syntax supported in Sclera is as follows: UPDATE [location_name.]table_name SET { column_name = scalar_expression } [, ...] [ WHERE condition ] Here, table_name is the name of the table to be updated. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. The optional WHERE clause specifies the condition (a boolean-valued scalar_expression ) that a row in the table must satisfy to be eligible for the update; rows that do not satisfy this condition are left unchanged. If the WHERE clause is not present, the update applies to all rows in the table. The update to a row is specified by the SET clause, which contains a comma-separated list of \"assignments\". Each assignment specifies that given an eligible row, the column with the name specified by column_name should get the value obtained by evaluation of the scalar_expression on that row. The update proceeds by executing the specified list of assignments on each eligible row. This syntax is compatible with the SQL standard, but is a restricted version of the syntax supported in PostgreSQL . Deleting Rows in a Base Table \u25c4 The DELETE statement deletes rows in a base table. The syntax supported in Sclera is as follows: DELETE FROM [location_name.]table_name [ WHERE condition ] Here, table_name is the name of the table from which rows are to be deleted. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. The optional WHERE clause specifies the condition (a boolean-valued scalar_expression ) that a row in the table must satisfy to be eligible for deletion; rows that do not satisfy this condition are left unchanged. If the WHERE clause is not present, all rows in the table are deleted. This syntax is a restricted version of that supported in the SQL standard and PostgreSQL . Dropping Base Tables \u25c4 The DROP TABLE statement drops a base table from Sclera as well as the underlying database system. This statement is similar to the REMOVE TABLE statement . However, REMOVE TABLE only removes the metadata of the named table from Sclera; it does not drop the table from the underlying data store. DROP TABLE remove the table's metadata from Sclera and also drops the table from the underlying datastore. The syntax supported in Sclera is as follows: DROP TABLE [location_name.]table_name Here, table_name is the name of the table to be dropped. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. This syntax is a restricted version of that supported in the SQL standard and PostgreSQL . Views \u25c4 A view is a SQL query with the appearance of a read-only table. A view can be used as a base table in any query . But, unlike a base table, a view does not contain any real data; it is just a placeholder for the associated query. Creating Views \u25c4 The standard CREATE VIEW statement has the following syntax: CREATE [ TEMP | TEMPORARY ] VIEW view_name AS select_expression Sclera generalizes the select_expression to a table_expression , resulting in: CREATE [ TEMP | TEMPORARY ] VIEW view_name AS table_expression This statement creates a view with the specified view_name and associates it with the query given by table_expression . Notice the similarity with the CREATE TABLE AS statement . The optional TEMPORARY or TEMP modifier creates a view that exists for the duration of the Sclera session only. When the session ends, the view is deleted and its metadata is removed from Sclera's metadata store. Unlike a base table, a view is not attached to a location ; this is because the underlying query (specified by the table_expression above) can span multiple locations. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL . Dropping Views \u25c4 The DROP VIEW statement drops a view from Sclera. The syntax supported in Sclera is as follows: DROP VIEW view_name Here, view_name is the name of the view to be dropped. This syntax is compatible with the SQL standard and is a restricted version of that supported in PostgreSQL .","title":"Standard SQL"},{"location":"sclerasql/sqlregular/#scalar-expressions","text":"A scalar expression (equivalently, a scalar_expression ) can be any of the following: A NULL or a constant A column reference to a column in any of the from_item elements in the FROM clause . The column reference can be qualified by the associated table name or alias. An expression with a type cast . The type cast can be used to convert the type of an expression to a supported data type . A scalar subquery , which is a SQL query , enclosed in parenthesis, whose result can have exactly one column and at most one row. If the result is empty, the value of the subquery is NULL , otherwise it gets the value of the column in the returned row. Sclera does not support correlated subqueries; in other words, you should be able to execute the scalar subquery as a stand-alone SQL query. The SQL standard is more general as it allows subqueries with expressions containing parameters coupling it to the enclosing query (see PostgreSQL's documentation ). An aggregate expression . Aggregate expressions can only appear in the SELECT clause , ORDER BY clause , or HAVING clause . A complex scalar expression constructed by applying supported functions and operators on other scalar expressions . The expressions in Sclera are mostly compliant with the supported subset of the SQL standard and PostgreSQL . The difference is in lack of support for correlated subqueries , extended data types and associated functions and operators .","title":"Scalar Expressions"},{"location":"sclerasql/sqlregular/#querying-data-using-select-expressions","text":"A SELECT statement (equivalently, a select_expression ) has the following syntax: SELECT [ ALL | DISTINCT [ ON ( expression [, ...] ) ] ] { expression [ [ AS ] output_name ] | abbreviation } [, ...] [ FROM from_item [, ...] ] [ WHERE condition ] [ GROUP BY expression [, ...] ] [ HAVING condition [, ...] ] [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select_expression ] [ ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ] [ LIMIT count [ ROW | ROWS ] ] [ OFFSET start [ ROW | ROWS ] ] [ FETCH [ FIRST | NEXT ] [ count ] [ ROW | ROWS ] ONLY ] This is a restriction of PostgreSQL's select statement syntax . The semantics of a select_expression (that is, SQL engine's interpretation of what is to be evaluated), and description of the various clauses appears below. The clauses are explained roughly in the order in which they are executed by Sclera's execution engine.","title":"Querying Data using Select Expressions"},{"location":"sclerasql/sqlregular/#from-clause","text":"The FROM clause specifies the input to the query as a list of from_item elements. Each from_item in the FROM clause can be one of: [location_name.]table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] table_name is a base table at the data location location_name . If the table_name is unique across the locations, you can omit the location_name . alias , when specified, becomes the substitute name for table_name within the scope of the query. If alias is specified, table_name can no longer be used in any expression . column_alias , when specified, becomes the substitute name for the corresponding column in the table table_name . If column_alias is specified for a column, the original column name can no longer be used in any expression . If the number of column aliases is less than the number of columns in table_name , the remaining columns of table_name retain their original names. If the number of column aliases is more than the number of columns in table_name , the remaining columns aliases are ignored. ( select_expression ) [ AS ] alias [ ( column_alias [, ...] ) ] Defines a virtual table computed using the nested select_expression and referenced using the name alias . By default, the output columns of the nested select_expression become the columns of the virtual table. However, if a column_alias list is specified, it overrides these default names, as earlier . VALUES ( constant [, ...] ) [, ...] [ AS ] alias [ ( column_alias [, ...] ) ] Defines a virtual table containing the rows enumerated explicitly as a comma-separated list of tuples, and referenced using the name alias . Note that all the rows must confirm to the same schema -- that is, each row must have the same number of elements, and the element at a given position in each row must have the same data type. By default, the Sclera assigns unique names to the columns in the rows. However, if a column_alias list is specified, it overrides these default names, as earlier . from_item [ NATURAL ] join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ] Defines a composite from_item formed by joining two from_item elements, hereafter referred to as inputs . join_type can be one of [ INNER ] JOIN , LEFT [ OUTER ] JOIN , RIGHT [ OUTER ] JOIN , FULL [ OUTER ] JOIN or CROSS JOIN (recall that words in [ .. ] are optional -- here, the words INNER and OUTER are just syntactic sugar). If the join_type is not a CROSS JOIN , you can specify at most one of the ON join_condition , USING ( join_column [, ...] ) ] or NATURAL . The join_condition in ON is a boolean-valued expression on the consolidated output columns of the two inputs. The join_column list in USING is shorthand for a join condition which is a conjunct of equalities among columns with the same name. Also, the output contains only one copy of the common columns in the join_column list. Example: X JOIN Y USING (A, B) is shorthand for X JOIN Y ON (X.A = Y.A AND X.B = Y.B) . The output contains only one copy of columns A and B . The NATURAL is a shorthand for a USING with the join_column list containing all columns with the same name across the two inputs; these column names need not be specified explicitly when NATURAL is used. Example: Given table X with columns A , B , C and table Y with columns B , C , D , the from_item X NATURAL JOIN Y is shorthand for X JOIN Y USING (B, C) . If none of the ON , USING or NATURAL are specified, the join is considered a CROSS JOIN . If the join_type is a CROSS JOIN , none of the ON , USING or NATURAL should be specified. A comma-separated list of more than one from_item elements translates to a cross-product of these elements, right to left; this cross-product is rewritten to an INNER JOIN by the optimizer if the WHERE clause includes cross-input predicates. The FROM clause is optional. If the clause is absent, the input is implicit as a single row with a single NULL -valued column. This clause is compliant with the supported subset of the SQL standard and PostgreSQL .","title":"FROM Clause"},{"location":"sclerasql/sqlregular/#cross-system-joins","text":"When a join contains inputs from more than one database system, it is a cross-system join. While planning the evaluation of cross-system joins, Sclera considers the left input of a join larger than the right input, and prefers moving the right input to the left input's location. The ordering of the from_item s in a FROM clause thus matters when evaluating cross-system joins (see the technical documentation for details). While this enables you to control how data is moved while evaluating a query, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system join, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query.","title":"Cross-System Joins"},{"location":"sclerasql/sqlregular/#where-clause","text":"The WHERE clause specifies a boolean expression on the column names in the output of the FROM clause . This boolean expression is used to filter the rows in the input. The boolean expression can only contain column names in the output of the FROM clause , and cannot contain an aggregate expression . The WHERE clause is optional. If the clause is absent, the input is not filtered. This clause is compliant with the SQL standard and PostgreSQL .","title":"WHERE Clause"},{"location":"sclerasql/sqlregular/#group-by-clause","text":"The GROUP BY clause groups the input rows, creating a group for each distinct combination of the GROUP BY expressions . The clause works with aggregate functions in the SELECT clause , ORDER BY clause , and/or HAVING clause . These clauses generate a row for each group, which can then be output , or used for ordering or filtering the result. Each expression in SELECT , ORDER BY or HAVING clauses not appearing within an aggregate function must build on the GROUP BY expressions or constants. The output is a row for each group. The GROUP BY clause in Sclera is mostly compliant with the SQL standard and PostgreSQL . In SQL standard and PostgreSQL , you can include an expression in the SELECT clause in the GROUP BY list by specifying its position number. This is not allowed in Sclera; instead, you can associate the SELECT expression with an alias ( output_name in the syntax stated above) and use that in the GROUP BY list.","title":"GROUP BY Clause"},{"location":"sclerasql/sqlregular/#select-clause","text":"The mandatory SELECT clause specifies the output of the query. The specification consists of a list of scalar expressions , optionally associated with alias names. Each scalar expression in the clause corresponds to a column in the output; the name of the column is the expression's alias name, or an automatically generated name. The SELECT expressions can only contain column names in the output of the FROM clause , and may include aggregate expressions .","title":"SELECT Clause"},{"location":"sclerasql/sqlregular/#abbreviations","text":"SQL provides certain abbreviations when specifying the SELECT expression list. An abbreviation is a special expression that expands to an expression list on compilation. In Sclera, an abbreviation can be one of the following: * [EXCEPT column_list] : The * expands to an expression list consists of all the columns in the output of the FROM clause . If EXCEPT column_list is specified, then the columns in the column_list are not included in the expansion. alias.* [EXCEPT column_list] : The alias.* expands to an expression list consists of all the columns in the from_item with alias alias in the FROM clause . If EXCEPT column_list is specified, then the columns in the column_list are not included in the expansion. In the above, column_list is either a column_alias , or a comma-separated column_alias list in parenthesis. The EXCEPT modifier is a Sclera addition to the syntax, it is not supported in the SQL standard or in PostgreSQL . Abbreviations cannot be used in the SELECT clause if a GROUP BY clause is present in the query.","title":"Abbreviations"},{"location":"sclerasql/sqlregular/#distinct","text":"The select clause also has an optional DISTINCT clause, included between SELECT and the expression list, which comes in two variants. The first variant, specified by the modifier DISTINCT , eliminates duplicate rows in the output. This clause is part of the SQL standard. The second variant, specified by DISTINCT ON (expression [, ...]) , is a generalization of the first variant. The rows are grouped based on the distinct values of the DISTINCT ON expressions, and one row from each row is output. If more than one rows exist in a group, this choice is arbitrary. However, if an ORDER BY clause is present in the query, the rows in each group are first ordered, and the first row (in the resulting sorted order of rows) is chosen for output. This variant is also supported by PostgreSQL , but is not a part of the SQL standard.","title":"Distinct"},{"location":"sclerasql/sqlregular/#aggregation","text":"If the SELECT expressions include aggregate expressions , then the input rows are aggregated. If a GROUP BY clause is present, there is a group for each distinct combination of the GROUP BY expression values; otherwise all the input rows form a single group. The rows in each group are aggregated based on the aggregate expressions, resulting in one output row for each group. For the above computation to be valid, all column names that do not appear as expressions in the GROUP BY clause (and are hence not constant within a group), must appear within an aggregation function . The SELECT clause in Sclera is compliant with the SQL standard and PostgreSQL .","title":"Aggregation"},{"location":"sclerasql/sqlregular/#having-clause","text":"The HAVING clause specifies a boolean-valued aggregate expression that is evaluated in the same way as the aggregate expressions in SELECT clause . However, unlike the SELECT clause , the resulting value is not output. Instead, it is used to filter the aggregation result. This clause is compliant with the SQL standard and PostgreSQL .","title":"HAVING Clause"},{"location":"sclerasql/sqlregular/#unionintersectexcept-clause","text":"The UNION , INTERSECT and EXCEPT clauses are used to compute the set union, intersection or difference, respectively, on the result of two queries. The modifier DISTINCT removes duplicate rows in the computed result; this is also the default when no modifier is specified. To disable the removal of duplicate rows, you can specify ALL instead. These clauses in Sclera are compliant with the SQL standard and PostgreSQL .","title":"UNION/INTERSECT/EXCEPT Clause"},{"location":"sclerasql/sqlregular/#cross-system-set-operations","text":"When the clause contains queries evaluated at more than one system, it specifies a cross-system set operation. To evaluate a cross-system set operation, Sclera needs both the query results to be present at a single location; let us call this the \"target location\" for the set operation. This target location is decided as follows: For each query, Sclera finds the location of the query result after evaluation. These locations are the candidates for the target location, and are listed in the order of appearance of the corresponding queries. From this list, Sclera then removes the Cache Store , if present, as well as the \"read-only\" locations . If the list is empty, Sclera assigns the Cache Store as the target location. This has the effect that cross-system set operations across read-only locations are evaluated by evaluating both the queries, and moving the query results to the cache store; the set operation is then computed at the cache store. If the list is not empty, Sclera assigns the location on the left in the list as the target location. This has the effect that the query in the other location is evaluated, and its result is moved to the target location. The set operation is then computed along with the evaluation of the query in the target location. The ordering of the queries thus matters when evaluating cross-system set operations. While this enables you to control how data is moved while evaluating the queries containing set operations, you need to pay special attention to this ordering -- especially when significant amounts of data needs to be transfered. In any case, when evaluating a query with a cross-system set operation, please take a close look at the query's evaluation plan (obtained using the EXPLAIN shell command ) before submitting the query. In the current version, Sclera moves data from a \"source\" to a \"target\" database system by reading in the data from the source and inserting it into a temporary table in the target. This transfer is done in a streaming (pipelined) manner wherever possible, to avoid reading the entire result in memory. This could be a bottleneck when large amounts of data (millions of rows) are transferred. More efficient data transfer mechanisms will be in place in later versions of Sclera.","title":"Cross-System Set Operations"},{"location":"sclerasql/sqlregular/#order-by-clause","text":"The ORDER BY clause specifies a list of scalar expressions that determines the sort order of the computed result. The optional modifiers ASC (for ascending ) or DESC (for descending ) associated with each expression in the ORDER BY list determine how that particular expression should be considered while determining the overall sort order. When ASC is specified, the larger values are ordered after the smaller values, and the NULLS are ordered after all values. When DESC is specified, the smaller values are ordered after the larger values, and the NULLS are ordered before all values. The default is ASC . The optional NULLS FIRST and NULLS LAST modifiers force the NULLS to be ordered before or after all values. The ORDER BY clause in Sclera is mostly compliant with the SQL standard and PostgreSQL . In SQL standard and PostgreSQL , you can include an expression in the SELECT clause in the ORDER BY list by specifying its position number. This is not allowed in Sclera; instead, you can associate the SELECT expression with an alias ( output_name in the syntax stated above) and use that in the ORDER BY list.","title":"ORDER BY Clause"},{"location":"sclerasql/sqlregular/#limit-fetch-and-offset-clauses","text":"The LIMIT clause retains the specified number of initial rows in the result. If the result has less than the specified number of rows, this clause has no effect. The FETCH clause is an alternative to LIMIT , with a relatively verbose syntax. Here, again, you can specify the number of initial rows to retain in the result; but you can also say FETCH FIRST ROW ONLY to retain only the first row, if present. In general, the words FIRST , NEXT , ROW and ROWS are optional syntactic sugar; the number of initial rows, if not specified, defaults to 1 . A query can have either the LIMIT clause or the FETCH clause. The OFFSET clause discards the specified number of initial rows in the result. If the result has less than the specified number of rows, all rows are discarded. When both the OFFSET and the LIMIT (or FETCH ) clauses are present in a query, the OFFSET clause applies before the LIMIT (or FETCH ) clause. In absence of an ORDER BY clause , the query result is in arbitrary order and consequently, the retained/discarded row set is not deterministic (i.e. a different set of rows may be retained/discarded on each invocation). As such, the recommended use of these clauses is in conjunction with the ORDER BY clause . The FETCH and OFFSET clauses in Sclera are compliant with the SQL standard and PostgreSQL . The LIMIT clause is supported in PostgreSQL , but not in the SQL standard.","title":"LIMIT, FETCH and OFFSET Clauses"},{"location":"sclerasql/sqlregular/#querying-data-using-generalized-table-expressions","text":"In standard SQL, as well as in PostgreSQL and other relational databases, a query is restricted to the SELECT statement described above . Sclera generalizes the query syntax such that anything that can appear in the FROM clause , possibly without the alias, is accepted as a query. We call this a table_expression . To recap, this includes: A standard SELECT statement . As a result, any query in the earlier query syntax remains admissible; this is indeed a generalization of the \"standard\" select expression / query discussed above . A base table name, possibly qualified with a location. You can rename the table and the columns the same way you could in the FROM clause . Syntax: [location_name.]table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ] (see above for explanation) A values expression, which explicitly lists out the rows. Syntax: VALUES ( constant [, ...] ) [, ...] [ AS ] alias [ ( column_alias [, ...] ) ] (see above for explanation) A join expression involving nested table_expression elements. Syntax: table_expression [ NATURAL ] join_type table_expression [ ON join_condition | USING ( join_column [, ...] ) ] (see above for explanation) A table expression in parenthesis, with optional alias. Syntax: ( table_expression ) [ [ AS ] alias [ ( column_alias [, ...] ) ] ] In addition, a table_expression can have additional WHERE , ORDER BY and LIMIT / OFFSET / FETCH clauses. This is not supported in the standard SQL FROM clause. A table expression with a WHERE clause . Syntax: ( table_expression ) WHERE condition A table expression with an ORDER BY clause . Syntax: ( table_expression ) ORDER BY expression [ ASC | DESC ] [ NULLS { FIRST | LAST } ] [, ...] ] A table expression with LIMIT , FETCH and/or OFFSET clauses Syntax: ( table_expression ) [ LIMIT count [ ROW | ROWS ] ] [ OFFSET start [ ROW | ROWS ] ] [ FETCH [ FIRST | NEXT ] [ count ] [ ROW | ROWS ] ONLY ] Using this generalized syntax, to get the contents of the table ORDERS , you just need to say > ORDERS; instead of the unnecessarily verbose > SELECT * FROM ORDERS; To filter and limit, you can say > ORDERS WHERE CUSTID = 10 LIMIT 3 instead of > SELECT * FROM ORDERS WHERE CUSTID = 10 LIMIT 3 Similarly, to get the join of the tables ORDERS and CUSTOMERS , you just need to say > ORDERS JOIN CUSTOMERS USING (CUSTID); instead of the verbose > SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID); Further, we can union the above with another table PREV by saying > ORDERS JOIN CUSTOMERS USING (CUSTID) UNION PREV; instead of the standard > SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID) UNION SELECT * FROM PREV; The queries in the verbose standard SELECT syntax are still valid, but the equivalent queries in the extended syntax are more compact. As we can see, the difference is more pronounced in more complex queries.","title":"Querying Data Using Generalized Table Expressions"},{"location":"sclerasql/sqlregular/#base-tables","text":"In this section we describe Sclera's SQL commands for creating, updating and deleting base tables.","title":"Base Tables"},{"location":"sclerasql/sqlregular/#creating-base-tables","text":"Creating tables in Sclera is different from adding tables to Sclera using the ADD TABLE command . The latter merely imports the metadata of an existing table into Sclera. The create table commands, described below, create a new table. The net effect of these commands is (a) creating the table on the underlying database system, and (b) adding the table metadata to Sclera's metadata store. If the database system understands SQL (e.g. MySQL ), Sclera creates the table by generating the appropriate CREATE TABLE command, taking care of dialect and datatype differences; if the database system does not understand SQL (e.g. NoSQL datastores), Sclera uses the system's API (or any other available mechanism) to create an underlying structure that is compatible with the metadata specified in the command. The CREATE TABLE command has two variants. The first variant creates an empty table. The second variant creates a table, and also populates it with the result of a query.","title":"Creating Base Tables"},{"location":"sclerasql/sqlregular/#creating-empty-tables","text":"The syntax of the CREATE TABLE command is as follows: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) This creates a new, empty table (persistent or temporary ) with the name table_name at location location_name (or the default location , if location_name is omitted), consisting of columns with names and types given by the associated column_name and column_type , and with column and table constraints specified by column_constraint and table_constraint . These terms are described in turn below.","title":"Creating Empty Tables"},{"location":"sclerasql/sqlregular/#temporary-modifier","text":"The optional TEMPORARY (shortened form: TEMP ) modifier in CREATE TEMPORARY TABLE creates a table that exists for the duration of the Sclera session only. When the session ends, the table is deleted from the underlying database system and its metadata is removed from Sclera's metadata store.","title":"Temporary Modifier"},{"location":"sclerasql/sqlregular/#table-name","text":"The mandatory table_name is the name of the table to be created. You can optionally qualify the name with location_name , the name of the location where the table is to be created. If the location_name is omitted, the table is created at the default location .","title":"Table Name"},{"location":"sclerasql/sqlregular/#columns-and-their-types","text":"A table needs to have at least one column. You can specify the list of columns and their types as a comma-separated list of column_name and data_type pairs. The data_type must be one of the data types supported by Sclera .","title":"Columns and Their Types"},{"location":"sclerasql/sqlregular/#column-constraints","text":"With each column in the column list, you can optionally specify a set of column constraints. Each column_constraint can be one of: NOT NULL : This column can never be NULL . The constraint is checked while inserting rows into the underlying table. PRIMARY KEY : This column is the primary key of the table -- as such, it must never be NULL , and a value of the column must be unique across all the rows in the table. This constraint can be specified for at most one column, and cannot be specified along with a PRIMARY KEY table constraint . REFERENCES [location_name.]table_name [ (column_name) ] : This column references the table table_name . The table with the name specified by table_name must be present in an underlying database system (if table_name is not unique across locations, the location_name must be specified). Further: If the column_name is specified, the table table_name must contain a column with the name specified by column_name of the same type as this column and with a PRIMARY KEY constraint. Also, for each row in this table, either this column is NULL , or there exists a row in table table_name with a matching value in the column column_name . If the column_name is not specified, the table table_name must have a primary key, which must be of the same type as this column. Also, for each row in this table, either this column is NULL , or there exists a row in table table_name with a matching primary key value. When the column_name is not specified, or equivalently, when it is the primary key of the table table_name , this column is called a foreign key (to table table_name ).","title":"Column Constraints"},{"location":"sclerasql/sqlregular/#table-constraints","text":"A table constraint is a constraint on an entire row of the table, rather than a single column. Table constraints are more general than column constraints; each column_constraint mentioned above can be equivalently stated as a table_constraint . However, table constraint also allows you to specify multi-column primary keys and references/foreign keys. The optional table_constraint list appears as a continuation of the column list . Each table_constraint can be one of: PRIMARY KEY ( column_name [, ...] ) : The set of columns within the parenthesis the primary key of the table. As such, each of these columns can never be NULL , and the combined set of values of these columns must be unique across all the rows in the table. This constraint cannot be specified along with a PRIMARY KEY column constraint . FOREIGN KEY ( column_name [, ...] ) REFERENCES [location_name.]table_name [ ( column_name [, ...] ) ] : The list of columns references the table table_name . The table with the name specified by table_name must be present in an underlying database system (if table_name is not unique across locations, the location_name must be specified). Further: If the reference table's column_name list is specified, (a) it must have the same number of columns as the foreign key list, (b) the table table_name must contain each column in the reference list and these columns must collectively have a PRIMARY KEY constraint, and (c) each column of the reference list must have the same type as the corresponding column (i.e. at the same position) in the foreign key list. Also, for each row in this table, either some column in the foreign-key list is NULL , or there exists a row in table table_name with a matching set of values of the respective columns in the reference list. If the reference table's column_name list is not specified, the table table_name must have a primary key, which (a) must have the same number of columns as the foreign key list, and (b) each column of this primary key must have the same type as the corresponding column (i.e. at the same position) in the foreign key list. Also, for each row in this table, either some column in the foreign-key list is NULL , or there exists a row in table table_name with a matching set of values of the respective columns in the primary key. Note that Sclera does not enforce the column or table constraints described above. Sclera passes on these constraints to the underlying database system, if possible, and it is the responsibility of the underlying database system to check for these constraints. Nevertheless, Sclera assumes that these constraints hold while querying the data in the tables. While creating tables on database systems which do not enforce such constraints, it is the responsibility of the user to make sure that these constraints hold while inserting rows into the table. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL .","title":"Table Constraints"},{"location":"sclerasql/sqlregular/#creating-tables-with-query-results","text":"This variant of the CREATE TABLE statement enables you to create a table, and populates it with the result of the given query. This is more efficient than performing the two tasks one after another, especially in transactional systems such as MySQL and PostgreSQL . Also, this is sometimes more convenient because you need not specify the schema explicitly; if not specified, the schema is derived from the output schema of the given query. The standard syntax of the CREATE TABLE AS command is as follows: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] AS select_expression where select_expression is as described earlier . This is almost identical to the CREATE TABLE syntax , except the new select_expression , and that the schema description (column definitions, column and table constraint specifications) is now optional. Sclera generalizes this expression to accept a table_expression instead of a select_expression . The generalized syntax is: CREATE [ { TEMPORARY | TEMP } ] TABLE [location_name.]table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] AS table_expression Note that since select_expression is a table_expression , statements in the standard syntax continues to be valid in this updated syntax. As in the case of queries , this generalization simplifies the CREATE TABLE AS statements. For instance, you can say: > CREATE TABLE CUSTORDERS AS ORDERS JOIN CUSTOMERS USING (CUSTID); instead of the standard: > CREATE TABLE CUSTORDERS AS SELECT * FROM ORDERS JOIN CUSTOMERS USING (CUSTID); This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL .","title":"Creating Tables with Query Results"},{"location":"sclerasql/sqlregular/#inserting-rows-into-a-base-table","text":"The INSERT statement inserts rows into an existing base table. The standard syntax of the INSERT statement is as follows: INSERT INTO [location_name.]table_name [ ( column_name [, ...] ) ] (select_expression | values_expression) Here, table_name is a base table at location location_name , into which the rows in the result of select_expression or value_expression will be inserted. The select_expression stands for the SELECT statement described earlier , and the value_expression stands for the VALUES clause discussed earlier , possibly without the alias. In the above, the location_name can be omitted if table_name is unique across locations. Sclera generalizes the syntax to: INSERT INTO [location_name.]table_name [ ( column_name [, ...] ) ] table_expression wherein the select_expression or value_expression have been replaced with table_expression . Recall that a select_expression or a value_expression is a table_expression as well; so the standard syntax continues to be valid. The optional column_name explicitly lists the table columns; each column in the list must be present in the table schema. If this list is not specified, it is taken to be the list of columns in the schema of the table table_name , in the order in which they were listed while creating the table. Given this explicit/implicit list, the following should hold: It must have the same number of columns as the result of the table_expression . For each column in the list, the type of a column in the table table_name must be compatible with the type of the corresponding (i.e. at the same position) column in the result. If the column_name list is explicitly specified, any column in the table table_name which does not appear in the list gets a NULL for every row inserted. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL .","title":"Inserting Rows into a Base Table"},{"location":"sclerasql/sqlregular/#updating-rows-in-a-base-table","text":"The UPDATE statement updates the value of one or more columns in rows in a base table. The syntax supported in Sclera is as follows: UPDATE [location_name.]table_name SET { column_name = scalar_expression } [, ...] [ WHERE condition ] Here, table_name is the name of the table to be updated. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. The optional WHERE clause specifies the condition (a boolean-valued scalar_expression ) that a row in the table must satisfy to be eligible for the update; rows that do not satisfy this condition are left unchanged. If the WHERE clause is not present, the update applies to all rows in the table. The update to a row is specified by the SET clause, which contains a comma-separated list of \"assignments\". Each assignment specifies that given an eligible row, the column with the name specified by column_name should get the value obtained by evaluation of the scalar_expression on that row. The update proceeds by executing the specified list of assignments on each eligible row. This syntax is compatible with the SQL standard, but is a restricted version of the syntax supported in PostgreSQL .","title":"Updating Rows in a Base Table"},{"location":"sclerasql/sqlregular/#deleting-rows-in-a-base-table","text":"The DELETE statement deletes rows in a base table. The syntax supported in Sclera is as follows: DELETE FROM [location_name.]table_name [ WHERE condition ] Here, table_name is the name of the table from which rows are to be deleted. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. The optional WHERE clause specifies the condition (a boolean-valued scalar_expression ) that a row in the table must satisfy to be eligible for deletion; rows that do not satisfy this condition are left unchanged. If the WHERE clause is not present, all rows in the table are deleted. This syntax is a restricted version of that supported in the SQL standard and PostgreSQL .","title":"Deleting Rows in a Base Table"},{"location":"sclerasql/sqlregular/#dropping-base-tables","text":"The DROP TABLE statement drops a base table from Sclera as well as the underlying database system. This statement is similar to the REMOVE TABLE statement . However, REMOVE TABLE only removes the metadata of the named table from Sclera; it does not drop the table from the underlying data store. DROP TABLE remove the table's metadata from Sclera and also drops the table from the underlying datastore. The syntax supported in Sclera is as follows: DROP TABLE [location_name.]table_name Here, table_name is the name of the table to be dropped. The table must exist at the location location_name . The location_name can be omitted if table_name is unique across locations. This syntax is a restricted version of that supported in the SQL standard and PostgreSQL .","title":"Dropping Base Tables"},{"location":"sclerasql/sqlregular/#views","text":"A view is a SQL query with the appearance of a read-only table. A view can be used as a base table in any query . But, unlike a base table, a view does not contain any real data; it is just a placeholder for the associated query.","title":"Views"},{"location":"sclerasql/sqlregular/#creating-views","text":"The standard CREATE VIEW statement has the following syntax: CREATE [ TEMP | TEMPORARY ] VIEW view_name AS select_expression Sclera generalizes the select_expression to a table_expression , resulting in: CREATE [ TEMP | TEMPORARY ] VIEW view_name AS table_expression This statement creates a view with the specified view_name and associates it with the query given by table_expression . Notice the similarity with the CREATE TABLE AS statement . The optional TEMPORARY or TEMP modifier creates a view that exists for the duration of the Sclera session only. When the session ends, the view is deleted and its metadata is removed from Sclera's metadata store. Unlike a base table, a view is not attached to a location ; this is because the underlying query (specified by the table_expression above) can span multiple locations. This statement is restricted in comparison, but compliant with the SQL standard and PostgreSQL .","title":"Creating Views"},{"location":"sclerasql/sqlregular/#dropping-views","text":"The DROP VIEW statement drops a view from Sclera. The syntax supported in Sclera is as follows: DROP VIEW view_name Here, view_name is the name of the view to be dropped. This syntax is compatible with the SQL standard and is a restricted version of that supported in PostgreSQL .","title":"Dropping Views"},{"location":"sclerasql/sqlseqalign/","text":"The ScleraSQL extension ALIGN enables you to align rows in two sequences, say two stock tickers, in a way as to minimize the total \"distance\" between the rows. The distance is arbitrary and is specified as a part of the ALIGN clause. We can think of ALIGN as a type of JOIN for ordered sequences. The difference from relational joins is that in the latter, any row can join with any row as long as the join condition is met; in ALIGN we also have the additional ordering constraint in that the rows need to be aligned in order. We can also provide an additional constraint to ALIGN by specifying a margin - the maximum number of skipped rows between two aligning rows. A margin of m will force the row at position i in one input to be aligned only rows between positions i-m and i+m in the other input. The alignment is done using a technique called Dynamic Time Warping . The syntax is similar to the SQL JOIN syntax, and is as follows: table_expression ALIGN table_expression [ ON distance [ MARGIN margin ] ] where: table_expression is an arbitrary table expression distance is a numeric scalar expression that gives the distance between a row in the left input and a row in the right input margin specifies the maximum number of skipped rows between two aligned rows, as explained earlier If the distance is not specified or is a constant, or if the margin is zero , the operator simply aligns the rows at position i in one input with the row in position i in the right input.","title":"Sequence Alignment"},{"location":"sclerasql/visualization/","text":"Sclera's visualization component, ScleraViz, enables quick and easy visualization of your query results. ScleraViz is integrated with ScleraSQL ; this means a few lines of ScleraSQL can fetch, clean, analyze and visualize your data in a single sweep. ScleraViz is inspired by Grammar of Graphics , specifically R's ggplot2 -- but is implemented as an extension to ScleraSQL and uses D3 as the rendering engine. Moreover, unlike ggplot2, ScleraViz can clean, analyze and plot streaming data. An online preview, with a number of examples with code, is available at https://scleraviz.herokuapp.com . In this document, we first describe how to set up Sclera to use ScleraViz , then illustrate ScleraViz by example , and finally give the detailed syntax of ScleraSQL visualization queries . Command Line Setup \u25c4 In the following, <sclera-root> is the directory where Sclera is (or, is to be) installed. To run ScleraViz queries, you need to: Install Sclera Command Line using scleradmin ( detailed instructions here ) $ scleradmin --install --root <sclera-root> Install the Sclera Web Display plugin $ scleradmin --add sclera-plugin-webdisplay --root <sclera-root> If you need to read CSV data, install the CSV plugin as well $ scleradmin --add sclera-plugin-csv --root <sclera-root> Start the Sclera Command Line Shell $ <sclera-root>/bin/sclera Welcome to Sclera 4.0 > _ In the Sclera Command Line Shell, start the display web server: Welcome to Sclera 4.0 > display start; HTTP server started at port 7070 Please point your browser to http://localhost:7070 To start the server at a port other than 7070, you need to specify the port number explicitly at the end. For example, display start 8080; starts the server at localhost:8080 . Start a web browser and open the URL http://localhost:7070 Running a ScleraViz query (described in the rest of this document) on the Sclera Command Line will display the visualization in the browser. In addition, to display the contents of a query as a table in the browser, you can say: > display <query>; For example, the following displays the result as a two column, single row table in the browser: > display select 'Hello' as greeting1, 'World' as greeting2; Leaving the keyword display out displays the result in the command line shell, as usual. You can also display arbitrary text (say, comments) as well: > display \"The plot above is amazing!\"; When done, you can close the display web server by saying: > display stop; ScleraViz by Example \u25c4 This section very briefly introduces the rudiments of the plot specification. This is to quickly get you started on plotting your own graphs. A complete and formal description of the syntax appear in the next section . Example Dataset In the following, we will be using the \"tipping\" dataset as our running example. The data set contains information about tips received by a waiter in a restaurant over a period of time. The information includes the tip in dollars, total bill in dollars, gender of the bill payer, whether there were smokers in the party, day of the week, time of day, and size of the party. Further details on the data can be found here . To use the dataset, set up the tips table as follows: CREATE TEMP TABLE tips as EXTERNAL CSV(\"http://scleraviz.herokuapp.com/assets/data/tips.csv\") TYPEINFER(LIMIT 1); Notation: In the above and the following examples, the ScleraSQL keywords will be capitalized. This is for illustrative reasons only; ScleraSQL is case-insensitive. A Simple Plot \u25c4 Putting a PLOT afer the query plots the column y in the query result against the column x in the query result as a scatter plot. So, to plot the tip against the total_bill in the tipping dataset, we say: SELECT total_bill as x, tip AS y FROM tips PLOT; The above statement works on a bunch of defaults -- it creates a scatter-plot, which takes column x in the input as the default for the X coordinate, and column y in the input as the default for the y coordinate, and also takes default values for aesthetics such as point fill and stroke color, size, opacity (alpha), etc. In the discussion that follows, we show how to explicitly override these defaults, and even vary them as a function of the data. Specifying the Geometry \u25c4 A \"geometry\" refers to the manner in which a data point maps to the chart. In the scatterplot example above, the geometry is a POINT . We can explicitly specify the geometry as follows: SELECT total_bill as x, tip AS y FROM tips PLOT(GEOM=POINT) The location of a geometry is determined by a set of parameters. By default, these parameters are mapped to appropriately named columns in the input. For instance, each point above is placed according to the values in the x and y columns. We can override this default by explicitly specifing custom columns for these parameters. So, another way to write the example above is: tips PLOT(GEOM=POINT(x=total_bill, y=tip)) Here, the parameters x and y are mapped to input columns total_bill and tip respectively; in general, they can be mapped to arbitrary expressions. Sclera supports a number of alternative geometries. For instance, to plot the average bill for a given party size against the party size as a histogram, we can say: SELECT `size`, AVG(total_bill) as avg_bill FROM tips GROUP BY `size` PLOT(GEOM=BAR(x=`size`, y=avg_bill)) (Note: size needs to be in backquotes as it is a keyword. This is a parser requirement; we understand that it is cumbersome and are working on workarounds. Meanwhile, it is always a good idea to put the column names in backquotes.) Similarly, to plot the same data as a line, we use GEOM=LINE : SELECT `size`, AVG(total_bill) as avg_bill FROM tips GROUP BY `size` PLOT(GEOM=LINE(x=`size`, y=avg_bill)) A full list of the supported geometries appears as a part of the detailed documentation . Aesthetics and Legends \u25c4 The aesthetics parameters specify the \"look and feel\" of a chart. For a scatterplot, for instance, the parameters specify the shape of the point (circle, square, triangle, and so on), the color to be used to fill the point, the color used for the boundary, and so on. The set of aesthetics parameters varies with the geometry. For example, a SHAPE makes sense for a point, but does not make sense for a line. The list of aesthetics parameters associated with respective geometries appears as a part of the detailed documentation . An aesthetics parameter can either be a constant, or an expression. If the parameter is a constant, e.g. FILL=\"black\" , then the value of the aesthetics is literally taken as the constant. If the parameter is an expression, then distinct values of the columns are mapped to distinct values of the aesthetic. For instance, if we specify FILL=foo where foo is an input column, then by default, each distinct value of foo is mapped to a distinct value of the color that is actually used for the fill. The mapping from each distict value of the column foo to the corresponding color is automatically maintained, and can be output as a legend, by saying FILL=foo LEGEND . The following example shows a scatterplot with expressions for FILL and SHAPE aesthetics. tips PLOT( GEOM=POINT(x=total_bill, y=tip), FILL=day LEGEND(TITLE=\"Day\"), SHAPE=`time` LEGEND(TITLE=\"Time\") ) A full description of the aesthetics parameters appears in the detailed documentation . Group \u25c4 The optional GROUP directive specifies an expression on the input data. The input data points are first partitioned based on the values of this expression (similar to the SQL GROUP BY ), and each partition is plotted separately in the same layer. For instance, adding a GROUP directive to a LINE will plot a line for each distinct value of the GROUP expression. SELECT `size`, gender, AVG(total_bill) as avg_bill FROM tips GROUP BY `size`, gender ORDER BY `size` PLOT(GEOM=LINE(x=`size`, y=avg_bill), GROUP=gender, STROKE=gender LEGEND) Key \u25c4 By default, each row in the input corresponds to a unique data point. The optional KEY directive enables associating multiple rows with the same data point -- successive rows with the same key value render as the same data point, overriding the previous rendering. This is helpful to display, say, a dynamic bar chart showing a running average for tips collected on each day of week. We specify KEY=day and, for every input row, generate a row containing the updated running average of tip for the associated value of day . With KEY=day , the first rows for a value create a bar, and the subsequent roes for the same value update the existing bar. SELECT day, T.avg(tip) as running_avg FROM tips T PARTITION BY day PLOT(GEOM=BAR(x=day, y=running_avg), KEY=day) Without KEY=day , a new bar will be generated for each update. SELECT day, T.avg(tip) as running_avg FROM tips T PARTITION BY day PLOT(GEOM=BAR(x=day, y=running_avg), ALPHA=0.1) Position Adjustments \u25c4 Following the grammar of graphics philosophy, the input data is assumed to have one row per data point. When two data points are rendered one over the other, they are said to \"collide\". Colliding data points can be repositioned by specifying a POSITION directive, which can take values dodge , stack , or jitter . Specifying POSITION=DODGE positions the data points in a row, one after other, around the common x position on the X axis. When the geometry is BAR , this gives the familiar \"grouped\" bar chart: SELECT day, gender, count(*) AS count FROM tips GROUP BY day, gender ORDER BY day, gender PLOT(GEOM=BAR(x=day, y=count), POSITION=DODGE, FILL=gender LEGEND) Specifying POSITION=STACK positions the data points in a column, one over other, at the common x position on the X axis. When the geometry is BAR , this gives the familiar \"stacked\" bar chart: SELECT day, gender, count(*) AS count FROM tips GROUP BY day, gender ORDER BY day, gender PLOT(GEOM=BAR(x=day, y=count), POSITION=STACK, FILL=gender LEGEND) Specifying POSITION=jitter adds a random offset to both the x and y positions of the data point. This is useful when we have overlapping points, so that the top points obscure the ones below -- the jitter randomly scatters the obscured points within a small ball around their position, making them visible. Stat \u25c4 The STAT directive specifies a statistical operation over the input data points. Supported operations include computing local regression smoothing, computing density histograms, and computing heat-maps. By default, the results are computation are overlayed over the base plot. For instance, consider the scatter-plot of average tips against the total bill. Including STAT=loess plots a line representing local regression smoothing over these data points. This curve brings out the trend in the data -- how the tips vary with increasing total_bill -- that may not be apparent from just the scatter plot. SELECT total_bill, avg(tip) as avg_tip FROM tips GROUP BY total_bill PLOT( GEOM=POINT(x=total_bill, y=avg_tip), STAT=LOESS(STROKE=\"orange\", STROKE_WIDTH=\"5px\") ) The computed result is plotted in its own layer (see the next section), using a geometry that is determined by the operation, and using aesthetic parameters that can be provided along with the operation, as in the example above. Mark \u25c4 The MARK directive marks out regions where a specified predicate is true. This is done in real time, as the data arrives, and so can be very useful in monitoring for events. For example, the following query marks regions in a stock ticker where the current high is greater than the running average high with green, and regions where the current low is less than the running average low with red. EXTERNAL CSV(\"http://scleraviz.herokuapp.com/assets/data/infy.csv\") TYPEINFER(LIMIT 1) AS infy PLOT( GEOM=LINE(x=ts, y=close), MARK XAXIS( high > infy.AVG(high), FILL=\"green\" ), MARK XAXIS( low < infy.AVG(low), FILL=\"red\" ) ) Multi-Layered Plots \u25c4 A layer represents a unit of rendering in the plot. The examples so far, except the stats, included a single layer -- which is the default. A layer can only support one kind of geometry, and the associated set of aesthetic parameters and position specifications. If you want to include, say, a scatter plot and a line in the same plot, you need to have separate layers for the points and the line. You can explicitly specify layers using the LAYER keyword. The initial scatterplot can be equivalently be specified as: tips PLOT(LAYER(GEOM=point(x=total_bill, y=tip))) We can specify multiple layers in the same plot. The following plots the average bill for a party size, along with a one-standard deviation interval in a separate layer: SELECT `size`, avg(total_bill) as avg_bill, stddev(total_bill) as sdev FROM tips GROUP BY `size` ORDER BY `size` PLOT( LAYER(GEOM=LINE(x=`size`, y=avg_bill)), LAYER(GEOM=RIBBON(x=`size`, ymin=avg_bill - sdev, ymax=avg_bill + sdev)) ) Another example on the same query, this time with bars and error lines: SELECT `size`, avg(total_bill) as avg_bill, stddev(total_bill) as sdev FROM tips GROUP BY `size` ORDER BY `size` PLOT( LAYER(GEOM=BAR(x=`size`, y=avg_bill)), LAYER( GEOM=POINTRANGEY( x=`size`, y=avg_bill, ymin=avg_bill - sdev, ymax=avg_bill + sdev ) ) ) The STAT directive mentioned in the previous section implicitly adds a new layer to the plot, with the result of the associated operation. Multi-Plots \u25c4 We can have multiple plots on the same input data; these plots are plotted simultaneously in a single scan of the input. For instance, the following specification renders two separate plots on the same input: tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) The plots are plotted one after the other, in separate rows. As the plots are rendered, the Y-axes of the first two plots, which plot the same column tip , and the X-axes of the last two plots, which plot the same column size are kept synchronized. Automatic Layouts \u25c4 ScalerViz provides an aligned layout mode that places the plots so that plots sharing the same X-axis columns are aligned vertically, and the plots sharing the same Y-axis columns are aligned horizontally. We try this mode, activated using the LAYOUT ALIGNED directive on the example above, to get: tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) LAYOUT ALIGNED Weighted Axes \u25c4 In the multi-plot examples above, all axes have the same size. However, we may sometimes want to emphasize some columns over the others. In ScleraViz, we can assign weights to axes. By default, all axes are assigned a weight of 1. Assigning a weight of, say 0.5, makes the size of an axis half of an axis with weight 1. Similarly, assigning a weight of 2.0 makes the size of an axis twice that of an axis with weight 1. The actual axis lengths are assigned respecting these relative size constrainst and the constraint that all the plots need to fit in the given width and height. tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) AXIS `size`(WEIGHT=0.25, TICKS=3) AXIS tip(WEIGHT=2) LAYOUT ALIGNED Cross Plots \u25c4 It is common to plot each of a set of columns against each of another set of columns. In ScleraViz, we can plot such \"cross-plots\" by using the single-layer PLOT syntax, and specifying a list of columns for the parameters of the geometry. tips PLOT( GEOM=POINT(x=(smoker, gender, day, `time`), y=(total_bill, tip)), ALPHA=0.3 ) LAYOUT ALIGNED Faceted Plots \u25c4 Faceting a plot on an expression involves displaying separate plots for datapoints partitioned by distinct values of the expression. ScleraViz can facet along columns (meaning that the plots corresponding to distinct values are layed out horizontally, in separate columns in a grid), or have faceting along rows (meaning that the plots corresponding to distinct values are layed out vertically, in separate rows in a grid), or both. For instance, the following plot separates out the tip and total bill data based on whether smokers were present (yes/no, along rows), and the gender of the bill-payer (male/female, along columns): tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) FACET(ROWS=smoker, COLUMNS=gender) Detailed Syntax \u25c4 This section gives the detailed formal syntax of the ScleraViz specification language. Formally, the ScleraViz specification is as follows (recall that | means alternatives, [] means optional, and ... means repetitions): query plot_spec [ , ... ] [ facet_spec ] [ axis_spec ] [ ... ] [ layout_spec ] [ grid_spec ] [ display_spec ] where the query is a ScleraSQL query . The plot specification plot_spec can appear one or more times, each plot_spec defining a new plot, followed by an optional facet specification facet_spec , zero or more axis_spec if we need to change the axis defaults. A layout_spec then gives the layout preferences. This is followed by a grid_spec if we need to change the grid defaults, and a display_spec to change the display area defaults. Plot / Layer Specification \u25c4 The plot_spec specifies a plot, and has two variants: PLOT ( layer_params ) | PLOT ( LAYER ( layer_params ) [, ...] ) The first variant is an shortened form of the second, and can be used when the plot has only one layer. The second variant is needed when the plot has multiple layers. Each layer_param specifies a layer in the plot. It contains several parts, each of them except geometry being optional, which can be specified in any order. We describe the sybtax of each part in the next few sections. Geometry \u25c4 The geometry specifies the manner in which the datapoint will be represented in the chart. The syntax is as follows: GEOM = geom_type [ ( geom_params ) ] The geom_type is the type of the geometry, and geom_params is a comma-separated list of key = value entries. The keys for each geometry are given the following table; the values for the keys can be a numeric scalar expression on the underlying table (or a constant). geom_type geom_params keys POINT or SCATTER X, Y LINE X, Y POINTRANGEY X, Y, YMIN, YMAX ERRORBAR or RANGEY X, YMIN, YMAX, [WIDTH] ERRORBARH or RANGEX XMIN, XMAX, Y, [HEIGHT] RIBBON X, YMIN, YMAX ABLINE YINTERCEPT, SLOPE HLINE Y VLINE X SEGMENT X, XEND, Y, YEND ARROW X, XEND, Y, YEND AREA X, Y RECT XMIN, XMAX, YMIN, YMAX HISTOGRAM or HIST XMIN, XMAX, FREQ BAR X, Y REGIONX XMIN, XMAX, YMIN, YMAX REGIONY XMIN, XMAX, YMIN, YMAX OHLC TS, OPEN, HIGH, LOW, CLOSE, [WIDTH] CANDLESTICK TS, OPEN, HIGH, LOW, CLOSE, [WIDTH] In the above table, keys enclosed in [] are optional, others are required. The geom_params are optional, and may only specify the values for a subset of the required keys for the geometry. When a required key needed by a geometry does not appear in the geom_params , it is mapped to a column of the same name in the input. The GEOM specification itself is optional -- if omitted, it defauts to POINT , with the keys X and Y mapped to input columns of the same name. Cross-Plots \u25c4 In the single-layer variant, the geom_param keys can have a list of expressions instead of the single expression allowed above. This is considered a shortcut, and expands to a separate plot for each value in the list. If multiple geom_param keys have a list of expressions, then the specification expands to a separate plot for each combination of the geom_param values across these lists. Apart from the geom_param values, each of the plots so generated have the directives and properies as specified in the rest of the PLOT specification. Aesthetics, Scales, and Legends \u25c4 The aesthetics directives change the default look (colors, shape, size, etc.) of the data points, and also assign legends. The syntax is as follows: aes_key = aes_value [ SCALE = scale [ ( scale_params ) ] ] [ ON NULL null_aes_const ] [ LEGEND [ ( legend_params ) ] ] The aes_key is an attribute such as FILL , STROKE , etc. (see the table below), and aes_value has the syntax: aes_const | aes_expr | IDENTITY ( aes_identity_expr ) aes_const is a constant of the appropriate type as specified in the table below. For example, FILL = \"red\" . aes_expr is a scalar expression of the appropriate type as specified in the table below. A scale maps distinct values of this expression to the appropriate value for the aes_key . For instance, specifying FILL = foo maps distinct values in the column foo to distinct colors from a default palette. The palette can be explicitly specied using a scale ( see below ). aes_identity_expr is an expression that evaluates to the literal value of an ordinal type. For instance, if a columns foo contains valid color names such as \"red\", \"blue\", \"black\", etc., then we can specify IDENTITY(foo) as the aes_value . Each geometry has an associated list of aes_key alternatives that it can interpret; aes_key specifications that are not associated with the specified geometry are ignored. The keys aes_key , types of aes_value and associated geometries are listed in the table below. aes_key Type Geometry Description SIZE Numeric POINT, POINTRANGEY Specifies the size of the point SHAPE Ordinal ( valid shape values ) POINT, POINTRANGEY Specifies the shape of the point FILL Ordinal ( valid color values ) POINT, POINTRANGEY, AREA, BAR, RIBBON, OHLC Color used to fill the shape ALPHA Numeric (between 0 and 1) all geometries Opacity of the rendering STROKE Ordinal ( valid color values ) all geometries Color used for the line STROKE-WIDTH Numeric all geometries Specifies the width of the line INTERPOLATE Ordinal ( valid interpolate values ) LINE, AREA, RIBBON Sets the path interpolation mode TENSION Numeric (between 0 and 1) LINE, AREA, RIBBON Sets the path tension Scale \u25c4 Using a non-constant expression aes_expr for the aes_value makes the aesthetics a function of the data. The mapping from the distinct values of the expression (e.g. day of the week) to the aesthetics specification (e.g. colors \"red\", \"blue\", \"green\") is called a \"scale\". The scale is numeric if it outputs numeric values; examples of numeric scales are: LINEAR , LOG , and SQRT . Further information on numeric scales, their defaults and capabilities appears in the D3 documentation . The scale is ordinal if it outputs ordinal values; examples of ordinal scales are ORDINAL , COLOR , and SYMBOL .Further information on the ordinal scales, their defaults and capabilities appears in the D3 documentation . To explicitly specify the scale, we specify the scale ( LINEAR , LOG , SQRT , ORDINAL , COLOR or SYMBOL ), and optionally, the associated parameters scale_params to override the defaults. The following table lists the default scales for each aesthetics parameter: aes_key Default Scale Default Domain Default Range SIZE LINEAR [0, 1] [0, 1] SHAPE SYMBOL Dynamic Symbol types supported in D3 FILL COLOR Dynamic Palette \"category20c\" ALPHA LINEAR [0, 1] [0, 1] STROKE COLOR Dynamic Palette \"category20c\" STROKE-WIDTH LINEAR [0, 1] [0.5, 2.5] A \"dynamic\" domain means that the domain is updated on the fly and consists of all the unique value for the associated expression seen so far. The following lists the syntax of the parameters scale_params associated with the respective scale s: LINEAR ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. LINEAR ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. LOG ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range. ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. LOG ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. SQRT ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range. ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. SQRT ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. ORDINAL ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_value [ , ... ] ) ) Explicit specification of domain and range. dom_value and range_value are ordinal. ORDINAL ( range_value [ , ... ] ) Explicit specification of range. Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_color [ , ... ] ) ) Explicit specification of domain and range. dom_value is ordinal. range_color is a valid color name or specification . COLOR ( range_color_palette ) Explicit specification of range as a palette. range_color_palette is color palette name supported by D3. It can be one of \"category10\" , \"category20\" , \"category20b\" , or \"category20c\" Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( range_color , range_color [ , ... ] ) Explicit specification of range as a list of colors. At least two colors must be present. range_color is a valid color name or specification . Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( dom_value AS range_color [ , ... ] ) Explicit specification of domain and range as a mapping from values to colors. dom_value is ordinal. range_color is a valid color name or specification . SYMBOL ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_symbol [ , ... ] ) ) Explicit specification of domain and range. dom_value is ordinal. range_symbol is a valid symbol name in D3 . SYMBOL ( range_symbol , [ , ... ] ) Explicit specification of range as a list of symbols. range_symbol is a valid symbol name in D3 . Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. SYMBOL ( dom_value AS range_symbol [ , ... ] ) Explicit specification of domain and range as a mapping from values to symbols. dom_value is ordinal. range_symbol is a valid symbol name in D3 . In the above, ORDINAL scale is a generalization of COLOR and SYMBOL , where range_value is constrained to colors and symbols respectively. Handling NULLs \u25c4 The scale maps the values to aesthetic specifications. By default, the NULLs are mapped to a default aesthetic specification, and are ignored in legends. One way to map NULLs as aesthetic specifications is to replace them with an actial value using COALESCE(...) in the ScleraSQL query. This is not recommended as this changes the data for the sake of visualization. A better alternative is to use the ON NULL modifier to specify the aesthetic specification (constant) to be used for NULLs (recall the syntax above). For instance, FILL = foo ON NULL \"red\" will colour all datapoints where column foo is NULL as red. Legend \u25c4 The mapping from the aesthetics specification (e.g. colors \"red\", \"blue\", \"green\") to distinct values of the expression (e.g. day of the week) -- that is, the inverse of the scale -- is documented as a legend. To add a legend for an aes_key , add the keyword LEGEND after the specification (recall the syntax ). The LEGEND modifier takes an optional set of parameters, labeled legend_params in the syntax , and can include one or more of the following in a comma-separated list: ORIENTATION = orientation_param The orientation_param can be VERTICAL or HORIZONTAL and defined the manner in which the legend will be rendered; the default is VERTICAL . Since the allocated space for legends is at the right of the chart, VERTICAL orientation is a better fit, and is recommended. TITLE = title_string Specifies the title of the legend. If omitted, the title is not rendered. LABELS = ( label_string [ , ... ] ) Use the specified labels instead of the values provided by the associated scale mapping. LABELS = ( value AS label_string [ , ... ] ) Specifies an explicit mapping of the labels to use instead of the values provided by the associated scale mapping. Group \u25c4 The optional GROUP expression groups data points; each group is then rendered separately. The syntax is: GROUP = scalar_expr where scalar_expr is a ScleraSQL scalar expression on the input. Key \u25c4 The optional KEY expression identifies a rendered data point (a point in a scatter-plot, a bar in a bar-chart, etc.). All input rows with the same value of the KEY expression map to the same rendered data point; the first such row creates the data point, and the subsequent rows modify the same. The syntax is: KEY = scalar_expr where scalar_expr is a ScleraSQL scalar expression on the input. If KEY is not specified, a new datapoint is created for each input row. Position \u25c4 The optional POSITION directive specifies how to reposition \"colliding\" data points -- that is, data points that are rendered one over the other as they have the same value of the relevant geom_param parameters (e.g. x expression, y expression, or both). The syntax is: POSITION = { DODGE | STACK | JITTER } [ ( pos_params ) ] POSITION = DODGE repositions the colliding data points one after the other, around the common x value; applied to a bar chart, this is a \"grouped\" bar chart. POSITION = STACK repositions the colliding data points one over the other, at the common x value; applied to a bar chart, this is a \"stacked\" bar chart. POSITION = JITTER repositions the colliding data points randomly around the common x and y value, spreading a set of colliding scatterplot points, for instance, randomly into a circular region. The POSITION specification can also include pos_params that are passed as arguments to the renderer. The arguments are dependent on the associated geometry of the underlying data points, and have the form pos_key = pos_value as listed in the table below: POSITION GEOM Parameter Value DODGE BAR PADDING Padding between bars DODGE BAR OUTERPADDING Padding at the start and end STACK BAR PADDING Paddding between bars JITTER POINT x Maximum jitter along 'X' axis JITTER POINT y Maximum jitter along 'Y' axis Stat \u25c4 The STAT directive specifies a statistical operation over the input data points. The syntax is: STAT = { LOESS | BIN | BIN2D } [ ( stat_params [ , aes_params ] ) ] Applies on a scatter plot, i.e. GEOM = POINT(x = ..., y = ...) , STAT = LOESS computes local regression smoothing, STAT = BIN computes density histograms over the x expression values, and STAT = BIN2D computes two-dimensional histograms (heat-maps) over the x and y expression values. The result is a rendering in another layer. STAT = LOESS renders a line, STAT = BIN renders a histogram, and STAT = BIN2D renders a two-dimensional tiling representing the bins with an opacity proportional to the number of points covered under the bin. The STAT specification also includes stat_params that are passed as arguments to the operation, and aes_params that override the default aesthetics parameters for the layer containing the rendered result. The stat_params for each STAT type is listed in the table below: STAT Parameter Required / Optional Description LOESS BANDWIDTH Optional When computing the loess fit at a particular point, this fraction of source points closest to the current point is taken into account for computing a least-squares regression. A sensible value is usually 0.25 to 0.5, the default value is 0.3. See Apache Commons Math Documentation LOESS ITERS Optional This many robustness iterations are done. A sensible value is usually 0 (just the initial fit without any robustness iterations) to 4, the default value is 2. See Apache Commons Math Documentation LOESS ACCURACY Optional If the median residual at a certain robustness iteration is less than this amount, no more iterations are done. If the median residual at a certain robustness iteration is less than this amount, no more iterations are done. See Apache Commons Math Documentation LOESS WEIGHT Optional Expression giving the coefficients by which the robustness weight of a point is multiplied. Default: 1. See Apache Commons Math Documentation BIN BINWIDTH Required Width of a histogram bin. BIN MIN Optional Minimum X-value. If omitted, is determined by pre-scanning the data BIN2D XBINWIDTH Either this or BINWIDTH required Width of a histogram bin BIN2D YBINWIDTH Either this or BINWIDTH required Height of a histogram bin BIN2D BINWIDTH Required when either XBINWIDTH or YBINWIDTH omitted Width / height of a histogram bin, used when XBINWIDTH and YBINWIDTH are equal BIN2D XMIN Optional Minimum X-value. If omitted, is determined by pre-scanning the data BIN2D YMIN Optional Minimum Y-value. If omitted, is determined by pre-scanning the data Mark \u25c4 The MARK directive marks out regions where a specified predicate is true. The syntax is: MARK { XAXIS | YAXIS } ( mark_predicate [ , aes_params [ , ... ] ] ) XAXIS refers to the marking along the X axis, and leads to vertical marked regions. Similarly, YAXIS refers to marking along the Y axis, and leads to horizontal marked regions. The mark_predicate refers to the predicate (that is, boolean-values expression) on the input; the marked region includes exactly the points where this predicate evaluates to true. The MARK specification also aes_params that override the default aesthetics parameters for the layer containing the marked regions. Tooltip \u25c4 We can attach a tooltip to each data point rendered, so that when a viewer hovers a mouse cursor over the data point, this tooltip is displayed. The syntax is, simply: TOOLTIP = tooltip_expr Where tooltip is a string-valued expression. The tooltip message is generated by evaluating this expression over the row associated with the data point being hovered over. Hiding the Layer \u25c4 Stat and mark directives discussed earlier generate derivative layers by doing computation over the rows associated with data points in a given layer. After these derivative layers are generated, the default is to retain the original (base) layer. This is not always desirable, especially if the derivative layer and the derived layer have different ranges of values. An example is a scatter-plot and its density (computed using STAT=BIN(...) -- both share the 'X-axis', but the Y-axis for the scatter-plot could be very different from the Y-axis for the histogram which displays the relevant counts of the points. Displaying both these together in the same plot is not feasible. ScleraViz therefore enables hiding of the original (base) layer, by including the directive HIDDEN in the layer. In the scatter-plot example, specifying HIDDEN in the scatterplot layer alongside STAT=BIN(...) hides the scatter-plot, and displays only the histogram. Layer Display Order \u25c4 Layers of a plot are overlayed one over the other. This makes it important to be able to explicitly specify the order in which the layers are rendered; this is done using the syntax: DISPLAYORDER = display_order where display_order is an arbitrary numeric value. This can be specified for the base layers as well as for the derived layers (those added as results of STAT and MARK operations). At runtime, the renderer sorts the available layers on increasing display_order , and rennders the layers in the resulting order. Facet \u25c4 Faceting a plot on an expression involves displaying separate plots for datapoints partitioned by distinct values of the expression. ScleraViz allows faceting on a single expression, or a pair of expression; the resulting plots can be layed out in a row or columns or a row/column grid. The syntax has the following alternatives ( scalar_expr is a ScleraSQL scalar expression on the input): FACET ( ROWS = scalar_expr ) Generates a plot for each distinct value of the given expression. The resulting plots are layed out in a row, their widths adjusted to fit the available total width. FACET ( COLUMNS = scalar_expr ) Generates a plot for each distinct value of the given expression. The resulting plots are layed out in a column, their heights adjusted to fit the available total height. FACET ( ROWS = scalar_expr, COLUMNS = scalar_expr ) Generates a plot for each distinct pair of values of the respective expressions. The resulting plots are layed out in a grid, in the row and column order according as specified, their widths and heights adjusted to fit the available total width and total height. Axis \u25c4 SclerViz identifies axes with expressions. Axes associated with the same expression across plots are actually renderings of the same internal representation; this keeps the axes renderings in sync as new data is received and the plots are updated. The axis specification changes the default behaviour and appearance of an axis. The syntax is: AXIS axis_expr ( axis_param [, ...] ) where axis_expr is the expression mapped by the axis in the plots, and axis_param can be: LABEL ( label_string ) Assigns the label label_string to all renderings of the axis. If the axis with axis_expr appears in two different plots, the label appears in both the axis renderings. SCALE = { TIME | LINEAR | LOG | SQRT } [ ( min_value, max_value ) ] Assigns a continuous scale to the axis. The scale maps the values of the axis_expr to coordinates along the axis. The optional interval (min_value, max_value) explicitly specifies the min and max values for the axis. If not specified, the interval is computed dynamically, on the fly as data comes in. Either of min_value and max_value can be NULL , in which case it is considered unspecified, and is computed dynamically. SCALE = ORDINAL Assigns a scale to the axis. The scale maps the values of the axis_expr , assumed discrete, to evenly placed coordinates along the axis. INCREASING = { TRUE | FALSE } Declares whether the data is increasing along the axis. This extra knowledge of the input order helps speed up the computations while adjusting the scale when new data arrives. WINDOW = window_size The data points are displayed in a sliding window of size window_size . The points are accumulated till window_size points are displayed, and then the plot slides as additional data arrives to display only the latest window_size points. TICKFORMAT = format Formats the tick labels. For LINEAR , LOG and SQRT axis types, the format is specified in D3's format specification language . For TIME axis, the alternative time format specification language is used instead. TICKS = num_ticks Sets a limit on the number of ticks displayed on this axis. WEIGHT = axis_weight Sets the size of this axis relative to other axes. By default, all axes are assigned a weight of 1. Assigning a weight of, say 0.5, makes the size of an axis half of an axis with weight 1. Similarly, assigning a weight of 2.0 makes the size of an axis twice that of an axis with weight 1. The actual axis lengths are assigned respecting these relative size constrainst and the constraint that all the plots need to fit in the given width and height. Layout \u25c4 When multiple plots are specified, the default layout is to display them in a column, with the heights adjusted to fit the total height. The optional LAYOUT ALIGNED places the multiple plots so that the X-axes on the same expression and the Y-axes on the same expression are aligned in a column or row, respectively. Grid Specification \u25c4 Sets the aesthetics of the displayed grid and the axes. The syntax is: GRID ( grid_param [ , ... ] ) where grid_param can be: COLOR = color Sets the background color. color is a valid color name or specification . Default is \"#F5F5F5\" axis_id( axis_aes [ , ... ] ) axis_id is XAXIS or YAXIS XAXIS sets the aesthetics for all the X axes across all the plots. YAXIS sets the aesthetics for all the Y axes across all the plots. axis_aes can be: COLOR = color Sets the color of the axis. color is a valid color name or specification . Default is \"#000\" TICKS = color Sets the color of the ticks. Ticks are the vertical grid lines for XAXIS , and the horizontal grid lines for YAXIS . color is a valid color name or specification . Default is \"#FFF\". TICKS = none removes the ticks. Display Specification \u25c4 The display specification sets the dimensions of the plot and legend display areas. The syntax is: DISPLAY ( display_param [ , ... ] ) where display_param can be: WIDTH = width Sets the width of the plot diplay area, in pixels. Default is 800 pixels. HEIGHT = height Sets the height of the plot diplay area, in pixels. Default is 450 pixels. MARGIN ( margin_spec ) Sets the size of the four margins. margin_spec can be: TOP = top_margin Sets the top margin, in pixels. Default is 20 pixels. RIGHT = right_margin Sets the right margin, in pixels. Default is 20 pixels. BOTTOM = bottom_margin Sets the bottom margin, in pixels. Default is 30 pixels. LEFT = left_margin Sets the bottom margin, in pixels. Default is 40 pixels. LEGEND ( legend_spec ) Sets the dimensions of the legend display area. legend_spec can be: PADDING = legend_padding Sets the padding between the plot and the legend, in pixels. Default is 20 pixels. WIDTH = legend_width Sets the width of the legend display area, in pixels. Default is 100 pixels. Acknowledgements \u25c4 ScleraViz is inspired by Grammar of Graphics , specifically R's ggplot2 . The plots are rendered using D3 , and the legends are rendered using D3-Legend .","title":"Visualization"},{"location":"sclerasql/visualization/#command-line-setup","text":"In the following, <sclera-root> is the directory where Sclera is (or, is to be) installed. To run ScleraViz queries, you need to: Install Sclera Command Line using scleradmin ( detailed instructions here ) $ scleradmin --install --root <sclera-root> Install the Sclera Web Display plugin $ scleradmin --add sclera-plugin-webdisplay --root <sclera-root> If you need to read CSV data, install the CSV plugin as well $ scleradmin --add sclera-plugin-csv --root <sclera-root> Start the Sclera Command Line Shell $ <sclera-root>/bin/sclera Welcome to Sclera 4.0 > _ In the Sclera Command Line Shell, start the display web server: Welcome to Sclera 4.0 > display start; HTTP server started at port 7070 Please point your browser to http://localhost:7070 To start the server at a port other than 7070, you need to specify the port number explicitly at the end. For example, display start 8080; starts the server at localhost:8080 . Start a web browser and open the URL http://localhost:7070 Running a ScleraViz query (described in the rest of this document) on the Sclera Command Line will display the visualization in the browser. In addition, to display the contents of a query as a table in the browser, you can say: > display <query>; For example, the following displays the result as a two column, single row table in the browser: > display select 'Hello' as greeting1, 'World' as greeting2; Leaving the keyword display out displays the result in the command line shell, as usual. You can also display arbitrary text (say, comments) as well: > display \"The plot above is amazing!\"; When done, you can close the display web server by saying: > display stop;","title":"Command Line Setup"},{"location":"sclerasql/visualization/#scleraviz-by-example","text":"This section very briefly introduces the rudiments of the plot specification. This is to quickly get you started on plotting your own graphs. A complete and formal description of the syntax appear in the next section . Example Dataset In the following, we will be using the \"tipping\" dataset as our running example. The data set contains information about tips received by a waiter in a restaurant over a period of time. The information includes the tip in dollars, total bill in dollars, gender of the bill payer, whether there were smokers in the party, day of the week, time of day, and size of the party. Further details on the data can be found here . To use the dataset, set up the tips table as follows: CREATE TEMP TABLE tips as EXTERNAL CSV(\"http://scleraviz.herokuapp.com/assets/data/tips.csv\") TYPEINFER(LIMIT 1); Notation: In the above and the following examples, the ScleraSQL keywords will be capitalized. This is for illustrative reasons only; ScleraSQL is case-insensitive.","title":"ScleraViz by Example"},{"location":"sclerasql/visualization/#a-simple-plot","text":"Putting a PLOT afer the query plots the column y in the query result against the column x in the query result as a scatter plot. So, to plot the tip against the total_bill in the tipping dataset, we say: SELECT total_bill as x, tip AS y FROM tips PLOT; The above statement works on a bunch of defaults -- it creates a scatter-plot, which takes column x in the input as the default for the X coordinate, and column y in the input as the default for the y coordinate, and also takes default values for aesthetics such as point fill and stroke color, size, opacity (alpha), etc. In the discussion that follows, we show how to explicitly override these defaults, and even vary them as a function of the data.","title":"A Simple Plot"},{"location":"sclerasql/visualization/#specifying-the-geometry","text":"A \"geometry\" refers to the manner in which a data point maps to the chart. In the scatterplot example above, the geometry is a POINT . We can explicitly specify the geometry as follows: SELECT total_bill as x, tip AS y FROM tips PLOT(GEOM=POINT) The location of a geometry is determined by a set of parameters. By default, these parameters are mapped to appropriately named columns in the input. For instance, each point above is placed according to the values in the x and y columns. We can override this default by explicitly specifing custom columns for these parameters. So, another way to write the example above is: tips PLOT(GEOM=POINT(x=total_bill, y=tip)) Here, the parameters x and y are mapped to input columns total_bill and tip respectively; in general, they can be mapped to arbitrary expressions. Sclera supports a number of alternative geometries. For instance, to plot the average bill for a given party size against the party size as a histogram, we can say: SELECT `size`, AVG(total_bill) as avg_bill FROM tips GROUP BY `size` PLOT(GEOM=BAR(x=`size`, y=avg_bill)) (Note: size needs to be in backquotes as it is a keyword. This is a parser requirement; we understand that it is cumbersome and are working on workarounds. Meanwhile, it is always a good idea to put the column names in backquotes.) Similarly, to plot the same data as a line, we use GEOM=LINE : SELECT `size`, AVG(total_bill) as avg_bill FROM tips GROUP BY `size` PLOT(GEOM=LINE(x=`size`, y=avg_bill)) A full list of the supported geometries appears as a part of the detailed documentation .","title":"Specifying the Geometry"},{"location":"sclerasql/visualization/#aesthetics-and-legends","text":"The aesthetics parameters specify the \"look and feel\" of a chart. For a scatterplot, for instance, the parameters specify the shape of the point (circle, square, triangle, and so on), the color to be used to fill the point, the color used for the boundary, and so on. The set of aesthetics parameters varies with the geometry. For example, a SHAPE makes sense for a point, but does not make sense for a line. The list of aesthetics parameters associated with respective geometries appears as a part of the detailed documentation . An aesthetics parameter can either be a constant, or an expression. If the parameter is a constant, e.g. FILL=\"black\" , then the value of the aesthetics is literally taken as the constant. If the parameter is an expression, then distinct values of the columns are mapped to distinct values of the aesthetic. For instance, if we specify FILL=foo where foo is an input column, then by default, each distinct value of foo is mapped to a distinct value of the color that is actually used for the fill. The mapping from each distict value of the column foo to the corresponding color is automatically maintained, and can be output as a legend, by saying FILL=foo LEGEND . The following example shows a scatterplot with expressions for FILL and SHAPE aesthetics. tips PLOT( GEOM=POINT(x=total_bill, y=tip), FILL=day LEGEND(TITLE=\"Day\"), SHAPE=`time` LEGEND(TITLE=\"Time\") ) A full description of the aesthetics parameters appears in the detailed documentation .","title":"Aesthetics and Legends"},{"location":"sclerasql/visualization/#group","text":"The optional GROUP directive specifies an expression on the input data. The input data points are first partitioned based on the values of this expression (similar to the SQL GROUP BY ), and each partition is plotted separately in the same layer. For instance, adding a GROUP directive to a LINE will plot a line for each distinct value of the GROUP expression. SELECT `size`, gender, AVG(total_bill) as avg_bill FROM tips GROUP BY `size`, gender ORDER BY `size` PLOT(GEOM=LINE(x=`size`, y=avg_bill), GROUP=gender, STROKE=gender LEGEND)","title":"Group"},{"location":"sclerasql/visualization/#key","text":"By default, each row in the input corresponds to a unique data point. The optional KEY directive enables associating multiple rows with the same data point -- successive rows with the same key value render as the same data point, overriding the previous rendering. This is helpful to display, say, a dynamic bar chart showing a running average for tips collected on each day of week. We specify KEY=day and, for every input row, generate a row containing the updated running average of tip for the associated value of day . With KEY=day , the first rows for a value create a bar, and the subsequent roes for the same value update the existing bar. SELECT day, T.avg(tip) as running_avg FROM tips T PARTITION BY day PLOT(GEOM=BAR(x=day, y=running_avg), KEY=day) Without KEY=day , a new bar will be generated for each update. SELECT day, T.avg(tip) as running_avg FROM tips T PARTITION BY day PLOT(GEOM=BAR(x=day, y=running_avg), ALPHA=0.1)","title":"Key"},{"location":"sclerasql/visualization/#position-adjustments","text":"Following the grammar of graphics philosophy, the input data is assumed to have one row per data point. When two data points are rendered one over the other, they are said to \"collide\". Colliding data points can be repositioned by specifying a POSITION directive, which can take values dodge , stack , or jitter . Specifying POSITION=DODGE positions the data points in a row, one after other, around the common x position on the X axis. When the geometry is BAR , this gives the familiar \"grouped\" bar chart: SELECT day, gender, count(*) AS count FROM tips GROUP BY day, gender ORDER BY day, gender PLOT(GEOM=BAR(x=day, y=count), POSITION=DODGE, FILL=gender LEGEND) Specifying POSITION=STACK positions the data points in a column, one over other, at the common x position on the X axis. When the geometry is BAR , this gives the familiar \"stacked\" bar chart: SELECT day, gender, count(*) AS count FROM tips GROUP BY day, gender ORDER BY day, gender PLOT(GEOM=BAR(x=day, y=count), POSITION=STACK, FILL=gender LEGEND) Specifying POSITION=jitter adds a random offset to both the x and y positions of the data point. This is useful when we have overlapping points, so that the top points obscure the ones below -- the jitter randomly scatters the obscured points within a small ball around their position, making them visible.","title":"Position Adjustments"},{"location":"sclerasql/visualization/#stat","text":"The STAT directive specifies a statistical operation over the input data points. Supported operations include computing local regression smoothing, computing density histograms, and computing heat-maps. By default, the results are computation are overlayed over the base plot. For instance, consider the scatter-plot of average tips against the total bill. Including STAT=loess plots a line representing local regression smoothing over these data points. This curve brings out the trend in the data -- how the tips vary with increasing total_bill -- that may not be apparent from just the scatter plot. SELECT total_bill, avg(tip) as avg_tip FROM tips GROUP BY total_bill PLOT( GEOM=POINT(x=total_bill, y=avg_tip), STAT=LOESS(STROKE=\"orange\", STROKE_WIDTH=\"5px\") ) The computed result is plotted in its own layer (see the next section), using a geometry that is determined by the operation, and using aesthetic parameters that can be provided along with the operation, as in the example above.","title":"Stat"},{"location":"sclerasql/visualization/#mark","text":"The MARK directive marks out regions where a specified predicate is true. This is done in real time, as the data arrives, and so can be very useful in monitoring for events. For example, the following query marks regions in a stock ticker where the current high is greater than the running average high with green, and regions where the current low is less than the running average low with red. EXTERNAL CSV(\"http://scleraviz.herokuapp.com/assets/data/infy.csv\") TYPEINFER(LIMIT 1) AS infy PLOT( GEOM=LINE(x=ts, y=close), MARK XAXIS( high > infy.AVG(high), FILL=\"green\" ), MARK XAXIS( low < infy.AVG(low), FILL=\"red\" ) )","title":"Mark"},{"location":"sclerasql/visualization/#multi-layered-plots","text":"A layer represents a unit of rendering in the plot. The examples so far, except the stats, included a single layer -- which is the default. A layer can only support one kind of geometry, and the associated set of aesthetic parameters and position specifications. If you want to include, say, a scatter plot and a line in the same plot, you need to have separate layers for the points and the line. You can explicitly specify layers using the LAYER keyword. The initial scatterplot can be equivalently be specified as: tips PLOT(LAYER(GEOM=point(x=total_bill, y=tip))) We can specify multiple layers in the same plot. The following plots the average bill for a party size, along with a one-standard deviation interval in a separate layer: SELECT `size`, avg(total_bill) as avg_bill, stddev(total_bill) as sdev FROM tips GROUP BY `size` ORDER BY `size` PLOT( LAYER(GEOM=LINE(x=`size`, y=avg_bill)), LAYER(GEOM=RIBBON(x=`size`, ymin=avg_bill - sdev, ymax=avg_bill + sdev)) ) Another example on the same query, this time with bars and error lines: SELECT `size`, avg(total_bill) as avg_bill, stddev(total_bill) as sdev FROM tips GROUP BY `size` ORDER BY `size` PLOT( LAYER(GEOM=BAR(x=`size`, y=avg_bill)), LAYER( GEOM=POINTRANGEY( x=`size`, y=avg_bill, ymin=avg_bill - sdev, ymax=avg_bill + sdev ) ) ) The STAT directive mentioned in the previous section implicitly adds a new layer to the plot, with the result of the associated operation.","title":"Multi-Layered Plots"},{"location":"sclerasql/visualization/#multi-plots","text":"We can have multiple plots on the same input data; these plots are plotted simultaneously in a single scan of the input. For instance, the following specification renders two separate plots on the same input: tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) The plots are plotted one after the other, in separate rows. As the plots are rendered, the Y-axes of the first two plots, which plot the same column tip , and the X-axes of the last two plots, which plot the same column size are kept synchronized.","title":"Multi-Plots"},{"location":"sclerasql/visualization/#automatic-layouts","text":"ScalerViz provides an aligned layout mode that places the plots so that plots sharing the same X-axis columns are aligned vertically, and the plots sharing the same Y-axis columns are aligned horizontally. We try this mode, activated using the LAYOUT ALIGNED directive on the example above, to get: tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) LAYOUT ALIGNED","title":"Automatic Layouts"},{"location":"sclerasql/visualization/#weighted-axes","text":"In the multi-plot examples above, all axes have the same size. However, we may sometimes want to emphasize some columns over the others. In ScleraViz, we can assign weights to axes. By default, all axes are assigned a weight of 1. Assigning a weight of, say 0.5, makes the size of an axis half of an axis with weight 1. Similarly, assigning a weight of 2.0 makes the size of an axis twice that of an axis with weight 1. The actual axis lengths are assigned respecting these relative size constrainst and the constraint that all the plots need to fit in the given width and height. tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=tip), ALPHA=0.3) PLOT(GEOM=POINT(x=`size`, y=total_bill), ALPHA=0.3) PLOT(GEOM=POINT(x=total_bill, y=total_bill), ALPHA=0.3) AXIS `size`(WEIGHT=0.25, TICKS=3) AXIS tip(WEIGHT=2) LAYOUT ALIGNED","title":"Weighted Axes"},{"location":"sclerasql/visualization/#cross-plots","text":"It is common to plot each of a set of columns against each of another set of columns. In ScleraViz, we can plot such \"cross-plots\" by using the single-layer PLOT syntax, and specifying a list of columns for the parameters of the geometry. tips PLOT( GEOM=POINT(x=(smoker, gender, day, `time`), y=(total_bill, tip)), ALPHA=0.3 ) LAYOUT ALIGNED","title":"Cross Plots"},{"location":"sclerasql/visualization/#faceted-plots","text":"Faceting a plot on an expression involves displaying separate plots for datapoints partitioned by distinct values of the expression. ScleraViz can facet along columns (meaning that the plots corresponding to distinct values are layed out horizontally, in separate columns in a grid), or have faceting along rows (meaning that the plots corresponding to distinct values are layed out vertically, in separate rows in a grid), or both. For instance, the following plot separates out the tip and total bill data based on whether smokers were present (yes/no, along rows), and the gender of the bill-payer (male/female, along columns): tips PLOT(GEOM=POINT(x=total_bill, y=tip), ALPHA=0.3) FACET(ROWS=smoker, COLUMNS=gender)","title":"Faceted Plots"},{"location":"sclerasql/visualization/#detailed-syntax","text":"This section gives the detailed formal syntax of the ScleraViz specification language. Formally, the ScleraViz specification is as follows (recall that | means alternatives, [] means optional, and ... means repetitions): query plot_spec [ , ... ] [ facet_spec ] [ axis_spec ] [ ... ] [ layout_spec ] [ grid_spec ] [ display_spec ] where the query is a ScleraSQL query . The plot specification plot_spec can appear one or more times, each plot_spec defining a new plot, followed by an optional facet specification facet_spec , zero or more axis_spec if we need to change the axis defaults. A layout_spec then gives the layout preferences. This is followed by a grid_spec if we need to change the grid defaults, and a display_spec to change the display area defaults.","title":"Detailed Syntax"},{"location":"sclerasql/visualization/#plot-layer-specification","text":"The plot_spec specifies a plot, and has two variants: PLOT ( layer_params ) | PLOT ( LAYER ( layer_params ) [, ...] ) The first variant is an shortened form of the second, and can be used when the plot has only one layer. The second variant is needed when the plot has multiple layers. Each layer_param specifies a layer in the plot. It contains several parts, each of them except geometry being optional, which can be specified in any order. We describe the sybtax of each part in the next few sections.","title":"Plot / Layer Specification"},{"location":"sclerasql/visualization/#geometry","text":"The geometry specifies the manner in which the datapoint will be represented in the chart. The syntax is as follows: GEOM = geom_type [ ( geom_params ) ] The geom_type is the type of the geometry, and geom_params is a comma-separated list of key = value entries. The keys for each geometry are given the following table; the values for the keys can be a numeric scalar expression on the underlying table (or a constant). geom_type geom_params keys POINT or SCATTER X, Y LINE X, Y POINTRANGEY X, Y, YMIN, YMAX ERRORBAR or RANGEY X, YMIN, YMAX, [WIDTH] ERRORBARH or RANGEX XMIN, XMAX, Y, [HEIGHT] RIBBON X, YMIN, YMAX ABLINE YINTERCEPT, SLOPE HLINE Y VLINE X SEGMENT X, XEND, Y, YEND ARROW X, XEND, Y, YEND AREA X, Y RECT XMIN, XMAX, YMIN, YMAX HISTOGRAM or HIST XMIN, XMAX, FREQ BAR X, Y REGIONX XMIN, XMAX, YMIN, YMAX REGIONY XMIN, XMAX, YMIN, YMAX OHLC TS, OPEN, HIGH, LOW, CLOSE, [WIDTH] CANDLESTICK TS, OPEN, HIGH, LOW, CLOSE, [WIDTH] In the above table, keys enclosed in [] are optional, others are required. The geom_params are optional, and may only specify the values for a subset of the required keys for the geometry. When a required key needed by a geometry does not appear in the geom_params , it is mapped to a column of the same name in the input. The GEOM specification itself is optional -- if omitted, it defauts to POINT , with the keys X and Y mapped to input columns of the same name.","title":"Geometry"},{"location":"sclerasql/visualization/#cross-plots_1","text":"In the single-layer variant, the geom_param keys can have a list of expressions instead of the single expression allowed above. This is considered a shortcut, and expands to a separate plot for each value in the list. If multiple geom_param keys have a list of expressions, then the specification expands to a separate plot for each combination of the geom_param values across these lists. Apart from the geom_param values, each of the plots so generated have the directives and properies as specified in the rest of the PLOT specification.","title":"Cross-Plots"},{"location":"sclerasql/visualization/#aesthetics-scales-and-legends","text":"The aesthetics directives change the default look (colors, shape, size, etc.) of the data points, and also assign legends. The syntax is as follows: aes_key = aes_value [ SCALE = scale [ ( scale_params ) ] ] [ ON NULL null_aes_const ] [ LEGEND [ ( legend_params ) ] ] The aes_key is an attribute such as FILL , STROKE , etc. (see the table below), and aes_value has the syntax: aes_const | aes_expr | IDENTITY ( aes_identity_expr ) aes_const is a constant of the appropriate type as specified in the table below. For example, FILL = \"red\" . aes_expr is a scalar expression of the appropriate type as specified in the table below. A scale maps distinct values of this expression to the appropriate value for the aes_key . For instance, specifying FILL = foo maps distinct values in the column foo to distinct colors from a default palette. The palette can be explicitly specied using a scale ( see below ). aes_identity_expr is an expression that evaluates to the literal value of an ordinal type. For instance, if a columns foo contains valid color names such as \"red\", \"blue\", \"black\", etc., then we can specify IDENTITY(foo) as the aes_value . Each geometry has an associated list of aes_key alternatives that it can interpret; aes_key specifications that are not associated with the specified geometry are ignored. The keys aes_key , types of aes_value and associated geometries are listed in the table below. aes_key Type Geometry Description SIZE Numeric POINT, POINTRANGEY Specifies the size of the point SHAPE Ordinal ( valid shape values ) POINT, POINTRANGEY Specifies the shape of the point FILL Ordinal ( valid color values ) POINT, POINTRANGEY, AREA, BAR, RIBBON, OHLC Color used to fill the shape ALPHA Numeric (between 0 and 1) all geometries Opacity of the rendering STROKE Ordinal ( valid color values ) all geometries Color used for the line STROKE-WIDTH Numeric all geometries Specifies the width of the line INTERPOLATE Ordinal ( valid interpolate values ) LINE, AREA, RIBBON Sets the path interpolation mode TENSION Numeric (between 0 and 1) LINE, AREA, RIBBON Sets the path tension","title":"Aesthetics, Scales, and Legends"},{"location":"sclerasql/visualization/#scale","text":"Using a non-constant expression aes_expr for the aes_value makes the aesthetics a function of the data. The mapping from the distinct values of the expression (e.g. day of the week) to the aesthetics specification (e.g. colors \"red\", \"blue\", \"green\") is called a \"scale\". The scale is numeric if it outputs numeric values; examples of numeric scales are: LINEAR , LOG , and SQRT . Further information on numeric scales, their defaults and capabilities appears in the D3 documentation . The scale is ordinal if it outputs ordinal values; examples of ordinal scales are ORDINAL , COLOR , and SYMBOL .Further information on the ordinal scales, their defaults and capabilities appears in the D3 documentation . To explicitly specify the scale, we specify the scale ( LINEAR , LOG , SQRT , ORDINAL , COLOR or SYMBOL ), and optionally, the associated parameters scale_params to override the defaults. The following table lists the default scales for each aesthetics parameter: aes_key Default Scale Default Domain Default Range SIZE LINEAR [0, 1] [0, 1] SHAPE SYMBOL Dynamic Symbol types supported in D3 FILL COLOR Dynamic Palette \"category20c\" ALPHA LINEAR [0, 1] [0, 1] STROKE COLOR Dynamic Palette \"category20c\" STROKE-WIDTH LINEAR [0, 1] [0.5, 2.5] A \"dynamic\" domain means that the domain is updated on the fly and consists of all the unique value for the associated expression seen so far. The following lists the syntax of the parameters scale_params associated with the respective scale s: LINEAR ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. LINEAR ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. LOG ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range. ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. LOG ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. SQRT ( DOMAIN = ( dom_min , dom_max ) , RANGE = ( range_min, range_max ) ) Explicit specification of domain and range. ( dom_min, dom_max ) and ( range_min, range_max ) are numeric intervals. SQRT ( dom_min, dom_max ) Explicit specification of only the domain, range at default [0, 1]. ORDINAL ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_value [ , ... ] ) ) Explicit specification of domain and range. dom_value and range_value are ordinal. ORDINAL ( range_value [ , ... ] ) Explicit specification of range. Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_color [ , ... ] ) ) Explicit specification of domain and range. dom_value is ordinal. range_color is a valid color name or specification . COLOR ( range_color_palette ) Explicit specification of range as a palette. range_color_palette is color palette name supported by D3. It can be one of \"category10\" , \"category20\" , \"category20b\" , or \"category20c\" Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( range_color , range_color [ , ... ] ) Explicit specification of range as a list of colors. At least two colors must be present. range_color is a valid color name or specification . Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. COLOR ( dom_value AS range_color [ , ... ] ) Explicit specification of domain and range as a mapping from values to colors. dom_value is ordinal. range_color is a valid color name or specification . SYMBOL ( DOMAIN = ( dom_value [ , ... ] ) , RANGE = ( range_symbol [ , ... ] ) ) Explicit specification of domain and range. dom_value is ordinal. range_symbol is a valid symbol name in D3 . SYMBOL ( range_symbol , [ , ... ] ) Explicit specification of range as a list of symbols. range_symbol is a valid symbol name in D3 . Domain is dynamic -- that is, updated on the fly and consists of all the unique value for the associated expression seen so far. SYMBOL ( dom_value AS range_symbol [ , ... ] ) Explicit specification of domain and range as a mapping from values to symbols. dom_value is ordinal. range_symbol is a valid symbol name in D3 . In the above, ORDINAL scale is a generalization of COLOR and SYMBOL , where range_value is constrained to colors and symbols respectively.","title":"Scale"},{"location":"sclerasql/visualization/#handling-nulls","text":"The scale maps the values to aesthetic specifications. By default, the NULLs are mapped to a default aesthetic specification, and are ignored in legends. One way to map NULLs as aesthetic specifications is to replace them with an actial value using COALESCE(...) in the ScleraSQL query. This is not recommended as this changes the data for the sake of visualization. A better alternative is to use the ON NULL modifier to specify the aesthetic specification (constant) to be used for NULLs (recall the syntax above). For instance, FILL = foo ON NULL \"red\" will colour all datapoints where column foo is NULL as red.","title":"Handling NULLs"},{"location":"sclerasql/visualization/#legend","text":"The mapping from the aesthetics specification (e.g. colors \"red\", \"blue\", \"green\") to distinct values of the expression (e.g. day of the week) -- that is, the inverse of the scale -- is documented as a legend. To add a legend for an aes_key , add the keyword LEGEND after the specification (recall the syntax ). The LEGEND modifier takes an optional set of parameters, labeled legend_params in the syntax , and can include one or more of the following in a comma-separated list: ORIENTATION = orientation_param The orientation_param can be VERTICAL or HORIZONTAL and defined the manner in which the legend will be rendered; the default is VERTICAL . Since the allocated space for legends is at the right of the chart, VERTICAL orientation is a better fit, and is recommended. TITLE = title_string Specifies the title of the legend. If omitted, the title is not rendered. LABELS = ( label_string [ , ... ] ) Use the specified labels instead of the values provided by the associated scale mapping. LABELS = ( value AS label_string [ , ... ] ) Specifies an explicit mapping of the labels to use instead of the values provided by the associated scale mapping.","title":"Legend"},{"location":"sclerasql/visualization/#group_1","text":"The optional GROUP expression groups data points; each group is then rendered separately. The syntax is: GROUP = scalar_expr where scalar_expr is a ScleraSQL scalar expression on the input.","title":"Group"},{"location":"sclerasql/visualization/#key_1","text":"The optional KEY expression identifies a rendered data point (a point in a scatter-plot, a bar in a bar-chart, etc.). All input rows with the same value of the KEY expression map to the same rendered data point; the first such row creates the data point, and the subsequent rows modify the same. The syntax is: KEY = scalar_expr where scalar_expr is a ScleraSQL scalar expression on the input. If KEY is not specified, a new datapoint is created for each input row.","title":"Key"},{"location":"sclerasql/visualization/#position","text":"The optional POSITION directive specifies how to reposition \"colliding\" data points -- that is, data points that are rendered one over the other as they have the same value of the relevant geom_param parameters (e.g. x expression, y expression, or both). The syntax is: POSITION = { DODGE | STACK | JITTER } [ ( pos_params ) ] POSITION = DODGE repositions the colliding data points one after the other, around the common x value; applied to a bar chart, this is a \"grouped\" bar chart. POSITION = STACK repositions the colliding data points one over the other, at the common x value; applied to a bar chart, this is a \"stacked\" bar chart. POSITION = JITTER repositions the colliding data points randomly around the common x and y value, spreading a set of colliding scatterplot points, for instance, randomly into a circular region. The POSITION specification can also include pos_params that are passed as arguments to the renderer. The arguments are dependent on the associated geometry of the underlying data points, and have the form pos_key = pos_value as listed in the table below: POSITION GEOM Parameter Value DODGE BAR PADDING Padding between bars DODGE BAR OUTERPADDING Padding at the start and end STACK BAR PADDING Paddding between bars JITTER POINT x Maximum jitter along 'X' axis JITTER POINT y Maximum jitter along 'Y' axis","title":"Position"},{"location":"sclerasql/visualization/#stat_1","text":"The STAT directive specifies a statistical operation over the input data points. The syntax is: STAT = { LOESS | BIN | BIN2D } [ ( stat_params [ , aes_params ] ) ] Applies on a scatter plot, i.e. GEOM = POINT(x = ..., y = ...) , STAT = LOESS computes local regression smoothing, STAT = BIN computes density histograms over the x expression values, and STAT = BIN2D computes two-dimensional histograms (heat-maps) over the x and y expression values. The result is a rendering in another layer. STAT = LOESS renders a line, STAT = BIN renders a histogram, and STAT = BIN2D renders a two-dimensional tiling representing the bins with an opacity proportional to the number of points covered under the bin. The STAT specification also includes stat_params that are passed as arguments to the operation, and aes_params that override the default aesthetics parameters for the layer containing the rendered result. The stat_params for each STAT type is listed in the table below: STAT Parameter Required / Optional Description LOESS BANDWIDTH Optional When computing the loess fit at a particular point, this fraction of source points closest to the current point is taken into account for computing a least-squares regression. A sensible value is usually 0.25 to 0.5, the default value is 0.3. See Apache Commons Math Documentation LOESS ITERS Optional This many robustness iterations are done. A sensible value is usually 0 (just the initial fit without any robustness iterations) to 4, the default value is 2. See Apache Commons Math Documentation LOESS ACCURACY Optional If the median residual at a certain robustness iteration is less than this amount, no more iterations are done. If the median residual at a certain robustness iteration is less than this amount, no more iterations are done. See Apache Commons Math Documentation LOESS WEIGHT Optional Expression giving the coefficients by which the robustness weight of a point is multiplied. Default: 1. See Apache Commons Math Documentation BIN BINWIDTH Required Width of a histogram bin. BIN MIN Optional Minimum X-value. If omitted, is determined by pre-scanning the data BIN2D XBINWIDTH Either this or BINWIDTH required Width of a histogram bin BIN2D YBINWIDTH Either this or BINWIDTH required Height of a histogram bin BIN2D BINWIDTH Required when either XBINWIDTH or YBINWIDTH omitted Width / height of a histogram bin, used when XBINWIDTH and YBINWIDTH are equal BIN2D XMIN Optional Minimum X-value. If omitted, is determined by pre-scanning the data BIN2D YMIN Optional Minimum Y-value. If omitted, is determined by pre-scanning the data","title":"Stat"},{"location":"sclerasql/visualization/#mark_1","text":"The MARK directive marks out regions where a specified predicate is true. The syntax is: MARK { XAXIS | YAXIS } ( mark_predicate [ , aes_params [ , ... ] ] ) XAXIS refers to the marking along the X axis, and leads to vertical marked regions. Similarly, YAXIS refers to marking along the Y axis, and leads to horizontal marked regions. The mark_predicate refers to the predicate (that is, boolean-values expression) on the input; the marked region includes exactly the points where this predicate evaluates to true. The MARK specification also aes_params that override the default aesthetics parameters for the layer containing the marked regions.","title":"Mark"},{"location":"sclerasql/visualization/#tooltip","text":"We can attach a tooltip to each data point rendered, so that when a viewer hovers a mouse cursor over the data point, this tooltip is displayed. The syntax is, simply: TOOLTIP = tooltip_expr Where tooltip is a string-valued expression. The tooltip message is generated by evaluating this expression over the row associated with the data point being hovered over.","title":"Tooltip"},{"location":"sclerasql/visualization/#hiding-the-layer","text":"Stat and mark directives discussed earlier generate derivative layers by doing computation over the rows associated with data points in a given layer. After these derivative layers are generated, the default is to retain the original (base) layer. This is not always desirable, especially if the derivative layer and the derived layer have different ranges of values. An example is a scatter-plot and its density (computed using STAT=BIN(...) -- both share the 'X-axis', but the Y-axis for the scatter-plot could be very different from the Y-axis for the histogram which displays the relevant counts of the points. Displaying both these together in the same plot is not feasible. ScleraViz therefore enables hiding of the original (base) layer, by including the directive HIDDEN in the layer. In the scatter-plot example, specifying HIDDEN in the scatterplot layer alongside STAT=BIN(...) hides the scatter-plot, and displays only the histogram.","title":"Hiding the Layer"},{"location":"sclerasql/visualization/#layer-display-order","text":"Layers of a plot are overlayed one over the other. This makes it important to be able to explicitly specify the order in which the layers are rendered; this is done using the syntax: DISPLAYORDER = display_order where display_order is an arbitrary numeric value. This can be specified for the base layers as well as for the derived layers (those added as results of STAT and MARK operations). At runtime, the renderer sorts the available layers on increasing display_order , and rennders the layers in the resulting order.","title":"Layer Display Order"},{"location":"sclerasql/visualization/#facet","text":"Faceting a plot on an expression involves displaying separate plots for datapoints partitioned by distinct values of the expression. ScleraViz allows faceting on a single expression, or a pair of expression; the resulting plots can be layed out in a row or columns or a row/column grid. The syntax has the following alternatives ( scalar_expr is a ScleraSQL scalar expression on the input): FACET ( ROWS = scalar_expr ) Generates a plot for each distinct value of the given expression. The resulting plots are layed out in a row, their widths adjusted to fit the available total width. FACET ( COLUMNS = scalar_expr ) Generates a plot for each distinct value of the given expression. The resulting plots are layed out in a column, their heights adjusted to fit the available total height. FACET ( ROWS = scalar_expr, COLUMNS = scalar_expr ) Generates a plot for each distinct pair of values of the respective expressions. The resulting plots are layed out in a grid, in the row and column order according as specified, their widths and heights adjusted to fit the available total width and total height.","title":"Facet"},{"location":"sclerasql/visualization/#axis","text":"SclerViz identifies axes with expressions. Axes associated with the same expression across plots are actually renderings of the same internal representation; this keeps the axes renderings in sync as new data is received and the plots are updated. The axis specification changes the default behaviour and appearance of an axis. The syntax is: AXIS axis_expr ( axis_param [, ...] ) where axis_expr is the expression mapped by the axis in the plots, and axis_param can be: LABEL ( label_string ) Assigns the label label_string to all renderings of the axis. If the axis with axis_expr appears in two different plots, the label appears in both the axis renderings. SCALE = { TIME | LINEAR | LOG | SQRT } [ ( min_value, max_value ) ] Assigns a continuous scale to the axis. The scale maps the values of the axis_expr to coordinates along the axis. The optional interval (min_value, max_value) explicitly specifies the min and max values for the axis. If not specified, the interval is computed dynamically, on the fly as data comes in. Either of min_value and max_value can be NULL , in which case it is considered unspecified, and is computed dynamically. SCALE = ORDINAL Assigns a scale to the axis. The scale maps the values of the axis_expr , assumed discrete, to evenly placed coordinates along the axis. INCREASING = { TRUE | FALSE } Declares whether the data is increasing along the axis. This extra knowledge of the input order helps speed up the computations while adjusting the scale when new data arrives. WINDOW = window_size The data points are displayed in a sliding window of size window_size . The points are accumulated till window_size points are displayed, and then the plot slides as additional data arrives to display only the latest window_size points. TICKFORMAT = format Formats the tick labels. For LINEAR , LOG and SQRT axis types, the format is specified in D3's format specification language . For TIME axis, the alternative time format specification language is used instead. TICKS = num_ticks Sets a limit on the number of ticks displayed on this axis. WEIGHT = axis_weight Sets the size of this axis relative to other axes. By default, all axes are assigned a weight of 1. Assigning a weight of, say 0.5, makes the size of an axis half of an axis with weight 1. Similarly, assigning a weight of 2.0 makes the size of an axis twice that of an axis with weight 1. The actual axis lengths are assigned respecting these relative size constrainst and the constraint that all the plots need to fit in the given width and height.","title":"Axis"},{"location":"sclerasql/visualization/#layout","text":"When multiple plots are specified, the default layout is to display them in a column, with the heights adjusted to fit the total height. The optional LAYOUT ALIGNED places the multiple plots so that the X-axes on the same expression and the Y-axes on the same expression are aligned in a column or row, respectively.","title":"Layout"},{"location":"sclerasql/visualization/#grid-specification","text":"Sets the aesthetics of the displayed grid and the axes. The syntax is: GRID ( grid_param [ , ... ] ) where grid_param can be: COLOR = color Sets the background color. color is a valid color name or specification . Default is \"#F5F5F5\" axis_id( axis_aes [ , ... ] ) axis_id is XAXIS or YAXIS XAXIS sets the aesthetics for all the X axes across all the plots. YAXIS sets the aesthetics for all the Y axes across all the plots. axis_aes can be: COLOR = color Sets the color of the axis. color is a valid color name or specification . Default is \"#000\" TICKS = color Sets the color of the ticks. Ticks are the vertical grid lines for XAXIS , and the horizontal grid lines for YAXIS . color is a valid color name or specification . Default is \"#FFF\". TICKS = none removes the ticks.","title":"Grid Specification"},{"location":"sclerasql/visualization/#display-specification","text":"The display specification sets the dimensions of the plot and legend display areas. The syntax is: DISPLAY ( display_param [ , ... ] ) where display_param can be: WIDTH = width Sets the width of the plot diplay area, in pixels. Default is 800 pixels. HEIGHT = height Sets the height of the plot diplay area, in pixels. Default is 450 pixels. MARGIN ( margin_spec ) Sets the size of the four margins. margin_spec can be: TOP = top_margin Sets the top margin, in pixels. Default is 20 pixels. RIGHT = right_margin Sets the right margin, in pixels. Default is 20 pixels. BOTTOM = bottom_margin Sets the bottom margin, in pixels. Default is 30 pixels. LEFT = left_margin Sets the bottom margin, in pixels. Default is 40 pixels. LEGEND ( legend_spec ) Sets the dimensions of the legend display area. legend_spec can be: PADDING = legend_padding Sets the padding between the plot and the legend, in pixels. Default is 20 pixels. WIDTH = legend_width Sets the width of the legend display area, in pixels. Default is 100 pixels.","title":"Display Specification"},{"location":"sclerasql/visualization/#acknowledgements","text":"ScleraViz is inspired by Grammar of Graphics , specifically R's ggplot2 . The plots are rendered using D3 , and the legends are rendered using D3-Legend .","title":"Acknowledgements"},{"location":"sdk/sdkextdataaccess/","text":"In this document, we show how to build custom connectors to any data source. These connectors enable ingestion of data from arbitrary sources in a ScleraSQL query. You only need to format the data as rows of a table, and Sclera will take care of evaluating your SQL queries on the same -- these queries can include transforming, filtering and aggregating this data, as well as joining this data with data ingested from other connectors, or with data in tables stored in other data stores. Sclera - CSV Connector and Sclera - Text Files Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the SQL documentation for external data access . Building Data Access Connectors \u25c4 To build a custom data access connector, you need to provide implementations of the following abstract classes in the SDK: ExternalSourceService ( API Link ) Provides the external data source as a service to Sclera. Contains an id that identifies this service. Contains a method createSource that is used to create a new ExternalSource instance for this service. ExternalSource ( API Link ) Represents the external data source. Provides the schema and other metadata about the sourced data to Sclera at compile-time. Provides the sourced data to Sclera at runtime. TableResult ( API Link ) Represents the data sourced from the external data source. Provides an iterator over the rows containing the sourced data. Example: Building a CSV File Connector \u25c4 This section shows how to implement a lightweight clone of the CSV file connector using the Sclera Extensions SDK. The referenced code is available on GitHub . This connector enables using data from a CSV file within a SQL query. The CSV file can be local or remote as long as it is accessible through an URL. A simple SQL query that uses the connector is as follows: SELECT * FROM EXTERNAL CSVLITE(\"http://scleraviz.herokuapp.com/assets/data/tips.csv\") This query retrieves the CSV data in the file at the specified URL and incorporates it as a virtual table for further processing. A more complicated query could have filters, joins and aggregates on this virtual table. While processing this query, Sclera comes across the EXTERNAL keyword, and accordingly identifies \"CSVLITE\" as an external datasource service. Consulting the service provider specification file , it finds the service provider as an object of the class CSVSourceService ( source ), which implements the abstract class ExternalSourceService . (For details, see the Java Service Provider Interface documentation .) The class CSVSourceService implements the method createSource , as required by the abstract class ExternalSourceService . This method takes a list of datasource parameters specified in the SQL query -- in the query above, the list consists of only one parameter, the URL, of type CharConst . The implementation of the method parses and validates the parameter, and uses them to create an object of the class TickerSource , which implements the abstract class ExternalSource mentioned above. The service identifier is provided by the id attribute. For CSVSourceService , the identifier is \"CSVLITE\" . For the query above, Sclera calls the method createSource of the CSVSourceService object, with the URL as the parameter, and gets back a CSVSource object. The class CSVSource implements the following methods and attributes, as required by the abstract class ExternalSource : The attribute name provides a user-interpretable name to this data source. In our implementation, this is taken to be the same as the service identifier. The attribute columns provides Sclera with the schema of the output that this data source will emit at runtime. The schema contains the name of each column, and its type. This must be consistent with that provided by CSVResult (see below). In our implementation, the CSVResult object is used to provide the columns in CSVSource -- but there may be cases where this coupling might not be possible (for instance, the CSVResult object might get computed later at runtime), hence the redundancy. The method result is called by Sclera at runtime, and returns an object of the class CSVResult , which implements the abstract class TableResult mentioned above. The method toString provides a printable string, which will be used for this datasource when EXPLAIN is run on a query using this data source. Sclera plans the query mentioned above taking into account the schema and result sort order provided by the CSVSource object. When the plan is evaluated , the object's result method returns the object CSVResult , which retrieves the data from the URL, as discussed in a moment. The class CSVResult implements the following methods, as required by the abstract class TableResult : The attribute columns provides Sclera with the schema of the data that this data source will emit at runtime. The schema contains the name of each column, and its type. The method rows returns an iterator over TableRow objects containing the data. A TableRow object can be constructed from a mapping of column names to column values. The column names and data types must be consistent with that provided the attribute columns above. The attribute resultOrder tells Sclera how the emitted data is sorted. This information is optional, but helps in eliminating redundant sorts on the emitted result. In our implementation, the order of rows in the source CSV data is not known, hence this attribute is an empty list. For the query above, Apache Commons CSV Library is used to retrieve the CSV data from the specified URL. This data is converted into an iterator of TableRow instances which is returned as a part of CSVResult , as described above. Packaging and Deploying the Connector \u25c4 The implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead. Dependencies \u25c4 The implementation has a dependency on: the Apache commons-csv library . the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. see the included sbt build file for details. Deployment Steps \u25c4 The connector can be deployed simply by having its jar and all its dependencies in the CLASSPATH . Alternatively, for a managed deployment, follow the following steps: First, publish the implementation as a package, locally or in a public artifact repository. In sbt, you can publish locally by running sbt publishLocal . Use scleradmin to install the component and its dependencies: > scleradmin --add <plugin> --root <sclera-root> where <plugin> is the artifact identifier for your published package, and <sclera-root> is the directory where Sclera is installed. For example, having published the \"CSVLITE\" plugin described above as com.example:sclera-plugin-csv-lite:1.0-SNAPSHOT , we can deploy it as: > scleradmin --add \"com.example:sclera-plugin-csv-lite:1.0-SNAPSHOT\" --root /path/to/sclera The connector should now be visible to Sclera, and can be used in the queries. Note: Please ensure that the identifier you assign to your connector is unique -- that is, it does not conflict with the identifier of any other available ExternalSourceService instance.","title":"Data Access Plugins"},{"location":"sdk/sdkextdataaccess/#building-data-access-connectors","text":"To build a custom data access connector, you need to provide implementations of the following abstract classes in the SDK: ExternalSourceService ( API Link ) Provides the external data source as a service to Sclera. Contains an id that identifies this service. Contains a method createSource that is used to create a new ExternalSource instance for this service. ExternalSource ( API Link ) Represents the external data source. Provides the schema and other metadata about the sourced data to Sclera at compile-time. Provides the sourced data to Sclera at runtime. TableResult ( API Link ) Represents the data sourced from the external data source. Provides an iterator over the rows containing the sourced data.","title":"Building Data Access Connectors"},{"location":"sdk/sdkextdataaccess/#example-building-a-csv-file-connector","text":"This section shows how to implement a lightweight clone of the CSV file connector using the Sclera Extensions SDK. The referenced code is available on GitHub . This connector enables using data from a CSV file within a SQL query. The CSV file can be local or remote as long as it is accessible through an URL. A simple SQL query that uses the connector is as follows: SELECT * FROM EXTERNAL CSVLITE(\"http://scleraviz.herokuapp.com/assets/data/tips.csv\") This query retrieves the CSV data in the file at the specified URL and incorporates it as a virtual table for further processing. A more complicated query could have filters, joins and aggregates on this virtual table. While processing this query, Sclera comes across the EXTERNAL keyword, and accordingly identifies \"CSVLITE\" as an external datasource service. Consulting the service provider specification file , it finds the service provider as an object of the class CSVSourceService ( source ), which implements the abstract class ExternalSourceService . (For details, see the Java Service Provider Interface documentation .) The class CSVSourceService implements the method createSource , as required by the abstract class ExternalSourceService . This method takes a list of datasource parameters specified in the SQL query -- in the query above, the list consists of only one parameter, the URL, of type CharConst . The implementation of the method parses and validates the parameter, and uses them to create an object of the class TickerSource , which implements the abstract class ExternalSource mentioned above. The service identifier is provided by the id attribute. For CSVSourceService , the identifier is \"CSVLITE\" . For the query above, Sclera calls the method createSource of the CSVSourceService object, with the URL as the parameter, and gets back a CSVSource object. The class CSVSource implements the following methods and attributes, as required by the abstract class ExternalSource : The attribute name provides a user-interpretable name to this data source. In our implementation, this is taken to be the same as the service identifier. The attribute columns provides Sclera with the schema of the output that this data source will emit at runtime. The schema contains the name of each column, and its type. This must be consistent with that provided by CSVResult (see below). In our implementation, the CSVResult object is used to provide the columns in CSVSource -- but there may be cases where this coupling might not be possible (for instance, the CSVResult object might get computed later at runtime), hence the redundancy. The method result is called by Sclera at runtime, and returns an object of the class CSVResult , which implements the abstract class TableResult mentioned above. The method toString provides a printable string, which will be used for this datasource when EXPLAIN is run on a query using this data source. Sclera plans the query mentioned above taking into account the schema and result sort order provided by the CSVSource object. When the plan is evaluated , the object's result method returns the object CSVResult , which retrieves the data from the URL, as discussed in a moment. The class CSVResult implements the following methods, as required by the abstract class TableResult : The attribute columns provides Sclera with the schema of the data that this data source will emit at runtime. The schema contains the name of each column, and its type. The method rows returns an iterator over TableRow objects containing the data. A TableRow object can be constructed from a mapping of column names to column values. The column names and data types must be consistent with that provided the attribute columns above. The attribute resultOrder tells Sclera how the emitted data is sorted. This information is optional, but helps in eliminating redundant sorts on the emitted result. In our implementation, the order of rows in the source CSV data is not known, hence this attribute is an empty list. For the query above, Apache Commons CSV Library is used to retrieve the CSV data from the specified URL. This data is converted into an iterator of TableRow instances which is returned as a part of CSVResult , as described above.","title":"Example: Building a CSV File Connector"},{"location":"sdk/sdkextdataaccess/#packaging-and-deploying-the-connector","text":"The implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead.","title":"Packaging and Deploying the Connector"},{"location":"sdk/sdkextdataaccess/#dependencies","text":"The implementation has a dependency on: the Apache commons-csv library . the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. see the included sbt build file for details.","title":"Dependencies"},{"location":"sdk/sdkextdataaccess/#deployment-steps","text":"The connector can be deployed simply by having its jar and all its dependencies in the CLASSPATH . Alternatively, for a managed deployment, follow the following steps: First, publish the implementation as a package, locally or in a public artifact repository. In sbt, you can publish locally by running sbt publishLocal . Use scleradmin to install the component and its dependencies: > scleradmin --add <plugin> --root <sclera-root> where <plugin> is the artifact identifier for your published package, and <sclera-root> is the directory where Sclera is installed. For example, having published the \"CSVLITE\" plugin described above as com.example:sclera-plugin-csv-lite:1.0-SNAPSHOT , we can deploy it as: > scleradmin --add \"com.example:sclera-plugin-csv-lite:1.0-SNAPSHOT\" --root /path/to/sclera The connector should now be visible to Sclera, and can be used in the queries. Note: Please ensure that the identifier you assign to your connector is unique -- that is, it does not conflict with the identifier of any other available ExternalSourceService instance.","title":"Deployment Steps"},{"location":"sdk/sdkextdbms/","text":"In this document, we show how to build custom connectors to any relational/non-relational database management system. These connectors interface Sclera with an arbitrary database system, relational or non-relational, providing access to the underlying data, and also enable Sclera to push down computations in relevant parts user queries and commands on the interfaced database system. Sclera - Oracle Connector , Sclera - MySQL Connector , and Sclera - PostgreSQL Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the documentation on connecting Sclera to database systems . Building Database System Connectors \u25c4 To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: DBService ( API Link ) Provides the database system as a service to Sclera. Contains an id that identifies this service. Contains a method createLocation that is used to create a new location instance for this service. Location ( API Link ) Represents the underlying database system. Provides the configuration parameters, and properties (e.g. temporary or persistent, read-only or read-write, etc.). Provides the driver for interfacing with the underlying system. StatementDriver ( API Link ) Executes the statements provided by Sclera on the underlying database system, and passes back the results. Provides the metadata driver for accessing the metadata for the data stored in the underlying database system. StatementMetadataDriver ( API Link ) Provides the metadata for the data stored in the underlying database system. Special case: Relational Databases \u25c4 If the underlying system is a relational database system, which talks SQL and uses JDBC as the interface, Sclera does most of the work. For such systems, you need to provide implementations of the following: DBService ( API Link ) Provides the relational database system as a service to Sclera. RdbmsLocation ( API LInk ) Represents the underlying relational database system. Provides a standard implementation of StatementDriver and StatementMetadataDriver , based on JDBC. You only need to configure the JDBC configuration parameters (e.g. the JDBC URL) for the underlying system, and additional location properties (e.g. temporary or persistent, read-only or read-write, etc.). Provides the SQL mapper for translating Sclera's internal SQL representation to the SQL for the underlying system. SqlMapper ( API LInk ) Translates Sclera's internal SQL representation to the SQL for the underlying system. This is needed because the SQL clauses and constructs across different systems vary significantly and sometimes do not follow the standard . The Sclera - MySQL Connector , included with the Sclera platform, is open source and implements the relational database interface mentioned above. Packaging and Deploying the Connector \u25c4 The included Sclera - MySQL Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead. Dependencies \u25c4 The implementation has a dependency on: the database system's driver (e.g. the appropriate JDBC driver for relational database systems). the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - MySQL Connector's build file . Deployment Steps \u25c4 Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available DBService instance.","title":"Database System Plugins"},{"location":"sdk/sdkextdbms/#building-database-system-connectors","text":"To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: DBService ( API Link ) Provides the database system as a service to Sclera. Contains an id that identifies this service. Contains a method createLocation that is used to create a new location instance for this service. Location ( API Link ) Represents the underlying database system. Provides the configuration parameters, and properties (e.g. temporary or persistent, read-only or read-write, etc.). Provides the driver for interfacing with the underlying system. StatementDriver ( API Link ) Executes the statements provided by Sclera on the underlying database system, and passes back the results. Provides the metadata driver for accessing the metadata for the data stored in the underlying database system. StatementMetadataDriver ( API Link ) Provides the metadata for the data stored in the underlying database system.","title":"Building Database System Connectors"},{"location":"sdk/sdkextdbms/#special-case-relational-databases","text":"If the underlying system is a relational database system, which talks SQL and uses JDBC as the interface, Sclera does most of the work. For such systems, you need to provide implementations of the following: DBService ( API Link ) Provides the relational database system as a service to Sclera. RdbmsLocation ( API LInk ) Represents the underlying relational database system. Provides a standard implementation of StatementDriver and StatementMetadataDriver , based on JDBC. You only need to configure the JDBC configuration parameters (e.g. the JDBC URL) for the underlying system, and additional location properties (e.g. temporary or persistent, read-only or read-write, etc.). Provides the SQL mapper for translating Sclera's internal SQL representation to the SQL for the underlying system. SqlMapper ( API LInk ) Translates Sclera's internal SQL representation to the SQL for the underlying system. This is needed because the SQL clauses and constructs across different systems vary significantly and sometimes do not follow the standard . The Sclera - MySQL Connector , included with the Sclera platform, is open source and implements the relational database interface mentioned above.","title":"Special case: Relational Databases"},{"location":"sdk/sdkextdbms/#packaging-and-deploying-the-connector","text":"The included Sclera - MySQL Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead.","title":"Packaging and Deploying the Connector"},{"location":"sdk/sdkextdbms/#dependencies","text":"The implementation has a dependency on: the database system's driver (e.g. the appropriate JDBC driver for relational database systems). the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - MySQL Connector's build file .","title":"Dependencies"},{"location":"sdk/sdkextdbms/#deployment-steps","text":"Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available DBService instance.","title":"Deployment Steps"},{"location":"sdk/sdkextml/","text":"In this document, we show how to build custom connectors to any machine learning library. These connectors enable Sclera to include machine learning objects (such as classifiers) as first-class SQL objects (at par with Tables and Views , for instance), and also include machine learning tasks (such as classification) as relational operators within SQL. This is achieved by mapping the training and execution to the interfaced library's API, transforming the input and the output, and converting the result to a relational stream for consumption of other SQL operators. Sclera - Weka Connector is built using this SDK. For examples of how these connectors are used in Sclera, please refer to the documentation on using machine learning in SQL . Building Machine Learning Library Connectors \u25c4 To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: MLService ( API Link ) Provides machine learning operators as a service to Sclera, using the specified library. Contains an id that identifies this service. Contains the method createClassifier , createClusterer that is used to create (i.e. train), respectively, a new Classifier or Clusterer for this service. Classifier ( API Link ) Wrapper over classes implementing clustering algorithms. Training involves learning a classifier with a designated targetAttr using the feature attributes featureAttrs , all of which must be present in the input. Provides a function classifyOpt that returns the label for a new data point, if one can be assigned by the classifier; this is used by the CLASSIFIED WITH clause . Clusterer ( API Link ) Wrapper over classes implementing clustering algorithms. Training involves clustering the given data. Provides a function cluster that assigns a cluster id to a new data point; this is used by the CLUSTERED WITH clause . The Sclera - Weka Connector , included with the Sclera platform, is open source and implements the interface mentioned above. Packaging and Deploying the Connector \u25c4 The included Sclera - Weka Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead. Dependencies \u25c4 The implementation has a dependency on: the library for the machine learning package used. the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - Weka Connector's build file . Deployment Steps \u25c4 Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available MLService instance.","title":"Machine Learning Library Plugins"},{"location":"sdk/sdkextml/#building-machine-learning-library-connectors","text":"To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: MLService ( API Link ) Provides machine learning operators as a service to Sclera, using the specified library. Contains an id that identifies this service. Contains the method createClassifier , createClusterer that is used to create (i.e. train), respectively, a new Classifier or Clusterer for this service. Classifier ( API Link ) Wrapper over classes implementing clustering algorithms. Training involves learning a classifier with a designated targetAttr using the feature attributes featureAttrs , all of which must be present in the input. Provides a function classifyOpt that returns the label for a new data point, if one can be assigned by the classifier; this is used by the CLASSIFIED WITH clause . Clusterer ( API Link ) Wrapper over classes implementing clustering algorithms. Training involves clustering the given data. Provides a function cluster that assigns a cluster id to a new data point; this is used by the CLUSTERED WITH clause . The Sclera - Weka Connector , included with the Sclera platform, is open source and implements the interface mentioned above.","title":"Building Machine Learning Library Connectors"},{"location":"sdk/sdkextml/#packaging-and-deploying-the-connector","text":"The included Sclera - Weka Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead.","title":"Packaging and Deploying the Connector"},{"location":"sdk/sdkextml/#dependencies","text":"The implementation has a dependency on: the library for the machine learning package used. the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - Weka Connector's build file .","title":"Dependencies"},{"location":"sdk/sdkextml/#deployment-steps","text":"Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available MLService instance.","title":"Deployment Steps"},{"location":"sdk/sdkexttext/","text":"In this document, we show how to build custom connectors to any NLP / text analytics library to perform text analytics tasks. These connectors handle the invocation of the underlying library to process text data in table columns, in the query processing pipeline. The component Sclera - Apache OpenNLP Connector is built using this SDK. For examples of how the connector is used in Sclera, please refer to the documentation on using text analytics in SQL . Building Text Analytics Library Connectors \u25c4 To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: NlpService ( API Link ) Provides text analytics operators as a service to Sclera, using the specified library. Contains an id that identifies this service. Contains the method createObject that is used to create a new task object for the task named in the parameter taskName for this service. NlpTask ( API Link ) Wrapper over classes implementing text analytics algorithms. Provides a function eval that takes a data stream (an iterator over rows, with associated metadata) as input and returns the same data stream, with each row augmented by columns resultCols containing the output of executing the task taskName on the text in column inputCol . If the evaluation on a row emits multiple evaluation results, the input row is repeated in the output for each such result. The Sclera - Apache OpenNLP Connector , included with the Sclera platform, is open source and implements the interface mentioned above. The code for the Sclera - Apache OpenNLP Connector , in Scala, also appears as an illustrative example in the Sclera Extensions (Scala) Github repository . Packaging and Deploying the Connector \u25c4 The included Sclera - Apache OpenNLP Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead. Dependencies \u25c4 The implementation has a dependency on: the library for the text analytics package used. the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - Apache OpenNLP Connector's build file . Deployment Steps \u25c4 Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available NlpService instance.","title":"Text Analytics Library Plugins"},{"location":"sdk/sdkexttext/#building-text-analytics-library-connectors","text":"To build a custom datasource connector, you need to provide implementations of the following abstract classes in the SDK: NlpService ( API Link ) Provides text analytics operators as a service to Sclera, using the specified library. Contains an id that identifies this service. Contains the method createObject that is used to create a new task object for the task named in the parameter taskName for this service. NlpTask ( API Link ) Wrapper over classes implementing text analytics algorithms. Provides a function eval that takes a data stream (an iterator over rows, with associated metadata) as input and returns the same data stream, with each row augmented by columns resultCols containing the output of executing the task taskName on the text in column inputCol . If the evaluation on a row emits multiple evaluation results, the input row is repeated in the output for each such result. The Sclera - Apache OpenNLP Connector , included with the Sclera platform, is open source and implements the interface mentioned above. The code for the Sclera - Apache OpenNLP Connector , in Scala, also appears as an illustrative example in the Sclera Extensions (Scala) Github repository .","title":"Building Text Analytics Library Connectors"},{"location":"sdk/sdkexttext/#packaging-and-deploying-the-connector","text":"The included Sclera - Apache OpenNLP Connector implementation uses sbt for building the connector (installation details) . This is not a requirement -- any other build tool can be used instead.","title":"Packaging and Deploying the Connector"},{"location":"sdk/sdkexttext/#dependencies","text":"The implementation has a dependency on: the library for the text analytics package used. the \"sclera-core\" and \"sclera-config\" core components. Note that these dependencies is annotated \"provided\" since these libraries will already be available in the CLASSPATH when this connector is run with Sclera. (optional) the test framework scalatest for running the tests. These are specified in the build file. As an example, see the Sclera - Apache OpenNLP Connector's build file .","title":"Dependencies"},{"location":"sdk/sdkexttext/#deployment-steps","text":"Follow steps similar to those described here . Note: Please ensure that the identifier you assign to the connector is unique, that is - does not conflict with the identifier of any other available NlpService instance.","title":"Deployment Steps"},{"location":"sdk/sdkintro/","text":"Sclera is an open platform that can be extended, as needed, with extensions that connect Sclera with multiple data sources, platforms and analytics libraries. The components documentation describes the pre-built extensions that are packaged with the Sclera installation. These custom connectors can be built using the Sclera Extensions Software Development Kit (SDK) , provided natively by the sclera-core component . For details on the API, please refer to the sclera-core scaladoc . The following type of components are covered: Data Access Connectors \u25c4 These connectors enable ingestion of data from arbitrary sources in a ScleraSQL query. You only need to format the data as rows of a table, and Sclera will take care of evaluating streaming SQL queries on the same -- these queries can include transforming, filtering and aggregating this data, as well as joining this data with data ingested from other connectors, or with data in tables stored in other data stores. Sclera - CSV Connector and Sclera - Text Files Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building data access connectors, please see the Sclera Datasource Extensions SDK documentation. Database System Connectors \u25c4 These connectors interface Sclera with an arbitrary database system, relational or non-relational, providing access to the underlying data, and also enable Sclera to push down computations in relevant parts user queries and commands on the interfaced database system. Sclera - Oracle Connector , Sclera - MySQL Connector , and Sclera - PostgreSQL Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the data platform connection reference documentation . To learn more about building database system connectors, please see the Sclera Database System Extensions SDK documentation. Machine Learning Library Connectors \u25c4 These connectors interface Sclera with an arbitrary machine learning libraries, which provide implementations of classification, clustering and/or association rules mining. These connectors handle the invocation of the underlying library for training the models, and using them to label data in the query processing pipeline. Sclera - Weka Connector is built using this SDK. For examples of how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building machine learning library connectors, please see the Sclera Machine Learning Library Extensions SDK documentation. Text Analytics Library Connectors \u25c4 These connectors interface Sclera with an arbitrary text analytics libraries, to perform specific text analytics tasks. These connectors handle the invocation of the underlying library to process text data in table columns, in the query processing pipeline. Sclera - Apache OpenNLP Connector is built using this SDK. For examples on how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building text analytics library connectors, please see the Sclera Text Analytics Library Extensions SDK documentation.","title":"Extensions SDK Introduction"},{"location":"sdk/sdkintro/#data-access-connectors","text":"These connectors enable ingestion of data from arbitrary sources in a ScleraSQL query. You only need to format the data as rows of a table, and Sclera will take care of evaluating streaming SQL queries on the same -- these queries can include transforming, filtering and aggregating this data, as well as joining this data with data ingested from other connectors, or with data in tables stored in other data stores. Sclera - CSV Connector and Sclera - Text Files Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building data access connectors, please see the Sclera Datasource Extensions SDK documentation.","title":"Data Access Connectors"},{"location":"sdk/sdkintro/#database-system-connectors","text":"These connectors interface Sclera with an arbitrary database system, relational or non-relational, providing access to the underlying data, and also enable Sclera to push down computations in relevant parts user queries and commands on the interfaced database system. Sclera - Oracle Connector , Sclera - MySQL Connector , and Sclera - PostgreSQL Connector are built using this SDK. For examples of how these connectors are used in Sclera, please refer to the data platform connection reference documentation . To learn more about building database system connectors, please see the Sclera Database System Extensions SDK documentation.","title":"Database System Connectors"},{"location":"sdk/sdkintro/#machine-learning-library-connectors","text":"These connectors interface Sclera with an arbitrary machine learning libraries, which provide implementations of classification, clustering and/or association rules mining. These connectors handle the invocation of the underlying library for training the models, and using them to label data in the query processing pipeline. Sclera - Weka Connector is built using this SDK. For examples of how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building machine learning library connectors, please see the Sclera Machine Learning Library Extensions SDK documentation.","title":"Machine Learning Library Connectors"},{"location":"sdk/sdkintro/#text-analytics-library-connectors","text":"These connectors interface Sclera with an arbitrary text analytics libraries, to perform specific text analytics tasks. These connectors handle the invocation of the underlying library to process text data in table columns, in the query processing pipeline. Sclera - Apache OpenNLP Connector is built using this SDK. For examples on how these connectors are used in Sclera, please refer to the SQL documentation . To learn more about building text analytics library connectors, please see the Sclera Text Analytics Library Extensions SDK documentation.","title":"Text Analytics Library Connectors"},{"location":"setup/components/","text":"This document lists the various components of a Sclera installation. Required Components \u25c4 This section lists the components that are required for Sclera to work. Sclera - Core Engine \u25c4 Source Code \u00bb This is the core Sclera engine, which is responsible for parsing, optimizing and evaluating SQL commands and queries with the help of the other components. For the details, please see the technical details document. This component includes an embedded H2 database , which serves as the default metadata store and data cache . Sclera - Configuration Manager \u25c4 Source Code \u00bb Manages the configuration parameters for Sclera. The default configuratuon parameters are specified in reference.conf and can be overriden with a user-defined configuration file sclera.conf . Optional Extensions \u25c4 The components listed in this section are optional extensions -- they are not core to the working of the Sclera engine. Sclera - Command Line Shell \u25c4 Source Code \u00bb This component provides a command-line shell for interactive SQL processing. This shell accepts SQL queries and returns the result in a formatted manner. In addition, it supports administrative commands, and also additional commands for querying the metadata. For the details on how to use the command line shell, please refer to the Sclera Command Line Shell Reference document. Sclera - JDBC Driver \u25c4 Source Code \u00bb This component provides an embedded JDBC 4 interface to Sclera. The JDBC support is partial (for instance, functions related to transaction processing are not supported, and only forward scans of resultsets are permitted). However, the supported API should suffice for most analytics applications, and for interfacing with most JDBC-compliant BI tools. A detailed description on how to use the JDBC API appears in the Sclera JDBC Reference document. Sclera - CSV File Connector \u25c4 Source Code \u00bb This component enables Sclera to work with your data stored on your disk as CSV files . The CSV files are viewed as tables, and can be accessed in a manner similar to tables in a SQL query. You can also join the CSV file with tables in your database, with other CSV files, and aggregate the data as needed. Further details on how to use the connector are in the ScleraSQL Reference document. This is a sample component showcasing Sclera's ability to interface with external data. For the implementation details, please see the Sclera Data Access Connector Development document. Sclera - Text Files Connector \u25c4 Source Code \u00bb This component enables Sclera to work with free-form text files. The text files are viewed as tables, with two columns: an identifier column containing the file's path, and another column containing the file's contents. These files can now be accessed in a manner similar to tables in a SQL query. A common use case is to use this in conjunction with the Sclera - OpenNLP Connector which can be used to extract entities from the file contents. For details on how to use the connector, please see the ScleraSQL Reference document. This is a sample component showcasing Sclera's ability to interface with external data. For the implementation details, please see the Sclera Data Access Connector Development document. Sclera - PostgreSQL Connector \u25c4 Source Code \u00bb This component enables Sclera to work with your data stored in PostgreSQL . You just need to link your PostgreSQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The link uses the PostgreSQL JDBC Driver , which is downloaded as a part of the installation of this component. Details on how to link your PostgreSQL source to with Sclera can be found in the Sclera Database System Connection Reference document. Sclera - MySQL Connector \u25c4 Source Code \u00bb To work with Sclera, MySQL should be configured in the case-insensitive mode . This component enables Sclera to work with your data stored in MySQL . You just need to link your MySQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The connector uses MySQL Connector/J , which is automatically downloaded during the installation of this component. Details on how to link your MySQL source to with Sclera can be found in the Sclera Database System Connection Reference document. Important The MySQL Connector/J JDBC driver is licensed under the GNU General Public License version 2 with FOSS exception . Sclera - MySQL Connector is licensed under the Apache License version 2.0 , which is compatible with the said FOSS exception. Sclera - Heroku PostgreSQL Connector \u25c4 Source Code \u00bb This component enables Sclera to work with your data stored in PostgreSQL database hosted at Heroku . You just need to link your Heroku PostgreSQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. Details on how to link your Heroku PostgreSQL source to with Sclera can be found in the Sclera Database System Connection Reference document. Sclera - Oracle Connector \u25c4 Source Code \u00bb This component enables Sclera to work with your data stored in Oracle . You just need to link your Oracle database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The link uses the Oracle Thin JDBC Driver , which is not downloaded as a part of the installation of this component. You need to download the driver manually before using this component. Details on how to link your Oracle source to with Sclera can be found in the Sclera Database System Connection Reference document. Sclera - Weka Connector \u25c4 Source Code \u00bb This component enables Sclera to perform classification , clustering and association rule mining on data from within SQL. With this component, a classifier or a clusterer can be trained in just a single SQL command. Scoring new data using the classifier, or segmenting data using the clusterer gets done using a simple SQL operator (Sclera's extension) that seamlessly embeds within your SQL query. The component uses the Weka library, which is downloaded automatically as a part of the installation. Please refer to the ScleraSQL Reference document for details on using the component's features in a SQL query. Important The Weka library is licensed under the GNU General Public License version 2 . For compatibility , this component is licensed under the GNU General Public License version 2 as well. Please use this component in accordance with this license. To get a commercial license for Weka, please refer to the Weka FAQ . In keeping with the provisions of the GNU General Public License version 2, the source code for this component is available for download at the Sclera repository . This component is an OPTIONAL extension. As such, this component's license does NOT affect your use of any other Sclera component, or the core Sclera platform. Sclera - Apache OpenNLP Connector \u25c4 Source Code \u00bb This component enables Sclera to perform text analytics on free-form text. Current version of this component only supports extracting entities (such as names of persons and places, dates, emails) from the text. Later versions will include additional features such as sentiment/opinion mining. The entity extraction is exposed as a SQL operator (Sclera's extension) which can act on any relational input. The operator is given the name of the column containing the text data, and the output is the input will additional columns containing the extracted information. The output can then be aggregated, joined with other tables, etc. as usual within the SQL query. This component uses the Apache OpenNLP library, which is downloaded automatically as a part of the installation. To use this component, you will also need to provide Sclera with trained models for a sentence detector and name finders (extractors) for your language. These are not packaged with Sclera, but can be downloaded separately from the Apache OpenNLP models repository . The site provides models in Danish (code: da ), German (code: de ), English (code: en ), Dutch (code: dl ), Portuguese (code: pt ) and Swedish (code: se ). The models files can be downloaded from the site and kept in the directory $SCLERA_ASSETS/opennlp , where $SCLERA_ASSETS is the directory given by the sclera.services.assetdir configuration parameter . For greater accuracy on your data, you can also create your own name finders using Apache OpenNLP's toolkit . Please refer to the ScleraSQL Reference document for details on using the component's features in a SQL query. Sclera - Regular Expression Matcher \u25c4 Source Code \u00bb This component enables Sclera to efficiently and flexibly analyze ordered streaming data. The component introduces a construct that enables matching regular expressions over streaming data, and using them to compute sophisticated aggregates. This is a powerful construct, proprietary to Sclera, and enables computations that are ridiculously hard to express and expensive to compute using standard SQL. For details and examples on using these constructs in a SQL query, please refer to the ScleraSQL Reference document. Sclera - Web Display \u25c4 Source Code \u00bb This component enables users to visualize Sclera query results in a web-browser. This enables a richer, more visual experience with extensive support for data visualization. Specifically, you can run queries and display the results as a table, or use a very expressive graphics language to plot the results as regular, multilayered and faceted graphs in just a few lines of code. The graph specification language is inspired by the \"Grammar of Graphics\" (implemented in R as ggplot2 ), and is rendered using D3 in SVG . Unlike ggplot2 , the resulting plots are interactive, and can display streaming data in a continuous manner. Moreover, the specification language is well-integrated with ScleraSQL . For details and examples on using these constructs, please refer to the ScleraSQL Visualization Reference document.","title":"Components Reference"},{"location":"setup/components/#required-components","text":"This section lists the components that are required for Sclera to work.","title":"Required Components"},{"location":"setup/components/#sclera-core-engine","text":"Source Code \u00bb This is the core Sclera engine, which is responsible for parsing, optimizing and evaluating SQL commands and queries with the help of the other components. For the details, please see the technical details document. This component includes an embedded H2 database , which serves as the default metadata store and data cache .","title":"Sclera - Core Engine"},{"location":"setup/components/#sclera-configuration-manager","text":"Source Code \u00bb Manages the configuration parameters for Sclera. The default configuratuon parameters are specified in reference.conf and can be overriden with a user-defined configuration file sclera.conf .","title":"Sclera - Configuration Manager"},{"location":"setup/components/#optional-extensions","text":"The components listed in this section are optional extensions -- they are not core to the working of the Sclera engine.","title":"Optional Extensions"},{"location":"setup/components/#sclera-command-line-shell","text":"Source Code \u00bb This component provides a command-line shell for interactive SQL processing. This shell accepts SQL queries and returns the result in a formatted manner. In addition, it supports administrative commands, and also additional commands for querying the metadata. For the details on how to use the command line shell, please refer to the Sclera Command Line Shell Reference document.","title":"Sclera - Command Line Shell"},{"location":"setup/components/#sclera-jdbc-driver","text":"Source Code \u00bb This component provides an embedded JDBC 4 interface to Sclera. The JDBC support is partial (for instance, functions related to transaction processing are not supported, and only forward scans of resultsets are permitted). However, the supported API should suffice for most analytics applications, and for interfacing with most JDBC-compliant BI tools. A detailed description on how to use the JDBC API appears in the Sclera JDBC Reference document.","title":"Sclera - JDBC Driver"},{"location":"setup/components/#sclera-csv-file-connector","text":"Source Code \u00bb This component enables Sclera to work with your data stored on your disk as CSV files . The CSV files are viewed as tables, and can be accessed in a manner similar to tables in a SQL query. You can also join the CSV file with tables in your database, with other CSV files, and aggregate the data as needed. Further details on how to use the connector are in the ScleraSQL Reference document. This is a sample component showcasing Sclera's ability to interface with external data. For the implementation details, please see the Sclera Data Access Connector Development document.","title":"Sclera - CSV File Connector"},{"location":"setup/components/#sclera-text-files-connector","text":"Source Code \u00bb This component enables Sclera to work with free-form text files. The text files are viewed as tables, with two columns: an identifier column containing the file's path, and another column containing the file's contents. These files can now be accessed in a manner similar to tables in a SQL query. A common use case is to use this in conjunction with the Sclera - OpenNLP Connector which can be used to extract entities from the file contents. For details on how to use the connector, please see the ScleraSQL Reference document. This is a sample component showcasing Sclera's ability to interface with external data. For the implementation details, please see the Sclera Data Access Connector Development document.","title":"Sclera - Text Files Connector"},{"location":"setup/components/#sclera-postgresql-connector","text":"Source Code \u00bb This component enables Sclera to work with your data stored in PostgreSQL . You just need to link your PostgreSQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The link uses the PostgreSQL JDBC Driver , which is downloaded as a part of the installation of this component. Details on how to link your PostgreSQL source to with Sclera can be found in the Sclera Database System Connection Reference document.","title":"Sclera - PostgreSQL Connector"},{"location":"setup/components/#sclera-mysql-connector","text":"Source Code \u00bb To work with Sclera, MySQL should be configured in the case-insensitive mode . This component enables Sclera to work with your data stored in MySQL . You just need to link your MySQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The connector uses MySQL Connector/J , which is automatically downloaded during the installation of this component. Details on how to link your MySQL source to with Sclera can be found in the Sclera Database System Connection Reference document. Important The MySQL Connector/J JDBC driver is licensed under the GNU General Public License version 2 with FOSS exception . Sclera - MySQL Connector is licensed under the Apache License version 2.0 , which is compatible with the said FOSS exception.","title":"Sclera - MySQL Connector"},{"location":"setup/components/#sclera-heroku-postgresql-connector","text":"Source Code \u00bb This component enables Sclera to work with your data stored in PostgreSQL database hosted at Heroku . You just need to link your Heroku PostgreSQL database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. Details on how to link your Heroku PostgreSQL source to with Sclera can be found in the Sclera Database System Connection Reference document.","title":"Sclera - Heroku PostgreSQL Connector"},{"location":"setup/components/#sclera-oracle-connector","text":"Source Code \u00bb This component enables Sclera to work with your data stored in Oracle . You just need to link your Oracle database with Sclera, then import the metadata of select tables within the database. All this gets done in a couple of commands -- and enables you to include these tables within your Sclera queries. The link uses the Oracle Thin JDBC Driver , which is not downloaded as a part of the installation of this component. You need to download the driver manually before using this component. Details on how to link your Oracle source to with Sclera can be found in the Sclera Database System Connection Reference document.","title":"Sclera - Oracle Connector"},{"location":"setup/components/#sclera-weka-connector","text":"Source Code \u00bb This component enables Sclera to perform classification , clustering and association rule mining on data from within SQL. With this component, a classifier or a clusterer can be trained in just a single SQL command. Scoring new data using the classifier, or segmenting data using the clusterer gets done using a simple SQL operator (Sclera's extension) that seamlessly embeds within your SQL query. The component uses the Weka library, which is downloaded automatically as a part of the installation. Please refer to the ScleraSQL Reference document for details on using the component's features in a SQL query. Important The Weka library is licensed under the GNU General Public License version 2 . For compatibility , this component is licensed under the GNU General Public License version 2 as well. Please use this component in accordance with this license. To get a commercial license for Weka, please refer to the Weka FAQ . In keeping with the provisions of the GNU General Public License version 2, the source code for this component is available for download at the Sclera repository . This component is an OPTIONAL extension. As such, this component's license does NOT affect your use of any other Sclera component, or the core Sclera platform.","title":"Sclera - Weka Connector"},{"location":"setup/components/#sclera-apache-opennlp-connector","text":"Source Code \u00bb This component enables Sclera to perform text analytics on free-form text. Current version of this component only supports extracting entities (such as names of persons and places, dates, emails) from the text. Later versions will include additional features such as sentiment/opinion mining. The entity extraction is exposed as a SQL operator (Sclera's extension) which can act on any relational input. The operator is given the name of the column containing the text data, and the output is the input will additional columns containing the extracted information. The output can then be aggregated, joined with other tables, etc. as usual within the SQL query. This component uses the Apache OpenNLP library, which is downloaded automatically as a part of the installation. To use this component, you will also need to provide Sclera with trained models for a sentence detector and name finders (extractors) for your language. These are not packaged with Sclera, but can be downloaded separately from the Apache OpenNLP models repository . The site provides models in Danish (code: da ), German (code: de ), English (code: en ), Dutch (code: dl ), Portuguese (code: pt ) and Swedish (code: se ). The models files can be downloaded from the site and kept in the directory $SCLERA_ASSETS/opennlp , where $SCLERA_ASSETS is the directory given by the sclera.services.assetdir configuration parameter . For greater accuracy on your data, you can also create your own name finders using Apache OpenNLP's toolkit . Please refer to the ScleraSQL Reference document for details on using the component's features in a SQL query.","title":"Sclera - Apache OpenNLP Connector"},{"location":"setup/components/#sclera-regular-expression-matcher","text":"Source Code \u00bb This component enables Sclera to efficiently and flexibly analyze ordered streaming data. The component introduces a construct that enables matching regular expressions over streaming data, and using them to compute sophisticated aggregates. This is a powerful construct, proprietary to Sclera, and enables computations that are ridiculously hard to express and expensive to compute using standard SQL. For details and examples on using these constructs in a SQL query, please refer to the ScleraSQL Reference document.","title":"Sclera - Regular Expression Matcher"},{"location":"setup/components/#sclera-web-display","text":"Source Code \u00bb This component enables users to visualize Sclera query results in a web-browser. This enables a richer, more visual experience with extensive support for data visualization. Specifically, you can run queries and display the results as a table, or use a very expressive graphics language to plot the results as regular, multilayered and faceted graphs in just a few lines of code. The graph specification language is inspired by the \"Grammar of Graphics\" (implemented in R as ggplot2 ), and is rendered using D3 in SVG . Unlike ggplot2 , the resulting plots are interactive, and can display streaming data in a continuous manner. Moreover, the specification language is well-integrated with ScleraSQL . For details and examples on using these constructs, please refer to the ScleraSQL Visualization Reference document.","title":"Sclera - Web Display"},{"location":"setup/configuration/","text":"In this document we list the Sclera's configuration parameters, specified in <sclera-root>/config/sclera.conf , where <sclera-root> is the directory where Sclera is installed . The syntax of the file should be self-evident. For a detailed description of the underlying HOCON (Human-Optimized Config Object Notation) syntax, please refer to the specification document . Parameter Description sclera.exec.batchsize Number of rows to batch together for bulk input/output (default 100). sclera.location.datacache Location for cache data/intermediate results sclera.location.default Location to be used when explicit location is not specified sclera.location.schema.database Database in the schema DBMS for storing the data schema sclera.location.schema.dbms DBMS for storing the data schema sclera.location.temporary Location for storing temporary data sclera.service.assetdir Directory for storing component assets sclera.service.default.mlservice Default machine learning service sclera.service.default.nlpservice Default natural language processing service sclera.shell.explain Default value for explain (true = on, or false = off) sclera.shell.history File for storing Sclera shell history sclera.shell.prompt Shell prompt sclera.storage.datadir Directory for storing datafiles for embedded H2 sclera.storage.objectdir Directory for storing Sclera objects","title":"Configuration Reference"},{"location":"setup/dbms/","text":"Sclera enables a consolidated relational view of multiple underlying relational/non-relational data stores. We now describe how to connect your existing database systems to Sclera. This will enable you to seamlessly execute cross-platform computations via SQL, as discussed in the ScleraSQL Reference document. In addition to the data stores mentioned in this document, Sclera can also incorporate data from CSV files, raw text files, web-services, all within the same SQL query -- for details, please refer to the ScleraSQL External Data Access documentation. In the following, the keywords appear in upper case to distinguish them from the other terms; however, Sclera is case-insensitive and keywords in actual commands and queries can be in upper or lower case. Connecting to Database Systems \u25c4 Sclera can connect to Oracle Database 11g Release 2+ , PostgreSQL 9.1.2+ , and MySQL 5.5.28+ (and also MySQL-compatible systems such as Google Cloud SQL ). To connect with a database system not mentioned here, you will need a connector for that database system, which can be built using Sclera's plugin framework . In Sclera, a connected database system is called a location . A new location is added to Sclera using the ADD LOCATION command, which has the following syntax: ADD [ READONLY ] LOCATION location_name AS location_dbms( location_database [ , connection_properties [, ...] ] ) The optional READONLY modifier flags the location as a read-only location; this is explained later . The mandatory location_name is a unique name given to the database system being added, for reference within Sclera. The mandatory location_dbms is the name of the database system: MYSQL , or POSTGRESQL ; each of these are discussed in turn in the subsections that follow. The mandatory location_database is the name of the database within the location_dbms that contains the data we need to access; as discussed in the subsections below. The optional connection_properties list contains optional connection configuration properties for the location_dbms ; these are discussed in context of of specific database systems in the subsections below. Connecting to Oracle \u25c4 The following sets up the data location oraloc , configured as a JDBC connection to the Oracle database oradb on host orahost , port 1521 : > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost (port 1521 ), you can omit the host name: > ADD LOCATION oraloc AS ORACLE(\"oradb\"); The JDBC connection is set up using the Oracle Database Thin JDBC Driver , which is not downloaded during the Sclera-Oracle Connector installation. You will need to download this driver before using this component, as explained later Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\", \"user=orascott\", \"password=oratiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to specify the internal_logon property : > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\", \"user=orascott\", \"password=oratiger\", \"internal_logon=SYSDBA\"); Any of the properties listed in the Oracle JDBC documentation can be specified. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details. Setup Before using this component, you need to download the Oracle Database Thin JDBC Driver , and include the path to the downloaded driver in the CLASSPATH . Connecting to MySQL \u25c4 To work with Sclera, MySQL should be configured in the case-insensitive mode . The following sets up the data location myloc , configured as a JDBC connection to the MySQL database mydb on host myhost : > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost , you can omit the host name: > ADD LOCATION myloc AS MYSQL(\"mydb\"); The JDBC connection is set up using MySQL Connector/J , which is downloaded during the Sclera-MySQL Connector installation. (Note: Sclera assumes that MySQL runs in a case-insensitive mode .) Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\", \"user=myscott\", \"password=mytiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to use SSL connections: > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\", \"user=myscott\", \"password=mytiger\", \"useSSL=true\"); Any of the properties listed in the MySQL Connector/J documentation can be specified, except useOldAliasMetadataBehavior , which is internally set to true by Sclera. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details. Connecting to PostgreSQL \u25c4 The following sets up the data location pgloc , configured as a JDBC connection to the PostgreSQL database pgdb on host pghost : > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost , you can omit the host name: > ADD LOCATION pgloc AS POSTGRESQL(\"pgdb\"); The JDBC connection is set up using the PostgreSQL JDBC Driver , which is downloaded during the Sclera-PostgreSQL Connector installation. Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=pgtiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to use SSL connections: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=pgtiger\", \"ssl=true\"); Any of the properties listed in the PostgreSQL JDBC Driver documentation can be specified as above. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details. Connecting to Heroku Postgres \u25c4 The following sets up the data location heroku , configured as a JDBC connection to a Heroku Postgres database: > ADD LOCATION heroku AS HEROKU_POSTGRESQL(\"postgres://<username>:<password>@<host>:<port>/<dbname>\"); The parameter is the DATABASE_URL in the format provided by Heroku -- see Heroku's documentation for the details. The JDBC connection is set up using the PostgreSQL JDBC Driver . Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Note: Heroku Postgres connectivity was introduced in Sclera 3.0 and is currently in beta. Interactive Input of Property Values \u25c4 Sometimes, you may not want to specify a property value as a part of the statement. For instance, it might be desirable to not disclose the password in the statements for MySQL and PostgreSQL above. In such cases, you can specify the character ' ? ' as the property value instead. While processing the statement, Sclera will prompt for the actual property value and accept the input in a secure manner. For example: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=?\"); password: ******* Here, the password=? generates a prompt that enables you to enter the password interactively. Read-Write versus Read-Only Mode \u25c4 By default, Sclera uses the underlying database system in a read-write mode. This means that: The database system acts as a data source. However, if the data needs to be filtered, projected, aggregated, and/or joined with other tables within this database system, then the computation is pushed to this database system and only the result is read by Sclera. If the result as above needs to join with data coming from elsewhere, Sclera may decide to materialize the external data as a table within this database system, and perform the join on this database system. This involves creating a table, inserting this external data into the table and then performing the computation. If you do not want (or are not permitted to) create/update tables on a database system, connect to it as a readonly location: > ADD READONLY LOCATION myloc AS MYSQL(\"myhost/mydb\"); If you want specify a default for all connections, please set the sclera.connections.accessmode configuration parameter to readwrite or readonly . In the initial configuration, the value is readwrite . Removing Connected Platforms \u25c4 The following command removes a location referenced by location_name : REMOVE LOCATION location_name Sclera requires that all tables under a location be removed or deleted before removing a location (see the next section ). Importing Database Tables \u25c4 After a database system has been connected, you can pick tables from the database system and add to Sclera. By default, adding a table involves reading (or computing) the metadata -- the set of columns, their data types, the key constraints, etc. -- of the table and storing in the metadata store, so that you can manage the data and frame queries on the same . The command to add tables has the following syntax: ADD TABLE location_name.table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] This command adds to Sclera a new table named table_name from the database system connected as location location_name . You can optionally specify the table schema and constraints explicitly. The details of the parameters used in the schema specifications are similar to that in the CREATE TABLE statement ; please see the associated discussion on column names and types and column constraints . For instance, the following add a new table mytable from location loc and lets Sclera determine the table's schema: > ADD TABLE loc.mytable; However, the following mentions the schema as well, obviating the need for Sclera to determine the metadata: > ADD TABLE loc.mytable(a int primary key, b int); The above statement specifies the table as having two integer columns, a and b , with a being the primary key. The columns a and b must be present in the underlying table mytable at the source database system, with compatible data types. The underlying table can have other columns and constraints, but they are not visible to Sclera now. When the table metadata is specified explicitly, the actual table metadata is not queried, and Sclera does not verify that the columns are actually present, or the specified constraints (e.g. the primary key constraint in the example above) actually hold. However, this is useful when getting the table metadata from the underlying database system is expensive (this is just a one-time computation, though). The Sclera Command-Line Shell provides commands to list the set of imported tables under a location, tables in the location available for import, and so on. Removing Database Tables \u25c4 When you no longer need a table table_name in location location_name , you can remove it as follows: REMOVE TABLE [location_name.]table_name The location_name can be omitted if the table_name is unique across locations. This command only removes the metadata for the table from Sclera's metadata store . The actual table in the underlying data store is not deleted.","title":"Data Platform Connection Reference"},{"location":"setup/dbms/#connecting-to-database-systems","text":"Sclera can connect to Oracle Database 11g Release 2+ , PostgreSQL 9.1.2+ , and MySQL 5.5.28+ (and also MySQL-compatible systems such as Google Cloud SQL ). To connect with a database system not mentioned here, you will need a connector for that database system, which can be built using Sclera's plugin framework . In Sclera, a connected database system is called a location . A new location is added to Sclera using the ADD LOCATION command, which has the following syntax: ADD [ READONLY ] LOCATION location_name AS location_dbms( location_database [ , connection_properties [, ...] ] ) The optional READONLY modifier flags the location as a read-only location; this is explained later . The mandatory location_name is a unique name given to the database system being added, for reference within Sclera. The mandatory location_dbms is the name of the database system: MYSQL , or POSTGRESQL ; each of these are discussed in turn in the subsections that follow. The mandatory location_database is the name of the database within the location_dbms that contains the data we need to access; as discussed in the subsections below. The optional connection_properties list contains optional connection configuration properties for the location_dbms ; these are discussed in context of of specific database systems in the subsections below.","title":"Connecting to Database Systems"},{"location":"setup/dbms/#connecting-to-oracle","text":"The following sets up the data location oraloc , configured as a JDBC connection to the Oracle database oradb on host orahost , port 1521 : > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost (port 1521 ), you can omit the host name: > ADD LOCATION oraloc AS ORACLE(\"oradb\"); The JDBC connection is set up using the Oracle Database Thin JDBC Driver , which is not downloaded during the Sclera-Oracle Connector installation. You will need to download this driver before using this component, as explained later Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\", \"user=orascott\", \"password=oratiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to specify the internal_logon property : > ADD LOCATION oraloc AS ORACLE(\"orahost:1521/oradb\", \"user=orascott\", \"password=oratiger\", \"internal_logon=SYSDBA\"); Any of the properties listed in the Oracle JDBC documentation can be specified. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details. Setup Before using this component, you need to download the Oracle Database Thin JDBC Driver , and include the path to the downloaded driver in the CLASSPATH .","title":"Connecting to Oracle"},{"location":"setup/dbms/#connecting-to-mysql","text":"To work with Sclera, MySQL should be configured in the case-insensitive mode . The following sets up the data location myloc , configured as a JDBC connection to the MySQL database mydb on host myhost : > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost , you can omit the host name: > ADD LOCATION myloc AS MYSQL(\"mydb\"); The JDBC connection is set up using MySQL Connector/J , which is downloaded during the Sclera-MySQL Connector installation. (Note: Sclera assumes that MySQL runs in a case-insensitive mode .) Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\", \"user=myscott\", \"password=mytiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to use SSL connections: > ADD LOCATION myloc AS MYSQL(\"myhost/mydb\", \"user=myscott\", \"password=mytiger\", \"useSSL=true\"); Any of the properties listed in the MySQL Connector/J documentation can be specified, except useOldAliasMetadataBehavior , which is internally set to true by Sclera. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details.","title":"Connecting to MySQL"},{"location":"setup/dbms/#connecting-to-postgresql","text":"The following sets up the data location pgloc , configured as a JDBC connection to the PostgreSQL database pgdb on host pghost : > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\"); Sclera requires that the database server is accessible over the network and accepts connection via JDBC. If the the server is running on localhost , you can omit the host name: > ADD LOCATION pgloc AS POSTGRESQL(\"pgdb\"); The JDBC connection is set up using the PostgreSQL JDBC Driver , which is downloaded during the Sclera-PostgreSQL Connector installation. Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Connection Properties Username and password, if needed, can be specified as follows: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=pgtiger\"); You can also specify additional properties for customizing the JDBC connection. For instance, to use SSL connections: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=pgtiger\", \"ssl=true\"); Any of the properties listed in the PostgreSQL JDBC Driver documentation can be specified as above. Values of all properties (for instance, passwords) may not be mentioned in the command -- you can specify to enter a property value interactively. See the section on interactive input for details.","title":"Connecting to PostgreSQL"},{"location":"setup/dbms/#connecting-to-heroku-postgres","text":"The following sets up the data location heroku , configured as a JDBC connection to a Heroku Postgres database: > ADD LOCATION heroku AS HEROKU_POSTGRESQL(\"postgres://<username>:<password>@<host>:<port>/<dbname>\"); The parameter is the DATABASE_URL in the format provided by Heroku -- see Heroku's documentation for the details. The JDBC connection is set up using the PostgreSQL JDBC Driver . Sclera stores the JDBC configuration in its metadata store , and reconnects to the database at the start of every subsequent Sclera session. Note: Heroku Postgres connectivity was introduced in Sclera 3.0 and is currently in beta.","title":"Connecting to Heroku Postgres"},{"location":"setup/dbms/#interactive-input-of-property-values","text":"Sometimes, you may not want to specify a property value as a part of the statement. For instance, it might be desirable to not disclose the password in the statements for MySQL and PostgreSQL above. In such cases, you can specify the character ' ? ' as the property value instead. While processing the statement, Sclera will prompt for the actual property value and accept the input in a secure manner. For example: > ADD LOCATION pgloc AS POSTGRESQL(\"pghost/pgdb\", \"user=pgscott\", \"password=?\"); password: ******* Here, the password=? generates a prompt that enables you to enter the password interactively.","title":"Interactive Input of Property Values"},{"location":"setup/dbms/#read-write-versus-read-only-mode","text":"By default, Sclera uses the underlying database system in a read-write mode. This means that: The database system acts as a data source. However, if the data needs to be filtered, projected, aggregated, and/or joined with other tables within this database system, then the computation is pushed to this database system and only the result is read by Sclera. If the result as above needs to join with data coming from elsewhere, Sclera may decide to materialize the external data as a table within this database system, and perform the join on this database system. This involves creating a table, inserting this external data into the table and then performing the computation. If you do not want (or are not permitted to) create/update tables on a database system, connect to it as a readonly location: > ADD READONLY LOCATION myloc AS MYSQL(\"myhost/mydb\"); If you want specify a default for all connections, please set the sclera.connections.accessmode configuration parameter to readwrite or readonly . In the initial configuration, the value is readwrite .","title":"Read-Write versus Read-Only Mode"},{"location":"setup/dbms/#removing-connected-platforms","text":"The following command removes a location referenced by location_name : REMOVE LOCATION location_name Sclera requires that all tables under a location be removed or deleted before removing a location (see the next section ).","title":"Removing Connected Platforms"},{"location":"setup/dbms/#importing-database-tables","text":"After a database system has been connected, you can pick tables from the database system and add to Sclera. By default, adding a table involves reading (or computing) the metadata -- the set of columns, their data types, the key constraints, etc. -- of the table and storing in the metadata store, so that you can manage the data and frame queries on the same . The command to add tables has the following syntax: ADD TABLE location_name.table_name [ ( column_name data_type [ column_constraint [ ... ] ] [, ...] [ , table_constraint [, ...] ] ) ] This command adds to Sclera a new table named table_name from the database system connected as location location_name . You can optionally specify the table schema and constraints explicitly. The details of the parameters used in the schema specifications are similar to that in the CREATE TABLE statement ; please see the associated discussion on column names and types and column constraints . For instance, the following add a new table mytable from location loc and lets Sclera determine the table's schema: > ADD TABLE loc.mytable; However, the following mentions the schema as well, obviating the need for Sclera to determine the metadata: > ADD TABLE loc.mytable(a int primary key, b int); The above statement specifies the table as having two integer columns, a and b , with a being the primary key. The columns a and b must be present in the underlying table mytable at the source database system, with compatible data types. The underlying table can have other columns and constraints, but they are not visible to Sclera now. When the table metadata is specified explicitly, the actual table metadata is not queried, and Sclera does not verify that the columns are actually present, or the specified constraints (e.g. the primary key constraint in the example above) actually hold. However, this is useful when getting the table metadata from the underlying database system is expensive (this is just a one-time computation, though). The Sclera Command-Line Shell provides commands to list the set of imported tables under a location, tables in the location available for import, and so on.","title":"Importing Database Tables"},{"location":"setup/dbms/#removing-database-tables","text":"When you no longer need a table table_name in location location_name , you can remove it as follows: REMOVE TABLE [location_name.]table_name The location_name can be omitted if the table_name is unique across locations. This command only removes the metadata for the table from Sclera's metadata store . The actual table in the underlying data store is not deleted.","title":"Removing Database Tables"},{"location":"setup/install/","text":"Sclera is a stand-alone SQL processor with native support for machine learning, data virtualization and streaming data. Sclera can be deployed as: an independent application with an interactive command-line shell , or as a library that embeds within your applications to enable advanced real-time analytics capabilities . Prerequisite: Sclera requires Java 13 or higher . We recommend against installing Sclera with root/admin permissions. Sclera does not need root access for installation or at runtime. Installing and Maintaining Sclera Command Line Application \u25c4 The recommended way to install Sclera's interactive command line shell is through Sclera platform administration tool, scleradmin . We first describe how to install scleradmin . Next, we use scleradmin to install Sclera's core packages and command line interface . We then show how to add and remove optional Sclera plugins . Finally, we show how to update the installed core packages and plugins . Prerequisites: Python 3.8+ and pip . Installing scleradmin \u25c4 The following installs the latest version of scleradmin as a Python package: > python3.8 -m pip install scleradmin where python3.8 stands for the executable for Python version 3.8 or above in your setup. In the following sections, we show how to use scleradmin for installing Sclera and managing the installation. For full details on scleradmin usage, you may want to read the help information: > scleradmin --help Installing Sclera Core Packages and Shell \u25c4 The following command installs Sclera: > scleradmin --install --root <sclera-root> In the above, <sclera-root> is the directory where you want Sclera to be installed. This directory must not exist before installation, it is created by the command (this is a safeguard againt accidental overwrites). The contents of the directory after installation are described later in this document . The installation involves downloading core sclera components and associated libraries. This might take a few minutes; you can monitor the progress by viewing the generated logs in <sclera-root>/install/log/install.log . Using the Shell \u25c4 The shell can be started using the following command: > <sclera-root>/bin/sclera This starts the shell, wherein you can interactively run queries. When done, you can terminate the session by typing Control-D . Welcome to Sclera 4.0 > select \"Hello, world!\" as greeting; --------------- GREETING --------------- Hello, world! --------------- (1 row) > ^D Goodbye! For details on using the shell, please refer to the Command Line Shell Reference . Root Directory Structure \u25c4 After installation, the root directory has the following structure: [<sclera-root>] bin/ sclera.cmd # executable command file (generated for Windows systems) sclera # executable bash (generated for Linux, macOS, and other Unix-based systems) config/ sclera.conf # configuration file extlib/ # directory for additional libraries, plugins (initially empty) home/ assets/ data/ # data stored by the embedded temporary database (H2), etc. history # shell command history log/ sclera.log # execution log, contains details of runtime progress install/ boot/ # specification files for sclera components (core or plugin) launcher*.jar # SBT launcher, used for installing sclera components log/ install.log # installation log, contains details of installation progress lib/ # installation artifacts (jars, etc.) of installed components and their dependencies Plugin Management \u25c4 Sclera provides a variety of plugins that can be added using scleradmin . The command syntax is: > scleradmin --add <plugins> --root <sclera-root> In the above, <plugins> is a space-separated list of plugins to be added, and <sclera-root> , as earlier , is the root directory. For instance, to add the Sclera - CSV File Connector and Sclera - Text File Connector plugins to the Sclera instance installed at /path/to/sclera , the command is: > scleradmin --add sclera-csv-plugin sclera-textfiles-plugin --root /path/to/sclera To remove installed plugins, the syntax is similar. The following command removes the plugins installed above: > scleradmin --remove sclera-csv-plugin sclera-textfiles-plugin --root /path/to/sclera You can specify a list of plugins to add and another list of plugins to remove in the same command. For a list of available plugins and other components, please refer to the components documentation . Updating Installed Packages and Plugins \u25c4 The following command updates Sclera's core packages as well as the plugins to the latest version: > scleradmin --update --root <sclera-root> where <sclera-root> , as mentioned earlier , is the root directory. Embedding Sclera in Applications \u25c4 Sclera's JDBC driver sclera-jdbc provides a JDBC type 4 interface. Applications can therefore interface with Sclera using JDBC API . Since JDBC is a well-known standard, the application is written the same way as for any other JDBC compliant database system. The difference is that the queries that the application can submit are now in the much richer Sclera SQL with access to the Sclera plugins than the standard SQL. Alternatively, you can use Sclera's proprietary API in your applications. This API is much simpler than JDBC, but is not a standard. Sclera - JDBC Example \u25c4 We illustrate the use of the JDBC interface with Sclera using an example application. The code is available on GitHub, both in Java and Scala: Sclera - JDBC Example (Java version) on GitHub Sclera - JDBC Example (Scala version) on GitHub This example application shows how an application can interface with Sclera using the standard JDBC API . To use Sclera through JDBC, the application needs to: specify the Sclera home directory by setting the SCLERA_ROOT environment variable (if not set, the default is $HOME/.sclera ) add the following dependencies: Sclera Configuration Manager, sclera-config , Sclera Core Engine, sclera-core , Sclera JDBC Driver, sclera-jdbc , and Sclera plugins needed (if any). connect to Sclera's JDBC driver using the JDBC URL jdbc:scleradb , and execute commands and queries using the standard JDBC API . The example application described below is a command line tool to initialize Sclera, and execute queries. See here for details on the usage. Specify Sclera Root Directory \u25c4 We need to specify a directory where Sclera can keep its configuration, metadata, and internal database. This is done by setting the environment variable SCLERA_ROOT . If not specified, the default is $HOME/.sclera . Add Package Dependencies \u25c4 This example uses SBT as the build tool, and the build file is build.sbt . The required dependencies are added as: libraryDependencies ++= Seq( \"com.scleradb\" %% \"sclera-config\" % \"4.0\", \"com.scleradb\" %% \"sclera-core\" % \"4.0\", \"com.scleradb\" %% \"sclera-jdbc\" % \"4.0\" ) This is a minimal example, and does not include any Sclera plugins. If your example needs a Sclera Plugin, it should be added to the libraryDependencies as well. The latest versions of all Sclera components are listed at https://github.com/scleradb/sclera-version-map/blob/master/versions.ini . Interface with Sclera using the JDBC API \u25c4 This application consists of a single source file, JdbcExample.java / JdbcExample.scala . There are two procedures: initialize() : This initializes Sclera's schema (metadata). This is called when --init is specified on the command line. runQueries() : This executes queries provided on the command line and displays the results. Code Details: initialize() Links with Sclera's JDBC driver and gets a JDBC Connection . Creates a JDBC Statement using the JDBC connection. Executes the statement create schema on Sclera using the JDBC Statement . When a connection is initialized, Sclera first checks the sanity of its Schema and issues a warning if anything is wrong. Since we are initializing the schema, we bypass this step by passing a flag checkSchema in the properties while creating a connection. Code Details: runQueries(...) Links with Sclera's JDBC driver and gets a JDBC Connection . Creates a JDBC Statement using the JDBC connection. For each query in the list passed as the parameter, Executes the query using the JDBC Statement , getting the JDBC ResultSet Get the JDBC ResultSetMetadata for the ResultSet -- this provides the number of columns in the result and their names. Output the column names, followed by the result values one row at a time. Sclera - Proprietary API Example \u25c4 We illustrate the use of Sclera's proprietary API using the same example application as above. The code is available on GitHub: Sclera - Proprietary API Example on GitHub To use Sclera through the proprietary API, the application needs to: specify the Sclera home directory by setting the SCLERA_ROOT environment variable (if not set, the default is $HOME/.sclera ) add the following dependencies: Sclera Configuration Manager, sclera-config , Sclera Core Engine, sclera-core , Sclera plugins needed (if any). The example application described below is a command line tool to initialize Sclera, and execute queries. See here for details on the usage. Specify Sclera Root Directory \u25c4 We need to specify a directory where Sclera can keep its configuration, metadata, and internal database. This is done by setting the environment variable SCLERA_ROOT . If not specified, the default is $HOME/.sclera . Add Package Dependencies \u25c4 This example uses SBT as the build tool, and the build file is build.sbt . The required dependencies are added as: libraryDependencies ++= Seq( \"com.scleradb\" %% \"sclera-config\" % \"4.0\", \"com.scleradb\" %% \"sclera-core\" % \"4.0\" ) This is a minimal example, and does not include any Sclera plugins. If your example needs a Sclera Plugin, it should be added to the libraryDependencies as well. Interface with Sclera using the Proprietary API \u25c4 This application consists of a single source file, ApiExample.scala . There are two procedures: initialize() : This initializes Sclera's schema (metadata). This is called when --init is specified on the command line. runQueries() : This executes queries provided on the command line and displays the results. Code Details: initialize() Creates and initializes an instance of Sclera Processor Executes the statement create schema on Sclera using the Processor instance. When the Processor instance is initialized, Sclera first checks the sanity of its Schema and issues a warning if anything is wrong. Since we are initializing the schema, we bypass this step by passing a flag checkSchema in the properties while creating the Processor instance. Code Details: runQueries(...) Creates and initializes an instance of Sclera Processor For each query in the list passed as the parameter, Executes the query using the Processor instance, getting the result as an instance of type TableResult , containing an iterator over the returned rows and the metadata for the row columns. The rows are of type TableRow , and the metadata is a list of instances of type Column . Output the column names, followed by the result values one row at a time. Executable Script \u25c4 The build file in each of the above cases contains a task mkscript that generates an executable script for the application, called scleraexample in the bin subdirectory. You can generate the script using the command: > sbt mkscript The script is run as follows: > bin/scleraexample --init > bin/scleraexample \"select 'Hello' as greeting1, 'World!' as greeting2\" GREETING1, GREETING2 Hello, World!","title":"Installation Reference"},{"location":"setup/install/#installing-and-maintaining-sclera-command-line-application","text":"The recommended way to install Sclera's interactive command line shell is through Sclera platform administration tool, scleradmin . We first describe how to install scleradmin . Next, we use scleradmin to install Sclera's core packages and command line interface . We then show how to add and remove optional Sclera plugins . Finally, we show how to update the installed core packages and plugins . Prerequisites: Python 3.8+ and pip .","title":"Installing and Maintaining Sclera Command Line Application"},{"location":"setup/install/#installing-scleradmin","text":"The following installs the latest version of scleradmin as a Python package: > python3.8 -m pip install scleradmin where python3.8 stands for the executable for Python version 3.8 or above in your setup. In the following sections, we show how to use scleradmin for installing Sclera and managing the installation. For full details on scleradmin usage, you may want to read the help information: > scleradmin --help","title":"Installing scleradmin"},{"location":"setup/install/#installing-sclera-core-packages-and-shell","text":"The following command installs Sclera: > scleradmin --install --root <sclera-root> In the above, <sclera-root> is the directory where you want Sclera to be installed. This directory must not exist before installation, it is created by the command (this is a safeguard againt accidental overwrites). The contents of the directory after installation are described later in this document . The installation involves downloading core sclera components and associated libraries. This might take a few minutes; you can monitor the progress by viewing the generated logs in <sclera-root>/install/log/install.log .","title":"Installing Sclera Core Packages and Shell"},{"location":"setup/install/#using-the-shell","text":"The shell can be started using the following command: > <sclera-root>/bin/sclera This starts the shell, wherein you can interactively run queries. When done, you can terminate the session by typing Control-D . Welcome to Sclera 4.0 > select \"Hello, world!\" as greeting; --------------- GREETING --------------- Hello, world! --------------- (1 row) > ^D Goodbye! For details on using the shell, please refer to the Command Line Shell Reference .","title":"Using the Shell"},{"location":"setup/install/#root-directory-structure","text":"After installation, the root directory has the following structure: [<sclera-root>] bin/ sclera.cmd # executable command file (generated for Windows systems) sclera # executable bash (generated for Linux, macOS, and other Unix-based systems) config/ sclera.conf # configuration file extlib/ # directory for additional libraries, plugins (initially empty) home/ assets/ data/ # data stored by the embedded temporary database (H2), etc. history # shell command history log/ sclera.log # execution log, contains details of runtime progress install/ boot/ # specification files for sclera components (core or plugin) launcher*.jar # SBT launcher, used for installing sclera components log/ install.log # installation log, contains details of installation progress lib/ # installation artifacts (jars, etc.) of installed components and their dependencies","title":"Root Directory Structure"},{"location":"setup/install/#plugin-management","text":"Sclera provides a variety of plugins that can be added using scleradmin . The command syntax is: > scleradmin --add <plugins> --root <sclera-root> In the above, <plugins> is a space-separated list of plugins to be added, and <sclera-root> , as earlier , is the root directory. For instance, to add the Sclera - CSV File Connector and Sclera - Text File Connector plugins to the Sclera instance installed at /path/to/sclera , the command is: > scleradmin --add sclera-csv-plugin sclera-textfiles-plugin --root /path/to/sclera To remove installed plugins, the syntax is similar. The following command removes the plugins installed above: > scleradmin --remove sclera-csv-plugin sclera-textfiles-plugin --root /path/to/sclera You can specify a list of plugins to add and another list of plugins to remove in the same command. For a list of available plugins and other components, please refer to the components documentation .","title":"Plugin Management"},{"location":"setup/install/#updating-installed-packages-and-plugins","text":"The following command updates Sclera's core packages as well as the plugins to the latest version: > scleradmin --update --root <sclera-root> where <sclera-root> , as mentioned earlier , is the root directory.","title":"Updating Installed Packages and Plugins"},{"location":"setup/install/#embedding-sclera-in-applications","text":"Sclera's JDBC driver sclera-jdbc provides a JDBC type 4 interface. Applications can therefore interface with Sclera using JDBC API . Since JDBC is a well-known standard, the application is written the same way as for any other JDBC compliant database system. The difference is that the queries that the application can submit are now in the much richer Sclera SQL with access to the Sclera plugins than the standard SQL. Alternatively, you can use Sclera's proprietary API in your applications. This API is much simpler than JDBC, but is not a standard.","title":"Embedding Sclera in Applications"},{"location":"setup/install/#sclera-jdbc-example","text":"We illustrate the use of the JDBC interface with Sclera using an example application. The code is available on GitHub, both in Java and Scala: Sclera - JDBC Example (Java version) on GitHub Sclera - JDBC Example (Scala version) on GitHub This example application shows how an application can interface with Sclera using the standard JDBC API . To use Sclera through JDBC, the application needs to: specify the Sclera home directory by setting the SCLERA_ROOT environment variable (if not set, the default is $HOME/.sclera ) add the following dependencies: Sclera Configuration Manager, sclera-config , Sclera Core Engine, sclera-core , Sclera JDBC Driver, sclera-jdbc , and Sclera plugins needed (if any). connect to Sclera's JDBC driver using the JDBC URL jdbc:scleradb , and execute commands and queries using the standard JDBC API . The example application described below is a command line tool to initialize Sclera, and execute queries. See here for details on the usage.","title":"Sclera - JDBC Example"},{"location":"setup/install/#specify-sclera-root-directory","text":"We need to specify a directory where Sclera can keep its configuration, metadata, and internal database. This is done by setting the environment variable SCLERA_ROOT . If not specified, the default is $HOME/.sclera .","title":"Specify Sclera Root Directory"},{"location":"setup/install/#add-package-dependencies","text":"This example uses SBT as the build tool, and the build file is build.sbt . The required dependencies are added as: libraryDependencies ++= Seq( \"com.scleradb\" %% \"sclera-config\" % \"4.0\", \"com.scleradb\" %% \"sclera-core\" % \"4.0\", \"com.scleradb\" %% \"sclera-jdbc\" % \"4.0\" ) This is a minimal example, and does not include any Sclera plugins. If your example needs a Sclera Plugin, it should be added to the libraryDependencies as well. The latest versions of all Sclera components are listed at https://github.com/scleradb/sclera-version-map/blob/master/versions.ini .","title":"Add Package Dependencies"},{"location":"setup/install/#interface-with-sclera-using-the-jdbc-api","text":"This application consists of a single source file, JdbcExample.java / JdbcExample.scala . There are two procedures: initialize() : This initializes Sclera's schema (metadata). This is called when --init is specified on the command line. runQueries() : This executes queries provided on the command line and displays the results. Code Details: initialize() Links with Sclera's JDBC driver and gets a JDBC Connection . Creates a JDBC Statement using the JDBC connection. Executes the statement create schema on Sclera using the JDBC Statement . When a connection is initialized, Sclera first checks the sanity of its Schema and issues a warning if anything is wrong. Since we are initializing the schema, we bypass this step by passing a flag checkSchema in the properties while creating a connection. Code Details: runQueries(...) Links with Sclera's JDBC driver and gets a JDBC Connection . Creates a JDBC Statement using the JDBC connection. For each query in the list passed as the parameter, Executes the query using the JDBC Statement , getting the JDBC ResultSet Get the JDBC ResultSetMetadata for the ResultSet -- this provides the number of columns in the result and their names. Output the column names, followed by the result values one row at a time.","title":"Interface with Sclera using the JDBC API"},{"location":"setup/install/#sclera-proprietary-api-example","text":"We illustrate the use of Sclera's proprietary API using the same example application as above. The code is available on GitHub: Sclera - Proprietary API Example on GitHub To use Sclera through the proprietary API, the application needs to: specify the Sclera home directory by setting the SCLERA_ROOT environment variable (if not set, the default is $HOME/.sclera ) add the following dependencies: Sclera Configuration Manager, sclera-config , Sclera Core Engine, sclera-core , Sclera plugins needed (if any). The example application described below is a command line tool to initialize Sclera, and execute queries. See here for details on the usage.","title":"Sclera - Proprietary API Example"},{"location":"setup/install/#specify-sclera-root-directory_1","text":"We need to specify a directory where Sclera can keep its configuration, metadata, and internal database. This is done by setting the environment variable SCLERA_ROOT . If not specified, the default is $HOME/.sclera .","title":"Specify Sclera Root Directory"},{"location":"setup/install/#add-package-dependencies_1","text":"This example uses SBT as the build tool, and the build file is build.sbt . The required dependencies are added as: libraryDependencies ++= Seq( \"com.scleradb\" %% \"sclera-config\" % \"4.0\", \"com.scleradb\" %% \"sclera-core\" % \"4.0\" ) This is a minimal example, and does not include any Sclera plugins. If your example needs a Sclera Plugin, it should be added to the libraryDependencies as well.","title":"Add Package Dependencies"},{"location":"setup/install/#interface-with-sclera-using-the-proprietary-api","text":"This application consists of a single source file, ApiExample.scala . There are two procedures: initialize() : This initializes Sclera's schema (metadata). This is called when --init is specified on the command line. runQueries() : This executes queries provided on the command line and displays the results. Code Details: initialize() Creates and initializes an instance of Sclera Processor Executes the statement create schema on Sclera using the Processor instance. When the Processor instance is initialized, Sclera first checks the sanity of its Schema and issues a warning if anything is wrong. Since we are initializing the schema, we bypass this step by passing a flag checkSchema in the properties while creating the Processor instance. Code Details: runQueries(...) Creates and initializes an instance of Sclera Processor For each query in the list passed as the parameter, Executes the query using the Processor instance, getting the result as an instance of type TableResult , containing an iterator over the returned rows and the metadata for the row columns. The rows are of type TableRow , and the metadata is a list of instances of type Column . Output the column names, followed by the result values one row at a time.","title":"Interface with Sclera using the Proprietary API"},{"location":"setup/install/#executable-script","text":"The build file in each of the above cases contains a task mkscript that generates an executable script for the application, called scleraexample in the bin subdirectory. You can generate the script using the command: > sbt mkscript The script is run as follows: > bin/scleraexample --init > bin/scleraexample \"select 'Hello' as greeting1, 'World!' as greeting2\" GREETING1, GREETING2 Hello, World!","title":"Executable Script"}]}